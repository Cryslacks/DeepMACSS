{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20389c2a",
   "metadata": {},
   "source": [
    "# AI Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0bc09",
   "metadata": {},
   "source": [
    "# Import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b644fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as transform\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Calculates the Intersection Over Union for two specified bounding boxes\n",
    "def calc_iou(bb1, bb2):\n",
    "    # Get the coordinates of the intersecting box\n",
    "    inter_x = max(bb1[0], bb2[0])\n",
    "    inter_y = max(bb1[1], bb2[1])\n",
    "    inter_x2 = min(bb1[2], bb2[2])\n",
    "    inter_y2 = min(bb1[3], bb2[3])\n",
    "    \n",
    "    if inter_x2 < inter_x or inter_y2 < inter_y:\n",
    "        return 0.0\n",
    "    \n",
    "    inter_area = (inter_x2 - inter_x) * (inter_y2 - inter_y)\n",
    "\n",
    "    # If intersection area is or lower than 0 we dont have an intersection\n",
    "    #if inter_area <= 0:\n",
    "    #    return 0.0\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "    iou = inter_area / float(bb1_area + bb2_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "def calc_intersection(bb1, bb2):\n",
    "    inter_x = max(bb1[0], bb2[0])\n",
    "    inter_y = max(bb1[1], bb2[1])\n",
    "    inter_x2 = min(bb1[2], bb2[2])\n",
    "    inter_y2 = min(bb1[3], bb2[3])\n",
    "    \n",
    "    if inter_x2 < inter_x or inter_y2 < inter_y:\n",
    "        return 0.0\n",
    "    \n",
    "    return (inter_x2 - inter_x) * (inter_y2 - inter_y)\n",
    "\n",
    "# Calculates the area of a bounding box\n",
    "def calc_area(bb):\n",
    "    return (bb[2] - bb[0]) * (bb[3] - bb[1])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Padds a bounding box by a specific number, doubles the padding if text is specified\n",
    "def pad_bb(bb, pad, text=False):\n",
    "    x,y,x2,y2 = bb\n",
    "    if text:\n",
    "        return [x-pad*2, y-pad, x2+pad*2, y2+pad]\n",
    "    return [x-pad, y-pad, x2+pad, y2+pad]\n",
    "\n",
    "# Returns the smallest bounding box between two specified boxes\n",
    "def return_smallest(bb1, bb2):\n",
    "    bb1_x,bb1_y,bb1_x2,bb1_y2 = bb1\n",
    "    bb2_x,bb2_y,bb2_x2,bb2_y2 = bb2\n",
    "    bb1_size = (bb1_x2-bb1_x)*(bb1_y2-bb1_y)\n",
    "    bb2_size = (bb2_x2-bb2_x)*(bb2_y2-bb2_y)\n",
    "    \n",
    "    return bb2 if bb1_size > bb2_size else bb1\n",
    "\n",
    "# Gets the bounding boxes from an image by processing the image\n",
    "def get_bbs_from_image(im, clean=True, pad=0, text=False, ignore_padding=10, combine_all=False):\n",
    "    imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bbs = []\n",
    "    for cntr in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cntr)\n",
    "        #cv2.rectangle(im, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        if x > ignore_padding and y > ignore_padding and x < im.shape[:2][1]-ignore_padding and y < im.shape[:2][0]-ignore_padding:\n",
    "            bbs.append([x,y,x+w,y+h])\n",
    "                \n",
    "    t_bbs = []\n",
    "    [t_bbs.append(x) for x in bbs if x not in t_bbs]\n",
    "    \n",
    "    t_bbs = remove_small_bb_list(t_bbs, 10000)\n",
    "    combined_bbs = combine_bb_list(t_bbs, pad=pad, text=text)\n",
    "    \n",
    "    if clean:\n",
    "        combined_bbs = clean_bb_list(combined_bbs, pad=pad)\n",
    "    \n",
    "    if combine_all:\n",
    "        temp_bb = combined_bbs[0]\n",
    "        for bb in combined_bbs:\n",
    "            if temp_bb[0] > bb[0]:\n",
    "                temp_bb[0] = bb[0]\n",
    "            if temp_bb[1] > bb[1]:\n",
    "                temp_bb[1] = bb[1]\n",
    "            if temp_bb[2] < bb[2]:\n",
    "                temp_bb[2] = bb[2]\n",
    "            if temp_bb[3] < bb[3]:\n",
    "                temp_bb[3] = bb[3]\n",
    "        return [temp_bb]\n",
    "    return combined_bbs\n",
    "\n",
    "def combine_bb_list(bb_list, pad=0, text=False):\n",
    "    bbs = bb_list.copy()\n",
    "    iou_non_zero = True\n",
    "    while iou_non_zero:\n",
    "        iou_non_zero = False\n",
    "        for i in range(len(bbs)-1):\n",
    "            for c in range(i, len(bbs)):\n",
    "                if bbs[i] == bbs[c]:\n",
    "                    continue\n",
    "                    \n",
    "                iou = calc_iou(pad_bb(bbs[i], pad, text=text), bbs[c])\n",
    "                \n",
    "                if iou != 0:\n",
    "                    iou_non_zero = True\n",
    "                    bb = combine_bb(bbs[i], bbs[c])\n",
    "                    bb1 = bbs[i].copy()\n",
    "                    bb2 = bbs[c].copy()\n",
    "                    \n",
    "                    bbs.remove(bb1)\n",
    "                    bbs.remove(bb2)\n",
    "                    bbs.append(bb)\n",
    "                    break;\n",
    "            if iou_non_zero:\n",
    "                break;\n",
    "    return bbs\n",
    "\n",
    "def combine_bb(bb1, bb2):\n",
    "    bb1_x,bb1_y,bb1_x2,bb1_y2 = bb1\n",
    "    bb2_x,bb2_y,bb2_x2,bb2_y2 = bb2\n",
    "\n",
    "    if bb2_x < bb1_x:\n",
    "        bb1_x = bb2_x\n",
    "    if bb2_y < bb1_y:\n",
    "        bb1_y = bb2_y\n",
    "    if bb2_x2 > bb1_x2:\n",
    "        bb1_x2 = bb2_x2\n",
    "    if bb2_y2 > bb1_y2:\n",
    "        bb1_y2 = bb2_y2\n",
    "        \n",
    "    return [bb1_x, bb1_y, bb1_x2, bb1_y2]\n",
    "\n",
    "def clean_bb_list(bb_list, pad=0, text=False):\n",
    "    bbs = bb_list.copy()\n",
    "    iou_non_zero = True\n",
    "    while iou_non_zero:\n",
    "        iou_non_zero = False\n",
    "        for i in range(len(bbs)):\n",
    "            if i == len(bbs)-1:\n",
    "                break;\n",
    "                \n",
    "            iou = calc_iou(pad_bb(bbs[i], pad, text=text), bbs[i+1])\n",
    "\n",
    "            if iou == 0:\n",
    "                continue\n",
    "\n",
    "            iou_non_zero = True\n",
    "            bb = return_smallest(bbs[i], bbs[i+1])\n",
    "            bbs.remove(bb)\n",
    "            break;\n",
    "                \n",
    "    return bbs\n",
    "\n",
    "def remove_small_bb_list(bb_list, size):\n",
    "    cleaned_list = []\n",
    "    for bb in bb_list:\n",
    "        x,y,x2,y2 = bb\n",
    "        w = x2-x\n",
    "        h = y2-y\n",
    "        if w*h > size:\n",
    "            cleaned_list.append(bb)\n",
    "            \n",
    "    return cleaned_list\n",
    "\n",
    "# Normalizes a pixel specific bounding box [x, y, x2, y2] to normalized bounding box [x, y, w, h]\n",
    "def normalize_bb(bb, shape):\n",
    "    h_img,w_img = shape\n",
    "    x,y,x2,y2 = bb\n",
    "    norm_w,norm_h = [(x2-x)/w_img, (y2-y)/h_img]\n",
    "    return [((x+x2)/2)/w_img, ((y+y2)/2)/h_img, norm_w, norm_h]\n",
    "\n",
    "# Denormalizes a normalized bounding box [x, y, w, h] to pixel specific bounding box [x, y, x2, y2]\n",
    "def denormalize_bb(bb, shape):\n",
    "    h_img,w_img = shape\n",
    "    x,y,w,h = bb\n",
    "    x_min,y_min = [int(x*w_img-(w*w_img)/2), int(y*h_img-(h*h_img)/2)]\n",
    "    return [x_min, y_min, x_min+int(w*w_img), y_min+int(h*h_img)]\n",
    "\n",
    "# Stringifies a bounding box for output\n",
    "def bb_to_str(bb):\n",
    "    return str(bb[0])+' '+str(bb[1])+' '+str(bb[2])+' '+str(bb[3])\n",
    "\n",
    "# Destringifies a bounding box\n",
    "def str_to_bb(bb_str):\n",
    "    str_arr = bb_str.split(' ')\n",
    "    return [float(str_arr[0]), float(str_arr[1]), float(str_arr[2]), float(str_arr[3]), float(str_arr[4])]\n",
    "\n",
    "# Generates dataset structure by generating boundingbox labels, spliting data into train and validition sets\n",
    "# also providing the found boundingboxes for verification of labeling being successfull \n",
    "def generate_dataset(root_folder, labels=[], split_components=True, train_val_ratio=0.8, combine_all=False):\n",
    "    os.mkdir('./'+root_folder+'_generated/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/train/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/val/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/bbs/')\n",
    "    os.mkdir('./'+root_folder+'_generated/labels/')\n",
    "\n",
    "    if (split_components):    \n",
    "        for component in os.listdir('./'+root_folder):\n",
    "            os.mkdir('./'+root_folder+'_generated/images/train/'+component+'/')\n",
    "            os.mkdir('./'+root_folder+'_generated/images/val/'+component+'/')\n",
    "            images = os.listdir('./'+root_folder+'/'+component)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i]\n",
    "                img_type = 'val' if i > math.floor(len(images)*train_val_ratio) else 'train'\n",
    "                im = cv2.imread('./'+root_folder+'/'+component+'/'+image)\n",
    "                cv2.imwrite('./'+root_folder+'_generated/images/'+img_type+'/'+component+'/'+image, im)\n",
    "                bbs = get_bbs_from_image(im, clean=True, pad=30, text=True, combine_all=combine_all)\n",
    "                bbs_str = '' \n",
    "                for bb in bbs:\n",
    "                    bbs_str += str(labels[component])+' '+bb_to_str(normalize_bb(bb, im.shape[:2]))+'\\n'\n",
    "                    x,y,x2,y2 = pad_bb(bb, 5)\n",
    "                    cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.imwrite('./'+root_folder+'_generated/images/bbs/'+image, im)\n",
    "                f = open('./'+root_folder+'_generated/labels/'+image[:-3]+\"txt\", \"a\")\n",
    "                f.write(bbs_str[:-1])\n",
    "                f.close()\n",
    "    else:\n",
    "        images = os.listdir('./'+root_folder)\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            img_type = 'val' if i > math.floor(len(images)*train_val_ratio) else 'train'\n",
    "            im = cv2.imread('./'+root_folder+'/'+image)\n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/'+img_type+'/'+image, im)\n",
    "            bbs = get_bbs_from_image(im, clean=True, pad=30, text=True, combine_all=combine_all)\n",
    "            \n",
    "            bbs_str = '' \n",
    "            c = 0\n",
    "            for bb in bbs:\n",
    "                c = c + 1\n",
    "                bbs_str += str(c)+' '+bb_to_str(normalize_bb(bb, im.shape[:2]))+'\\n'\n",
    "                x,y,x2,y2 = pad_bb(bb, 5)\n",
    "                cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(im, str(c), (int((x+x2)/2)-50,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/bbs/'+image, im)\n",
    "            f = open('./'+root_folder+'_generated/labels/'+image[:-3]+\"txt\", \"a\")\n",
    "            f.write(bbs_str[:-1])\n",
    "            f.close()\n",
    "            \n",
    "def threshold_output(prediction, threshold=0.5):\n",
    "    output = {'boxes':[], 'scores':[], 'labels':[]}\n",
    "    for i in range(len(prediction['scores'])):\n",
    "        if prediction['scores'][i] > threshold:\n",
    "            output['boxes'].append(prediction['boxes'][i])\n",
    "            output['scores'].append(prediction['scores'][i])\n",
    "            output['labels'].append(prediction['labels'][i])\n",
    "    return output\n",
    "\n",
    "def interpol_precision(precision, fptp):\n",
    "    inter_prec = []\n",
    "    curr_prec = precision[0]\n",
    "    for i in range(len(precision)):\n",
    "        if fptp[i]:\n",
    "            curr_prec = precision[i]\n",
    "        inter_prec.append(curr_prec)\n",
    "    return inter_prec\n",
    "\n",
    "def calc_ap(precision, recall, fptp):\n",
    "    inter_prec = interpol_precision(precision, fptp)\n",
    "    AP = 0\n",
    "    inter_prec.append(0)\n",
    "    \n",
    "    p = 0\n",
    "    for i in range(0, 11):\n",
    "        for c in range(p, len(recall)):\n",
    "            if recall[c] < i*0.1:\n",
    "                if p != len(recall):\n",
    "                    p += 1\n",
    "            else:\n",
    "                break\n",
    "        AP += inter_prec[p]\n",
    "    \n",
    "    return AP/11\n",
    "\n",
    "def calc_map(model, data_loader, device, num_classes, threshold=0.5, IoU=0.5):\n",
    "    precision = [[] for k in range(0,num_classes)]\n",
    "    recall = [[] for k in range(0,num_classes)]\n",
    "    fptp = [[] for k in range(0,num_classes)] # 0 = false positive, 1 = true positive\n",
    "    fptp_p = [1 for k in range(0,num_classes)]\n",
    "    tpfn = [0 for k in range(0,num_classes)]\n",
    "    AP = [0 for k in range(0,num_classes)]\n",
    "    mAP = 0\n",
    "    predictions = []\n",
    "    batch_nr = 0\n",
    "    epoch_time = time.time()\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        batch_nr += 1\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        preds = model(images)\n",
    "        for i in range(len(preds)):\n",
    "            thres = threshold_output(preds[i], threshold)\n",
    "            for c in range(len(thres['labels'])):\n",
    "                b_label = False\n",
    "                for g in range(len(targets[i]['labels'])):\n",
    "                    if device.type == 'cpu':\n",
    "                        iou = calc_iou(targets[i]['boxes'][g].detach().numpy(), thres['boxes'][c].detach().numpy())\n",
    "                    else:\n",
    "                        iou = calc_iou(targets[i]['boxes'][g].cpu().detach().numpy(), thres['boxes'][c].cpu().detach().numpy())\n",
    "                    if iou > IoU and thres['labels'][c].item() == targets[i]['labels'][g].item():\n",
    "                        fptp[thres['labels'][c].item()].append(1)\n",
    "                        b_label = True\n",
    "                        break;\n",
    "                if not b_label:\n",
    "                        fptp[thres['labels'][c].item()].append(0)\n",
    "                        tpfn[thres['labels'][c].item()] += 1\n",
    "\n",
    "        for tar in targets:\n",
    "            for lab in tar['labels']:\n",
    "                tpfn[lab.item()] += 1\n",
    "        \n",
    "        print(\n",
    "            '\\r[Eval] mAP [{}/{}]\\tEpoch time elapsed: {}'.format(\n",
    "                batch_nr, len(data_loader), str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "            ),\n",
    "            end=''\n",
    "        )\n",
    "    \n",
    "    for i in range(len(fptp)):\n",
    "        for c in range(1,len(fptp[i])+1):\n",
    "            precision[i].append(sum(fptp[i][:c])/len(fptp[i][:c]))\n",
    "            recall[i].append(sum(fptp[i][:c])/tpfn[i])\n",
    "    \n",
    "    for i in range(len(recall)):\n",
    "        if precision[i] != []:\n",
    "            AP[i] = calc_ap(precision[i], recall[i], fptp[i])*100\n",
    "            mAP += AP[i]\n",
    "    mAP = mAP/(num_classes-1)\n",
    "    return AP[1:], mAP\n",
    "            \n",
    "def predict_and_save(model, root_dir, save_dir, labels=[], threshold=0.5, IoU=0, mask=False, unique_name='', skip_component=''):\n",
    "    # Check if the 4th final character is a dot aka if the input directory is a file\n",
    "    if root_dir[-4] == '.':\n",
    "        im = root_dir\n",
    "        img = cv2.imread(im)\n",
    "        cv2_img = cv2.imread(im)\n",
    "        if mask:\n",
    "            imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "        tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "        predictions = model(tensor_img)\n",
    "        dont_print_id = []\n",
    "        for i in range(len(predictions[0]['boxes'])):\n",
    "            score = predictions[0]['scores'][i].item()\n",
    "            if IoU > 0:\n",
    "                if i in dont_print_id:\n",
    "                    continue\n",
    "                bb1 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                for c in range(i, len(predictions[0]['boxes'])):\n",
    "                    bb2 = predictions[0]['boxes'][c].detach().numpy()\n",
    "                    if calc_intersection(bb1, bb2) > calc_area(bb1)*IoU:\n",
    "                        if labels[predictions[0]['labels'][i].item()-1] != skip_component and labels[predictions[0]['labels'][c].item()-1] == skip_component: \n",
    "                            dont_print_id.append(c)\n",
    "            if score > threshold:\n",
    "                x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "                \n",
    "                if len(labels) > 1:\n",
    "                    cv2.putText(cv2_img, labels[predictions[0]['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imwrite(save_dir+unique_name+(root_dir.split(\"/\")[-1]), cv2_img)\n",
    "    else:\n",
    "        for image in os.listdir(root_dir):\n",
    "            im = root_dir+image\n",
    "            img = Image.open(im)\n",
    "            cv2_img = cv2.imread(im)\n",
    "            tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "            tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "            predictions = model(tensor_img)\n",
    "            for i in range(len(predictions[0]['boxes'])):\n",
    "                score = predictions[0]['scores'][i].item()\n",
    "                if score > threshold:\n",
    "                    x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                    if len(labels) > 1:\n",
    "                        cv2.putText(cv2_img, labels[predictions[0]['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imwrite(save_dir+unique_name+image, cv2_img)\n",
    "\n",
    "def save_best_bb(pred1, pred2, IoU=0.5):\n",
    "    remove_pred1_ids = []\n",
    "    remove_pred2_ids = []\n",
    "    \n",
    "    # Go through pred1 checking which elements to remove and also pred2\n",
    "    for i in range(len(pred1['boxes'])):\n",
    "        score1 = pred1['scores'][i].item()\n",
    "        if pred1['boxes'][i].device.type == 'cpu':\n",
    "            bb1 = pred1['boxes'][i].detach().numpy()\n",
    "        else:\n",
    "            bb1 = pred1['boxes'][i].cpu().detach().numpy()\n",
    "        for c in range(len(pred2['boxes'])):\n",
    "            score2 = pred2['scores'][c].item()\n",
    "            \n",
    "            if pred2['boxes'][c].device.type == 'cpu':\n",
    "                bb2 = pred2['boxes'][c].detach().numpy()\n",
    "            else:\n",
    "                bb2 = pred2['boxes'][c].cpu().detach().numpy()\n",
    "            intersect = calc_intersection(bb1, bb2)\n",
    "            if intersect > calc_area(bb1)*IoU or intersect > calc_area(bb2)*IoU:\n",
    "                if score1 > score2:\n",
    "                    remove_pred2_ids.append(c)\n",
    "                else:\n",
    "                    remove_pred1_ids.append(i)\n",
    "            \n",
    "    # Remove the elements that should not be in the refined final list\n",
    "    final_pred = {'boxes':[], 'scores':[], 'labels':[]}\n",
    "    for i in range(len(pred1['boxes'])):\n",
    "        if i not in remove_pred1_ids:\n",
    "            final_pred['boxes'].append(pred1['boxes'][i])\n",
    "            final_pred['scores'].append(pred1['scores'][i])\n",
    "            final_pred['labels'].append(pred1['labels'][i])\n",
    "    for i in range(len(pred2['boxes'])):\n",
    "        if i not in remove_pred2_ids:\n",
    "            final_pred['boxes'].append(pred2['boxes'][i])\n",
    "            final_pred['scores'].append(pred2['scores'][i])\n",
    "            final_pred['labels'].append(pred2['labels'][i])\n",
    "    \n",
    "    return final_pred\n",
    "    \n",
    "def frcnn_load_singular_models(model_name, components, root_dir):\n",
    "    models = []\n",
    "    device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    for component in components:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "        num_classes = 2\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        model.to(device)\n",
    "        checkpoint = torch.load(root_dir+'/'+component+'/'+model_name+'.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        \n",
    "    return models\n",
    "\n",
    "\n",
    "def ssd_load_singular_models(model_name, components, root_dir, device):\n",
    "    models = []\n",
    "\n",
    "    for component in components:\n",
    "        model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "        num_classes = 2\n",
    "        in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "        model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "        \n",
    "        model_ssd.to(device)\n",
    "        \n",
    "        checkpoint = torch.load(root_dir+'/'+component+'/'+model_name+'.pt')\n",
    "        model_ssd.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_ssd.eval()\n",
    "        models.append(model_ssd)\n",
    "        \n",
    "    return models\n",
    "\n",
    "def predict_models(models, images, IoU=0.5):\n",
    "    predictions = [0 for k in range(len(images))]\n",
    "    for i in range(len(images)):\n",
    "        preds = []\n",
    "        for model in models:\n",
    "            pred = model([images[i]])\n",
    "            preds.append(pred[0])\n",
    "\n",
    "        for c in range(len(preds)):\n",
    "            preds[c]['labels'] *= (c+1)\n",
    "\n",
    "        final_pred = preds[0]\n",
    "        for c in range(1, len(preds)):\n",
    "            final_pred = save_best_bb(final_pred, preds[c], IoU=IoU)\n",
    "        predictions[i] = final_pred\n",
    "    return predictions\n",
    "    \n",
    "def predict_and_save_models(models, root_dir, save_dir, labels=[], threshold=0.5, IoU=0.5, mask=False):\n",
    "    if root_dir[-4] == '.':\n",
    "        im = root_dir\n",
    "        img = cv2.imread(im)\n",
    "        cv2_img = cv2.imread(im)\n",
    "        if mask:\n",
    "            imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "        tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "        predictions = []\n",
    "        for model in models:\n",
    "            predictions.append(model(tensor_img)[0])\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            predictions[i]['labels'] *= (i+1)\n",
    "            \n",
    "        final_pred = predictions[0]\n",
    "        for i in range(1, len(predictions)):\n",
    "            final_pred = save_best_bb(final_pred, predictions[i], IoU=IoU)\n",
    "            \n",
    "        for i in range(len(final_pred['boxes'])):\n",
    "            score = final_pred['scores'][i].item()\n",
    "            if score > threshold:\n",
    "                x,y,x2,y2 = final_pred['boxes'][i].detach().numpy()\n",
    "                cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                if len(labels) > 1:\n",
    "                    cv2.putText(cv2_img, labels[final_pred['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imwrite(save_dir+(root_dir.split(\"/\")[-1]), cv2_img)\n",
    "    else:\n",
    "        for image in os.listdir(root_dir):\n",
    "            im = root_dir+image\n",
    "            img = Image.open(im)\n",
    "            cv2_img = cv2.imread(im)\n",
    "            tensor_img = transform.to_tensor(img)\n",
    "            tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "            predictions = []\n",
    "            for model in models:\n",
    "                predictions.append(model(tensor_img)[0])\n",
    "\n",
    "            for i in range(len(predictions)):\n",
    "                predictions[i]['labels'] *= (i+1)\n",
    "\n",
    "            final_pred = predictions[0]\n",
    "            for i in range(1, len(predictions)):\n",
    "                final_pred = save_best_bb(final_pred, predictions[i], IoU=IoU)\n",
    "\n",
    "            for i in range(len(final_pred['boxes'])):\n",
    "                score = final_pred['scores'][i].item()\n",
    "                if score > threshold:\n",
    "                    x,y,x2,y2 = final_pred['boxes'][i].detach().numpy()\n",
    "                    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                    if len(labels) > 1:\n",
    "                        cv2.putText(cv2_img, labels[final_pred['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imwrite(save_dir+image, cv2_img)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=False):\n",
    "    batch_size = len(x)\n",
    "    if (batch_size < 2):\n",
    "        return x, [0], y\n",
    "    \n",
    "    # Get a random lambda\n",
    "    if alpha > 0:\n",
    "        lam = numpy.clip(numpy.random.beta(alpha, alpha), 0.4, 0.6)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    # convert tensor array to numpy for mixup\n",
    "    t_x = numpy.empty((batch_size, x[0].size()[1], x[0].size()[2]))\n",
    "    for i in range(len(x)):\n",
    "        t_x[i] = x[i].numpy().astype(numpy.float)\n",
    "    \n",
    "    # convert tuple to numpy array for easier indexing\n",
    "    t_y = numpy.empty((batch_size), dtype=numpy.object)\n",
    "    for i in range(len(y)):\n",
    "        t_y[i] = {}\n",
    "        for var in y[i]:\n",
    "            t_y[i][var] = y[i][var].numpy().astype(numpy.double)\n",
    "        \n",
    "    # Get a random set of indicies\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "    # Mix the two images and make them % transparent based on lambda\n",
    "    t_mixed_x = lam * t_x + (1 - lam) * t_x[index, :]\n",
    "    y_a, y_b = t_y, t_y[index]\n",
    "\n",
    "    # Zip together the bounding boxes of the zipped images\n",
    "    mixedup_bboxes = []\n",
    "    i = 0\n",
    "    for bbox, s_bbox in zip(y_a, y_b):\n",
    "        # If the two images zipped are the same, keep one of the boundingbox infos\n",
    "        if (bbox['boxes'][0] == s_bbox['boxes'][0]).all():\n",
    "            mix_len = len(bbox['boxes'])\n",
    "            mixedup_bboxes.append({'boxes':torch.zeros([mix_len, 4], dtype=torch.double), 'labels':torch.zeros([mix_len], dtype=torch.int64), 'image_id':torch.zeros([1], dtype=torch.int64), 'area':torch.zeros([mix_len], dtype=torch.float), 'iscrowd':torch.zeros([mix_len], dtype=torch.int64)})\n",
    "            for c in range(len(bbox['boxes'])):\n",
    "                mixedup_bboxes[i]['boxes'][c] = torch.from_numpy(bbox['boxes'][c])\n",
    "                mixedup_bboxes[i]['labels'][c] = bbox['labels'][c]\n",
    "                mixedup_bboxes[i]['area'][c] = bbox['area'][c]\n",
    "                mixedup_bboxes[i]['iscrowd'][c] = bbox['iscrowd'][c]\n",
    "            i += 1\n",
    "            continue;\n",
    "            \n",
    "        mix_len = len(bbox['boxes'])+len(s_bbox['boxes'])\n",
    "        mixedup_bboxes.append({'boxes':torch.zeros([mix_len, 4], dtype=torch.double), 'labels':torch.zeros([mix_len], dtype=torch.int64), 'image_id':torch.zeros([1], dtype=torch.int64), 'area':torch.zeros([mix_len], dtype=torch.float), 'iscrowd':torch.zeros([mix_len], dtype=torch.int64)})\n",
    "        for c in range(len(bbox['boxes'])):\n",
    "            mixedup_bboxes[i]['boxes'][c] = torch.from_numpy(bbox['boxes'][c])\n",
    "            mixedup_bboxes[i]['labels'][c] = bbox['labels'][c]\n",
    "            mixedup_bboxes[i]['area'][c] = bbox['area'][c]\n",
    "            mixedup_bboxes[i]['iscrowd'][c] = bbox['iscrowd'][c]\n",
    "        \n",
    "        for j in range(len(s_bbox['boxes'])):\n",
    "            mixedup_bboxes[i]['boxes'][c+j+1] = torch.from_numpy(s_bbox['boxes'][j])\n",
    "            mixedup_bboxes[i]['labels'][c+j+1] = s_bbox['labels'][j]\n",
    "            mixedup_bboxes[i]['area'][c+j+1] = s_bbox['area'][j]\n",
    "            mixedup_bboxes[i]['iscrowd'][c+j+1] = s_bbox['iscrowd'][j]\n",
    "            \n",
    "        mixedup_bboxes[i]['image_id'][0] = bbox['image_id'][0]\n",
    "        i += 1\n",
    "        \n",
    "    mixed_x = []\n",
    "    for v in t_mixed_x:\n",
    "        mixed_x.append(torch.FloatTensor([v]))\n",
    "    \n",
    "    return mixed_x, index, tuple(mixedup_bboxes)\n",
    "\n",
    "# Generates new combined data based on how many components it can fit into the screen without overlapping too much\n",
    "def generate_combined_data(root_folder, max_nr_components=3, IoU=0.05, start_index_filename=0):\n",
    "    os.mkdir('./'+root_folder+'_generated/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/')\n",
    "    os.mkdir('./'+root_folder+'_generated/labels/')\n",
    "    components = os.listdir('./'+root_folder+'/images')\n",
    "    images = []\n",
    "    for component in components:\n",
    "        images.append(os.listdir('./'+root_folder+'/images/'+component))\n",
    "\n",
    "    for i in range(len(components)):\n",
    "        print('Making dataset for:',components[i])\n",
    "        for img in images[i]:\n",
    "            image = []\n",
    "            labels = []\n",
    "            im = cv2.imread('./'+root_folder+'/images/'+components[i]+'/'+img)\n",
    "            imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            ret, image = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "            f = open('./'+root_folder+'/labels/'+img[:-3]+'txt', 'r')\n",
    "            data = f.read().split('\\n')\n",
    "            f.close()\n",
    "            for line in data:\n",
    "                labels.append(str_to_bb(line))\n",
    "\n",
    "            nr_components = 0\n",
    "            components_checked = [i]\n",
    "            for c in range(len(images)):\n",
    "                if max_nr_components == nr_components:\n",
    "                    break;\n",
    "\n",
    "                choices = []\n",
    "                for y in range(len(components)):\n",
    "                    if not y in components_checked:\n",
    "                        choices.append(y)\n",
    "                if len(choices) == 0:\n",
    "                    break;\n",
    "                c = numpy.random.choice(choices)\n",
    "                components_checked.append(c)\n",
    "                \n",
    "                idxs = [y for y in range(len(images[c]))]\n",
    "                random.shuffle(idxs)\n",
    "                for y in idxs:\n",
    "                    f = open('./'+root_folder+'/labels/'+images[c][y][:-3]+'txt', 'r')\n",
    "                    data = f.read().split('\\n')\n",
    "                    f.close()\n",
    "\n",
    "                    intersects = False\n",
    "                    new_labels = labels.copy()\n",
    "                    for line in data:\n",
    "                        line_bb = str_to_bb(line)\n",
    "                        denorm_line_bb = denormalize_bb(line_bb[1:], image.shape[:2])\n",
    "                        for bb in labels:\n",
    "                            denorm_bb = denormalize_bb(bb[1:], image.shape[:2])\n",
    "                            if calc_iou(denorm_bb, denorm_line_bb) > IoU or calc_iou(denorm_line_bb, denorm_bb) > IoU:\n",
    "                                intersects = True\n",
    "                                break;\n",
    "                        if intersects:\n",
    "                            break;\n",
    "                        new_labels.append(line_bb)\n",
    "                    if intersects:\n",
    "                        continue;\n",
    "\n",
    "                    labels = new_labels.copy()\n",
    "                    im2 = cv2.imread('./'+root_folder+'/images/'+components[c]+'/'+images[c][y])\n",
    "                    imgray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "                    ret, image2 = cv2.threshold(imgray2, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "                    image = image+image2\n",
    "                    nr_components += 1\n",
    "                    break;\n",
    "\n",
    "            str_labels = ''\n",
    "            for bb in labels:\n",
    "                str_labels += str(int(bb[0]))+' '+bb_to_str(bb[1:])+'\\n'\n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/'+str(start_index_filename)+'.jpg', image)\n",
    "            f = open('./'+root_folder+'_generated/labels/'+str(start_index_filename)+'.txt', 'w')\n",
    "            f.write(str_labels[:-1])\n",
    "            f.close()\n",
    "            start_index_filename += 1\n",
    "            \n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, root_dir, set_type, single_component=False, combined=False, preprocessed=False):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.component_names = []\n",
    "        if combined:\n",
    "            for image in os.listdir(root_dir+\"/images/\"+set_type):\n",
    "                self.images.append(root_dir+\"/images/\"+set_type+'/'+image)\n",
    "                self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "        elif not single_component:\n",
    "            for component in os.listdir(root_dir+\"/images/\"+set_type):\n",
    "                if component == 'Combined':\n",
    "                    continue\n",
    "                self.component_names.append(component)\n",
    "                for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+component):\n",
    "                    self.images.append(root_dir+\"/images/\"+set_type+\"/\"+component+\"/\"+image)\n",
    "                    self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "        else:\n",
    "            self.component_names.append(single_component)\n",
    "            for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+single_component):\n",
    "                self.images.append(root_dir+\"/images/\"+set_type+\"/\"+single_component+\"/\"+image)\n",
    "                self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "            \n",
    "        self.root = root_dir\n",
    "        self.single_component = single_component\n",
    "        self.combined = combined\n",
    "        self.preprocessed = preprocessed\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im = cv2.imread(self.images[idx])\n",
    "        if not self.preprocessed:\n",
    "            imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        else:\n",
    "            img = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        f = open(self.labels[idx], \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "\n",
    "        N = len(data)\n",
    "        boxes = torch.zeros([N, 4], dtype=torch.double)\n",
    "        labels = torch.zeros([N], dtype=torch.int64)\n",
    "        areas = torch.zeros([N])\n",
    "        \n",
    "        for i in range(N):\n",
    "            bb = denormalize_bb(str_to_bb(data[i])[1:], img.shape[:2])\n",
    "            boxes[i][0],boxes[i][1],boxes[i][2],boxes[i][3] = bb\n",
    "            areas[i] = calc_area(bb)\n",
    "            \n",
    "            if not self.single_component:\n",
    "                labels[i] = int(data[i].split(' ')[0])+1\n",
    "                continue\n",
    "                \n",
    "            labels[i] = 1\n",
    "                \n",
    "        return transform.to_tensor(img), {'boxes':boxes, 'labels':labels, 'image_id':torch.LongTensor([idx]), 'area':areas, 'iscrowd':torch.zeros([N], dtype=torch.int64)}\n",
    "\n",
    "def train_model(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler=False, folder_name=datetime.datetime.now().strftime(\"%b-%d_%H-%M\"), mixup=False, begin_epoch=0):\n",
    "    writer = SummaryWriter()\n",
    "    total_time = time.time()\n",
    "    \n",
    "    if not os.path.exists('./models/'+model_type+'/'+folder_name+'/'):\n",
    "        os.mkdir('./models/'+model_type+'/'+folder_name+'/')\n",
    "\n",
    "    for epoch in range(begin_epoch, num_epochs):\n",
    "        epoch_time = time.time()\n",
    "        epoch_loss = []\n",
    "        batch_nr = 0\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            batch_time = time.time()\n",
    "            if mixup:\n",
    "                images, _, targets = mixup_data(images, targets)\n",
    "\n",
    "            # Send them to device if using GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            pred = model(images, targets)\n",
    "            losses = sum(loss for loss in pred.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss.append(losses.item())\n",
    "            \n",
    "            writer.add_scalars(model_type+'_'+model_name, {\n",
    "                'train_loss': losses.item(),\n",
    "            }, epoch*len(data_loader)+batch_nr)\n",
    "            \n",
    "            batch_nr = batch_nr + 1\n",
    "            print_loss = losses.item()\n",
    "            \n",
    "            if batch_nr == epoch+1:\n",
    "                print_loss = numpy.average(epoch_loss)\n",
    "                \n",
    "            print(\n",
    "                '\\r[Train] Epoch {} [{}/{}] - Loss: {} \\tProgress [{}%] \\tEpoch time elapsed: {}'.format(\n",
    "                    epoch+1, batch_nr, len(data_loader), print_loss, round(((epoch/num_epochs)+(1/num_epochs*batch_nr/len(data_loader)))*100, 2), str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "        \n",
    "            \n",
    "        writer.add_scalars(model_type+'_'+model_name, {\n",
    "            'avg_epoch_loss': numpy.average(epoch_loss),\n",
    "        }, (epoch+1))\n",
    "            \n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        print()\n",
    "        evaluate_model(model, data_loader_val, device, writer, model_type, model_name, epoch+1)\n",
    "        #model.train()\n",
    "        print()\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, './models/'+model_type+'/'+folder_name+'/'+model_name+'-'+str(epoch+1)+'.pt')\n",
    "\n",
    "    print(\n",
    "        '\\rTraining completed! Loss: {} \\tTotal time elapsed: {}'.format(\n",
    "            losses.item(), str(datetime.timedelta(seconds=round(time.time()-total_time)))\n",
    "        ),\n",
    "        end=''\n",
    "    )\n",
    "    \n",
    "def evaluate_model(model, data_loader, device, writer, model_type, model_name, epoch):\n",
    "    with torch.no_grad():\n",
    "        epoch_time = time.time()\n",
    "        avg_loss = []\n",
    "        batch_nr = 0\n",
    "        for images, targets in data_loader:\n",
    "            # Send them to device if using GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            pred = model(images, targets)\n",
    "            losses = sum(loss for loss in pred.values())\n",
    "            avg_loss.append(losses.item())\n",
    "            \n",
    "            batch_nr = batch_nr + 1\n",
    "            print_loss = losses.item()\n",
    "            \n",
    "            if batch_nr == epoch+1:\n",
    "                print_loss = numpy.average(avg_loss)\n",
    "            print(\n",
    "                '\\r[Val] [{}/{}] - Loss: {} \\tEpoch time elapsed: {}'.format(\n",
    "                    batch_nr, len(data_loader), print_loss, str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "\n",
    "        writer.add_scalars(model_type+'_'+model_name, {\n",
    "            'val_loss': numpy.average(avg_loss),\n",
    "        }, epoch)\n",
    "\n",
    "def train_multi_frcnn(model_name, model_type, epochs, components=[]):\n",
    "    if components == []:\n",
    "        f = open('./dataset/labels.txt', \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "        components = {data[i]:i for i in range(len(data))}\n",
    "\n",
    "    for component in components:\n",
    "        dataset_train = SketchDataset('./dataset', 'train', single_component=component)\n",
    "        dataset_val = SketchDataset('./dataset', 'val', single_component=component)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "                dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "\n",
    "        device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        num_classes = 2\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                       step_size=3,\n",
    "                                                       gamma=0.1)\n",
    "\n",
    "        train_model(model, optimizer, data_loader, data_loader_val, device, epochs, model_type, model_name, lr_scheduler, folder_name=component+'_'+model_name)\n",
    "        \n",
    "def train_multi_ssd(model_name, model_type, epochs, components=[]):\n",
    "    if components == []:\n",
    "        f = open('./dataset/labels.txt', \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "        components = {data[i]:i for i in range(len(data))}\n",
    "\n",
    "    for component in components:\n",
    "        print('Training:',component)\n",
    "        print()\n",
    "        dataset_train = SketchDataset('./dataset', 'train', single_component=component)\n",
    "        dataset_val = SketchDataset('./dataset', 'val', single_component=component)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "                dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "        num_classes = 2\n",
    "        in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "        model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "\n",
    "        model_ssd.to(device)\n",
    "\n",
    "\n",
    "        params = [p for p in model_ssd.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.00005, \n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        # and a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                       step_size=3,\n",
    "                                                       gamma=0.1)\n",
    "        \n",
    "\n",
    "        train_model(model_ssd, optimizer, data_loader, data_loader_val, device, epochs, model_type, model_name, lr_scheduler, folder_name=component+'_'+model_name)\n",
    "    \n",
    "def load_frcnn(date, model_name):\n",
    "    device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    num_classes = 13\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    checkpoint = torch.load('./models/Faster-RCNN/'+date+'/'+model_name+'.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dic(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model,optimizer\n",
    "    \n",
    "def load_ssd(date, model_name):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "    num_classes = 13\n",
    "    in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "    num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "    model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "    model_ssd.to(device)\n",
    "\n",
    "    params = [p for p in model_ssd.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, \n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    checkpoint = torch.load('./models/SSD/'+date+'/'+model_name+'.pt')\n",
    "    model_ssd.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dic(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model_ssd,optimizer\n",
    "\n",
    "def handle_references(results):\n",
    "    return results\n",
    "def post_process_results(results, IoU=0.5):\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c0977",
   "metadata": {},
   "source": [
    "### Generate combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f408d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset for: Button\n",
      "Making dataset for: Checkbox\n",
      "Making dataset for: Combobox\n",
      "Making dataset for: Datagrid\n",
      "Making dataset for: Dropdown\n",
      "Making dataset for: Header\n",
      "Making dataset for: Input\n",
      "Making dataset for: List\n",
      "Making dataset for: Object\n",
      "Making dataset for: Radiobutton\n",
      "Making dataset for: Reference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0:35:41'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_now = time.time()\n",
    "generate_combined_data('temp/all_data_march', max_nr_components=7, start_index_filename=3124)\n",
    "str(datetime.timedelta(seconds=round(time.time()-time_now)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32c262",
   "metadata": {},
   "source": [
    "### Predict and save model based on threshold and IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34dfbe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'Input', 'Button', 'List', 'Datagrid', 'Dropdown', 'Combobox', 'Checkbox', 'Radiobutton', 'Reference', 'Object', 'ReferenceHead']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Eriksson\\anaconda3\\envs\\nnlm\\lib\\site-packages\\ipykernel_launcher.py:395: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "print(labels) \n",
    "predict_and_save(model_retina, './dataset_mAP/images/', './results/all-retina/v8_map/', labels=labels, threshold=0.5, IoU=0, mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e97522",
   "metadata": {},
   "source": [
    "### mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "428e75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:31\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:27\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:28\n",
      "Values @90:\n",
      "8.756188245921875\n",
      "\n",
      "Header \t\t-> 0.0\n",
      "Input \t\t-> 18.484848484848484\n",
      "Button \t\t-> 4.27807486631016\n",
      "List \t\t-> 28.98989898989899\n",
      "Datagrid \t-> 14.498834498834501\n",
      "Dropdown \t-> 11.255411255411255\n",
      "Combobox \t-> 13.363636363636363\n",
      "Checkbox \t-> 0.0\n",
      "Radiobutton \t-> 9.090909090909092\n",
      "Reference \t-> 0.0\n",
      "Object \t\t-> 5.112645401213658\n",
      "ReferenceHead \t-> 0.0\n",
      "\n",
      "Values @75:\n",
      "40.09377861697465\n",
      "\n",
      "Header \t\t-> 0.0\n",
      "Input \t\t-> 75.25381923617523\n",
      "Button \t\t-> 72.72727272727273\n",
      "List \t\t-> 46.51230788985441\n",
      "Datagrid \t-> 30.797422274695002\n",
      "Dropdown \t-> 74.81485141652607\n",
      "Combobox \t-> 83.46977096977099\n",
      "Checkbox \t-> 30.89572192513369\n",
      "Radiobutton \t-> 29.350840336134453\n",
      "Reference \t-> 2.840909090909091\n",
      "Object \t\t-> 23.50647852408416\n",
      "ReferenceHead \t-> 10.95594901313993\n",
      "\n",
      "Values @50:\n",
      "49.946678353157445\n",
      "\n",
      "Header \t\t-> 28.530105300770913\n",
      "Input \t\t-> 75.25381923617523\n",
      "Button \t\t-> 72.72727272727273\n",
      "List \t\t-> 46.51230788985441\n",
      "Datagrid \t-> 30.797422274695002\n",
      "Dropdown \t-> 74.81485141652607\n",
      "Combobox \t-> 83.46977096977099\n",
      "Checkbox \t-> 36.36363636363637\n",
      "Radiobutton \t-> 90.9090909090909\n",
      "Reference \t-> 19.84848484848485\n",
      "Object \t\t-> 23.50647852408416\n",
      "ReferenceHead \t-> 16.626899777527683\n"
     ]
    }
   ],
   "source": [
    "# This was evaluating the single network that trains on single components\n",
    "\n",
    "dataset_val = SketchDataset('./dataset_mAP', '', combined=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "#IoU = 0.75\n",
    "model.eval()\n",
    "\n",
    "frcnn_metric90 = calc_map(model_single, data_loader_val, torch.device('cpu'), 13, IoU=0.9)\n",
    "print()\n",
    "frcnn_metric75 = calc_map(model_single, data_loader_val, torch.device('cpu'), 13, IoU=0.75)\n",
    "print()\n",
    "frcnn_metric50 = calc_map(model_single, data_loader_val, device, 13, IoU=0.5)\n",
    "#print()\n",
    "#frcnn_combined_metric90 = calc_map(combined_frcnn, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "#ssd_combined_metric90 = calc_map(combined_ssd, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "print()\n",
    "print('Values @90:')\n",
    "print(frcnn_metric90[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric90[0][i])\n",
    "\n",
    "print()\n",
    "print('Values @75:')\n",
    "print(frcnn_metric75[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric75[0][i])\n",
    "\n",
    "print()\n",
    "print('Values @50:')\n",
    "print(frcnn_metric50[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric50[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc0d6581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:30\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:27\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:27\n",
      "Values @90:\n",
      "4.940642320241888\n",
      "\n",
      "Header \t\t-> 0.0\n",
      "Input \t\t-> 10.795454545454545\n",
      "Button \t\t-> 0.0\n",
      "List \t\t-> 12.662337662337661\n",
      "Datagrid \t-> 20.458077276259097\n",
      "Dropdown \t-> 2.1212121212121215\n",
      "Combobox \t-> 11.395338668065941\n",
      "Checkbox \t-> 0.0\n",
      "Radiobutton \t-> 0.0\n",
      "Reference \t-> 0.0\n",
      "Object \t\t-> 1.855287569573284\n",
      "ReferenceHead \t-> 0.0\n",
      "\n",
      "Values @75:\n",
      "33.56378356901082\n",
      "\n",
      "Header \t\t-> 9.090909090909092\n",
      "Input \t\t-> 40.71087976226316\n",
      "Button \t\t-> 54.7068428647376\n",
      "List \t\t-> 72.7730411374612\n",
      "Datagrid \t-> 56.66666666666666\n",
      "Dropdown \t-> 37.471164769773715\n",
      "Combobox \t-> 52.23120134761088\n",
      "Checkbox \t-> 33.608815426997246\n",
      "Radiobutton \t-> 27.737458619811562\n",
      "Reference \t-> 0.0\n",
      "Object \t\t-> 10.42759324009324\n",
      "ReferenceHead \t-> 7.3408299018055105\n",
      "\n",
      "Values @50:\n",
      "50.772714410379486\n",
      "\n",
      "Header \t\t-> 49.61436728988911\n",
      "Input \t\t-> 47.873952960517315\n",
      "Button \t\t-> 77.35834154679372\n",
      "List \t\t-> 83.27537311013488\n",
      "Datagrid \t-> 56.66666666666666\n",
      "Dropdown \t-> 37.471164769773715\n",
      "Combobox \t-> 52.23120134761088\n",
      "Checkbox \t-> 54.54545454545454\n",
      "Radiobutton \t-> 63.63636363636363\n",
      "Reference \t-> 30.401416765053124\n",
      "Object \t\t-> 15.213510125447504\n",
      "ReferenceHead \t-> 40.984760160848644\n"
     ]
    }
   ],
   "source": [
    "# This evaluation was made on a model that used combined data\n",
    "\n",
    "dataset_val = SketchDataset('./dataset_mAP', '', combined=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "#IoU = 0.75\n",
    "model.eval()\n",
    "\n",
    "frcnn_metric90 = calc_map(model, data_loader_val, torch.device('cpu'), 13, IoU=0.9)\n",
    "print()\n",
    "frcnn_metric75 = calc_map(model, data_loader_val, torch.device('cpu'), 13, IoU=0.75)\n",
    "print()\n",
    "frcnn_metric50 = calc_map(model, data_loader_val, device, 13, IoU=0.5)\n",
    "#print()\n",
    "#frcnn_combined_metric90 = calc_map(combined_frcnn, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "#ssd_combined_metric90 = calc_map(combined_ssd, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "print()\n",
    "print('Values @90:')\n",
    "print(frcnn_metric90[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric90[0][i])\n",
    "\n",
    "print()\n",
    "print('Values @75:')\n",
    "print(frcnn_metric75[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric75[0][i])\n",
    "\n",
    "print()\n",
    "print('Values @50:')\n",
    "print(frcnn_metric50[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric50[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae42f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Eriksson\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:33\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:29\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:04:29\n",
      "Values @90:\n",
      "1.1521464646464645\n",
      "\n",
      "Header \t\t-> 0.0\n",
      "Input \t\t-> 0.0\n",
      "Button \t\t-> 0.0\n",
      "List \t\t-> 0.0\n",
      "Datagrid \t-> 12.12121212121212\n",
      "Dropdown \t-> 1.7045454545454544\n",
      "Combobox \t-> 0.0\n",
      "Checkbox \t-> 0.0\n",
      "Radiobutton \t-> 0.0\n",
      "Reference \t-> 0.0\n",
      "Object \t\t-> 0.0\n",
      "ReferenceHead \t-> 0.0\n",
      "\n",
      "Values @75:\n",
      "30.716062972690015\n",
      "\n",
      "Header \t\t-> 0.0\n",
      "Input \t\t-> 44.0311826669088\n",
      "Button \t\t-> 18.181818181818183\n",
      "List \t\t-> 37.87419651056015\n",
      "Datagrid \t-> 86.09658893033786\n",
      "Dropdown \t-> 42.10376492194675\n",
      "Combobox \t-> 73.16993747940039\n",
      "Checkbox \t-> 44.54545454545455\n",
      "Radiobutton \t-> 11.436950146627565\n",
      "Reference \t-> 0.0\n",
      "Object \t\t-> 9.499969727242453\n",
      "ReferenceHead \t-> 1.6528925619834711\n",
      "\n",
      "Values @50:\n",
      "49.8419526628721\n",
      "\n",
      "Header \t\t-> 36.36363636363637\n",
      "Input \t\t-> 51.75385109479878\n",
      "Button \t\t-> 18.181818181818183\n",
      "List \t\t-> 73.80998512994582\n",
      "Datagrid \t-> 100.0\n",
      "Dropdown \t-> 58.089413600823335\n",
      "Combobox \t-> 83.46977096977099\n",
      "Checkbox \t-> 54.54545454545454\n",
      "Radiobutton \t-> 63.63636363636363\n",
      "Reference \t-> 23.18181818181818\n",
      "Object \t\t-> 14.028682836323641\n",
      "ReferenceHead \t-> 21.04263741371173\n"
     ]
    }
   ],
   "source": [
    "# This evaluation was made on 1 single epoch from a gigantic combined dataset\n",
    "\n",
    "dataset_val = SketchDataset('./dataset_mAP', '', combined=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "#IoU = 0.75\n",
    "\n",
    "frcnn_metric90 = calc_map(model_giga, data_loader_val, torch.device('cpu'), 13, IoU=0.9)\n",
    "print()\n",
    "frcnn_metric75 = calc_map(model_giga, data_loader_val, torch.device('cpu'), 13, IoU=0.75)\n",
    "print()\n",
    "frcnn_metric50 = calc_map(model_giga, data_loader_val, device, 13, IoU=0.5)\n",
    "#print()\n",
    "#frcnn_combined_metric90 = calc_map(combined_frcnn, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "#ssd_combined_metric90 = calc_map(combined_ssd, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "print()\n",
    "print('Values @90:')\n",
    "print(frcnn_metric90[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric90[0][i])\n",
    "\n",
    "print()\n",
    "print('Values @75:')\n",
    "print(frcnn_metric75[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric75[0][i])\n",
    "\n",
    "print()\n",
    "print('Values @50:')\n",
    "print(frcnn_metric50[1])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric50[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d8e6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] mAP [79/79]\tEpoch time elapsed: 0:04:18\n",
      "Values:\n",
      "37.07613552521579\n",
      "[45.45454545454545, 47.59099152813845, 20.299586776859506, 55.90242284092552, 44.40356969812863, 28.603930828529755, 45.65400285988522, 56.364126070008425, 2.6392961876832843, 7.142857142857142, 43.760308166841746, 47.097988748186374]\n",
      "\n",
      "Header \t\t-> 45.45454545454545\n",
      "Input \t\t-> 47.59099152813845\n",
      "Button \t\t-> 20.299586776859506\n",
      "List \t\t-> 55.90242284092552\n",
      "Datagrid \t-> 44.40356969812863\n",
      "Dropdown \t-> 28.603930828529755\n",
      "Combobox \t-> 45.65400285988522\n",
      "Checkbox \t-> 56.364126070008425\n",
      "Radiobutton \t-> 2.6392961876832843\n",
      "Reference \t-> 7.142857142857142\n",
      "Object \t\t-> 43.760308166841746\n",
      "ReferenceHead \t-> 47.097988748186374\n",
      "Values:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'frcnn_metric90' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-04ea6cbc001b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Values:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrcnn_metric90\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrcnn_metric90\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frcnn_metric90' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation of retina net for combined data \n",
    "\n",
    "dataset_val = SketchDataset('./dataset_mAP', '', combined=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "IoU = 0.50\n",
    "model_retina.eval()\n",
    "\n",
    "frcnn_metric902 = calc_map(model_retina, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "print()\n",
    "#ssd_metric90 = calc_map(model_ssd, data_loader_val, device, 13, IoU=IoU)\n",
    "#print()\n",
    "#frcnn_combined_metric90 = calc_map(combined_frcnn, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "#ssd_combined_metric90 = calc_map(combined_ssd, data_loader_val, torch.device('cpu'), 13, IoU=IoU)\n",
    "#print()\n",
    "\n",
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "print('Values:')\n",
    "print(frcnn_metric902[1])\n",
    "print(frcnn_metric902[0])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric902[0][i])\n",
    "\n",
    "\n",
    "print('Values:')\n",
    "print(frcnn_metric90[1])\n",
    "print(frcnn_metric90[0])\n",
    "print()\n",
    "\n",
    "formating = [2,2,2,2,1,1,1,1,1,1,2,1]\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],'\\t'*formating[i]+'->',frcnn_metric90[0][i])\n",
    "#print(ssd_metric90[1])\n",
    "#print(frcnn_combined_metric90[1])\n",
    "\n",
    "#print(ssd_combined_metric90[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa439f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Eriksson\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] mAP [298/298]\tEpoch time elapsed: 0:02:14([0.0, 53.57175087190761, 33.182628423270124, 53.59379750684098, 64.45360630313608, 15.18595041322314, 54.76190476190476, 53.261637352546444, 19.138941866214594, 79.11838819941586, 0, 0], 35.52238380820497)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation for ssd\n",
    "\n",
    "dataset_val = SketchDataset('./dataset', 'val', combined=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "model_metric90 = calc_map(model_ssd, data_loader_val, device, 13, IoU=0.9)\n",
    "print(model_metric90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69d93c",
   "metadata": {},
   "source": [
    "### mAP on many-network combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fba83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models\n"
     ]
    }
   ],
   "source": [
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = [data[i] for i in range(len(data))]\n",
    "device = torch.device('cpu') #torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "models_frcnn = frcnn_load_singular_models('v5_SGD-StepLR-5', labels[:-2], './models/Faster-RCNN/Singular_v5')\n",
    "combined_frcnn = lambda images: predict_models(models_frcnn, images)\n",
    "print('Loaded models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7f0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] mAP [40/40]\tEpoch time elapsed: 1:05:18\n",
      "[Eval] mAP [40/40]\tEpoch time elapsed: 1:04:52\n",
      "[Eval] mAP [21/40]\tEpoch time elapsed: 0:36:57"
     ]
    }
   ],
   "source": [
    "dataset_val = SketchDataset('./dataset_mAP', '', combined=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "model_metric90_frcnn = calc_map(combined_frcnn, data_loader_val, device, 13, IoU=0.9)\n",
    "print()\n",
    "model_metric75_frcnn = calc_map(combined_frcnn, data_loader_val, device, 13, IoU=0.75)\n",
    "print()\n",
    "model_metric50_frcnn = calc_map(combined_frcnn, data_loader_val, device, 13, IoU=0.5)\n",
    "print()\n",
    "print(model_metric90_frcnn)\n",
    "print(model_metric75_frcnn)\n",
    "print(model_metric50_frcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1552c021",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-20e8a01ebc37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mrot_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mrot_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'rotated_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill, resample)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[0mcenter_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[0;32m   2154\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2156\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAFFINE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfillcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   2496\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2497\u001b[0m             im.__transformer(\n\u001b[1;32m-> 2498\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillcolor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2499\u001b[0m             )\n\u001b[0;32m   2500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m__transformer\u001b[1;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[0;32m   2572\u001b[0m             \u001b[0mresample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2574\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root_dir = './temp/object_rotated/Object/'\n",
    "for image in os.listdir(root_dir):\n",
    "    im = root_dir+image\n",
    "    img = Image.open(im)\n",
    "    for i in range(-3, 4):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        rot_img = torchvision.transforms.functional.rotate(img, i, fill=255)\n",
    "        rot_img.save(root_dir+'rotated_'+str(i)+'_'+image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a61841",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319601f0",
   "metadata": {},
   "source": [
    "### Train multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87dca30d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Header\n",
      "\n",
      "[Train] Epoch 1 [21/21] - Loss: 7.545849323272705 \tProgress [20.0%] \tEpoch time elapsed: 0:00:5352\n",
      "[Val] [12/12] - Loss: 4.624712944030762 \tEpoch time elapsed: 0:00:11\n",
      "[Train] Epoch 2 [21/21] - Loss: 5.076709747314453 \tProgress [40.0%] \tEpoch time elapsed: 0:00:5172\n",
      "[Val] [12/12] - Loss: 3.2668211460113525 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 3 [21/21] - Loss: 3.775595188140869 \tProgress [60.0%] \tEpoch time elapsed: 0:00:5049\n",
      "[Val] [12/12] - Loss: 3.1986827850341797 \tEpoch time elapsed: 0:00:09\n",
      "[Train] Epoch 4 [21/21] - Loss: 3.2427618503570557 \tProgress [80.0%] \tEpoch time elapsed: 0:00:508\n",
      "[Val] [12/12] - Loss: 2.826225757598877 \tEpoch time elapsed: 0:00:098\n",
      "[Train] Epoch 5 [21/21] - Loss: 2.7989330291748047 \tProgress [100.0%] \tEpoch time elapsed: 0:00:49\n",
      "[Val] [12/12] - Loss: 2.8225202560424805 \tEpoch time elapsed: 0:00:09\n",
      "Training completed! Loss: 2.7989330291748047 \tTotal time elapsed: 0:05:03Training: Input\n",
      "\n",
      "[Train] Epoch 1 [23/23] - Loss: 4.750545024871826 \tProgress [20.0%] \tEpoch time elapsed: 0:01:0052\n",
      "[Val] [13/13] - Loss: 3.5456314086914062 \tEpoch time elapsed: 0:00:11\n",
      "[Train] Epoch 2 [23/23] - Loss: 2.92643404006958 \tProgress [40.0%] \tEpoch time elapsed: 0:00:54520\n",
      "[Val] [13/13] - Loss: 2.962364435195923 \tEpoch time elapsed: 0:00:109\n",
      "[Train] Epoch 3 [23/23] - Loss: 2.7767670154571533 \tProgress [60.0%] \tEpoch time elapsed: 0:00:540\n",
      "[Val] [13/13] - Loss: 2.734586477279663 \tEpoch time elapsed: 0:00:108\n",
      "[Train] Epoch 4 [23/23] - Loss: 2.5289342403411865 \tProgress [80.0%] \tEpoch time elapsed: 0:00:553\n",
      "[Val] [13/13] - Loss: 2.7330245971679688 \tEpoch time elapsed: 0:00:11\n",
      "[Train] Epoch 5 [23/23] - Loss: 2.7441492080688477 \tProgress [100.0%] \tEpoch time elapsed: 0:00:55\n",
      "[Val] [13/13] - Loss: 2.7584280967712402 \tEpoch time elapsed: 0:00:10\n",
      "Training completed! Loss: 2.7441492080688477 \tTotal time elapsed: 0:05:33Training: Button\n",
      "\n",
      "[Train] Epoch 1 [23/23] - Loss: 3.611475944519043 \tProgress [20.0%] \tEpoch time elapsed: 0:00:5656\n",
      "[Val] [13/13] - Loss: 4.265611171722412 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 2 [23/23] - Loss: 2.8938117027282715 \tProgress [40.0%] \tEpoch time elapsed: 0:00:532\n",
      "[Val] [13/13] - Loss: 3.4769287109375 \tEpoch time elapsed: 0:00:10099\n",
      "[Train] Epoch 3 [23/23] - Loss: 2.5301458835601807 \tProgress [60.0%] \tEpoch time elapsed: 0:00:532\n",
      "[Val] [13/13] - Loss: 3.2897071838378906 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 4 [23/23] - Loss: 2.6637675762176514 \tProgress [80.0%] \tEpoch time elapsed: 0:00:530\n",
      "[Val] [13/13] - Loss: 3.216032028198242 \tEpoch time elapsed: 0:00:100\n",
      "[Train] Epoch 5 [23/23] - Loss: 2.656412363052368 \tProgress [100.0%] \tEpoch time elapsed: 0:00:532\n",
      "[Val] [13/13] - Loss: 3.2130014896392822 \tEpoch time elapsed: 0:00:10\n",
      "Training completed! Loss: 2.656412363052368 \tTotal time elapsed: 0:05:19Training: List\n",
      "\n",
      "[Train] Epoch 1 [22/22] - Loss: 3.8591079711914062 \tProgress [20.0%] \tEpoch time elapsed: 0:00:543\n",
      "[Val] [12/12] - Loss: 3.8278965950012207 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 2 [22/22] - Loss: 3.3156216144561768 \tProgress [40.0%] \tEpoch time elapsed: 0:00:507\n",
      "[Val] [12/12] - Loss: 3.221423387527466 \tEpoch time elapsed: 0:00:09\n",
      "[Train] Epoch 3 [22/22] - Loss: 3.1615407466888428 \tProgress [60.0%] \tEpoch time elapsed: 0:00:521\n",
      "[Val] [12/12] - Loss: 2.9184913635253906 \tEpoch time elapsed: 0:00:09\n",
      "[Train] Epoch 4 [22/22] - Loss: 2.6655309200286865 \tProgress [80.0%] \tEpoch time elapsed: 0:00:521\n",
      "[Val] [12/12] - Loss: 2.880854368209839 \tEpoch time elapsed: 0:00:098\n",
      "[Train] Epoch 5 [22/22] - Loss: 2.7705459594726562 \tProgress [100.0%] \tEpoch time elapsed: 0:00:49\n",
      "[Val] [12/12] - Loss: 2.8552463054656982 \tEpoch time elapsed: 0:00:09\n",
      "Training completed! Loss: 2.7705459594726562 \tTotal time elapsed: 0:05:08Training: Datagrid\n",
      "\n",
      "[Train] Epoch 1 [24/24] - Loss: 3.618830919265747 \tProgress [20.0%] \tEpoch time elapsed: 0:00:5769\n",
      "[Val] [14/14] - Loss: 3.714689254760742 \tEpoch time elapsed: 0:00:100\n",
      "[Train] Epoch 2 [24/24] - Loss: 3.6099839210510254 \tProgress [40.0%] \tEpoch time elapsed: 0:00:530\n",
      "[Val] [14/14] - Loss: 3.0689785480499268 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 3 [24/24] - Loss: 2.558154344558716 \tProgress [60.0%] \tEpoch time elapsed: 0:00:5453\n",
      "[Val] [14/14] - Loss: 2.7498741149902344 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 4 [24/24] - Loss: 2.734078884124756 \tProgress [80.0%] \tEpoch time elapsed: 0:00:5320\n",
      "[Val] [14/14] - Loss: 2.7372353076934814 \tEpoch time elapsed: 0:00:10\n",
      "[Train] Epoch 5 [24/24] - Loss: 3.069439172744751 \tProgress [100.0%] \tEpoch time elapsed: 0:00:530\n",
      "[Val] [14/14] - Loss: 2.7277870178222656 \tEpoch time elapsed: 0:00:09\n",
      "Training completed! Loss: 3.069439172744751 \tTotal time elapsed: 0:05:23Training: Dropdown\n",
      "\n",
      "[Train] Epoch 1 [18/18] - Loss: 3.8518152236938477 \tProgress [20.0%] \tEpoch time elapsed: 0:00:42\n",
      "[Val] [10/10] - Loss: 3.9827940464019775 \tEpoch time elapsed: 0:00:08\n",
      "[Train] Epoch 2 [18/18] - Loss: 3.2577223777770996 \tProgress [40.0%] \tEpoch time elapsed: 0:00:409\n",
      "[Val] [10/10] - Loss: 3.4108846187591553 \tEpoch time elapsed: 0:00:07\n",
      "[Train] Epoch 3 [18/18] - Loss: 2.9590508937835693 \tProgress [60.0%] \tEpoch time elapsed: 0:00:405\n",
      "[Val] [10/10] - Loss: 3.1116726398468018 \tEpoch time elapsed: 0:00:07\n",
      "[Train] Epoch 4 [18/18] - Loss: 2.707118272781372 \tProgress [80.0%] \tEpoch time elapsed: 0:00:4097\n",
      "[Val] [10/10] - Loss: 2.963653326034546 \tEpoch time elapsed: 0:00:08\n",
      "[Train] Epoch 5 [18/18] - Loss: 2.548184871673584 \tProgress [100.0%] \tEpoch time elapsed: 0:00:409\n",
      "[Val] [10/10] - Loss: 2.978447675704956 \tEpoch time elapsed: 0:00:07\n",
      "Training completed! Loss: 2.548184871673584 \tTotal time elapsed: 0:04:01Training: Combobox\n",
      "\n",
      "[Train] Epoch 1 [18/18] - Loss: 4.073362350463867 \tProgress [20.0%] \tEpoch time elapsed: 0:00:421\n",
      "[Val] [10/10] - Loss: 3.5602962970733643 \tEpoch time elapsed: 0:00:08\n",
      "[Train] Epoch 2 [18/18] - Loss: 2.9883177280426025 \tProgress [40.0%] \tEpoch time elapsed: 0:00:407\n",
      "[Val] [10/10] - Loss: 2.9765472412109375 \tEpoch time elapsed: 0:00:07\n",
      "[Train] Epoch 3 [18/18] - Loss: 2.600734233856201 \tProgress [60.0%] \tEpoch time elapsed: 0:00:4097\n",
      "[Val] [10/10] - Loss: 2.7005763053894043 \tEpoch time elapsed: 0:00:07\n",
      "[Train] Epoch 4 [18/18] - Loss: 2.693868398666382 \tProgress [80.0%] \tEpoch time elapsed: 0:00:4097\n",
      "[Val] [10/10] - Loss: 2.60864520072937 \tEpoch time elapsed: 0:00:075\n",
      "[Train] Epoch 5 [18/18] - Loss: 2.6285786628723145 \tProgress [100.0%] \tEpoch time elapsed: 0:00:40\n",
      "[Val] [10/10] - Loss: 2.5792524814605713 \tEpoch time elapsed: 0:00:07\n",
      "Training completed! Loss: 2.6285786628723145 \tTotal time elapsed: 0:04:00Training: Checkbox\n",
      "\n",
      "[Train] Epoch 1 [12/12] - Loss: 4.4447126388549805 \tProgress [20.0%] \tEpoch time elapsed: 0:00:284\n",
      "[Val] [7/7] - Loss: 4.228935718536377 \tEpoch time elapsed: 0:00:052\n",
      "[Train] Epoch 2 [12/12] - Loss: 3.5021848678588867 \tProgress [40.0%] \tEpoch time elapsed: 0:00:265\n",
      "[Val] [7/7] - Loss: 3.4976792335510254 \tEpoch time elapsed: 0:00:05\n",
      "[Train] Epoch 3 [12/12] - Loss: 3.1589527130126953 \tProgress [60.0%] \tEpoch time elapsed: 0:00:262\n",
      "[Val] [7/7] - Loss: 3.1319193840026855 \tEpoch time elapsed: 0:00:05\n",
      "[Train] Epoch 4 [12/12] - Loss: 3.047482490539551 \tProgress [80.0%] \tEpoch time elapsed: 0:00:265\n",
      "[Val] [7/7] - Loss: 3.082076072692871 \tEpoch time elapsed: 0:00:054\n",
      "[Train] Epoch 5 [12/12] - Loss: 3.0276081562042236 \tProgress [100.0%] \tEpoch time elapsed: 0:00:26\n",
      "[Val] [7/7] - Loss: 3.050676107406616 \tEpoch time elapsed: 0:00:054\n",
      "Training completed! Loss: 3.0276081562042236 \tTotal time elapsed: 0:02:39Training: Radiobutton\n",
      "\n",
      "[Train] Epoch 1 [11/11] - Loss: 4.496007919311523 \tProgress [20.0%] \tEpoch time elapsed: 0:00:254\n",
      "[Val] [6/6] - Loss: 4.398093223571777 \tEpoch time elapsed: 0:00:05\n",
      "[Train] Epoch 2 [11/11] - Loss: 3.340782403945923 \tProgress [40.0%] \tEpoch time elapsed: 0:00:243\n",
      "[Val] [6/6] - Loss: 3.285078525543213 \tEpoch time elapsed: 0:00:044\n",
      "[Train] Epoch 3 [11/11] - Loss: 2.922410249710083 \tProgress [60.0%] \tEpoch time elapsed: 0:00:2423\n",
      "[Val] [6/6] - Loss: 2.8930537700653076 \tEpoch time elapsed: 0:00:04\n",
      "[Train] Epoch 4 [11/11] - Loss: 3.0483124256134033 \tProgress [80.0%] \tEpoch time elapsed: 0:00:243\n",
      "[Val] [6/6] - Loss: 2.85617995262146 \tEpoch time elapsed: 0:00:0443\n",
      "[Train] Epoch 5 [11/11] - Loss: 2.880324363708496 \tProgress [100.0%] \tEpoch time elapsed: 0:00:24\n",
      "[Val] [6/6] - Loss: 2.825627565383911 \tEpoch time elapsed: 0:00:041\n",
      "Training completed! Loss: 2.880324363708496 \tTotal time elapsed: 0:02:27Training: Reference\n",
      "\n",
      "[Train] Epoch 1 [49/49] - Loss: 3.2200374603271484 \tProgress [20.0%] \tEpoch time elapsed: 0:01:473\n",
      "[Val] [29/29] - Loss: 3.1793758869171143 \tEpoch time elapsed: 0:00:21\n",
      "[Train] Epoch 2 [49/49] - Loss: 2.8615903854370117 \tProgress [40.0%] \tEpoch time elapsed: 0:01:431\n",
      "[Val] [29/29] - Loss: 2.6311984062194824 \tEpoch time elapsed: 0:00:20\n",
      "[Train] Epoch 3 [49/49] - Loss: 2.5322370529174805 \tProgress [60.0%] \tEpoch time elapsed: 0:01:471\n",
      "[Val] [29/29] - Loss: 2.317511558532715 \tEpoch time elapsed: 0:00:198\n",
      "[Train] Epoch 4 [49/49] - Loss: 2.4722325801849365 \tProgress [80.0%] \tEpoch time elapsed: 0:01:419\n",
      "[Val] [29/29] - Loss: 2.2763772010803223 \tEpoch time elapsed: 0:00:19\n",
      "[Train] Epoch 5 [49/49] - Loss: 2.1979563236236572 \tProgress [100.0%] \tEpoch time elapsed: 0:01:42\n",
      "[Val] [29/29] - Loss: 2.2510573863983154 \tEpoch time elapsed: 0:00:19\n",
      "Training completed! Loss: 2.1979563236236572 \tTotal time elapsed: 0:10:19Training: Object\n",
      "\n",
      "[Train] Epoch 1 [36/36] - Loss: 3.334989070892334 \tProgress [20.0%] \tEpoch time elapsed: 0:01:2208\n",
      "[Val] [21/21] - Loss: 3.1222357749938965 \tEpoch time elapsed: 0:00:15\n",
      "[Train] Epoch 2 [36/36] - Loss: 2.490601062774658 \tProgress [40.0%] \tEpoch time elapsed: 0:01:1867\n",
      "[Val] [21/21] - Loss: 2.448803186416626 \tEpoch time elapsed: 0:00:144\n",
      "[Train] Epoch 3 [36/36] - Loss: 2.1686935424804688 \tProgress [60.0%] \tEpoch time elapsed: 0:01:209\n",
      "[Val] [21/21] - Loss: 2.128187656402588 \tEpoch time elapsed: 0:00:144\n",
      "[Train] Epoch 4 [36/36] - Loss: 2.1112220287323 \tProgress [80.0%] \tEpoch time elapsed: 0:01:211:19\n",
      "[Val] [21/21] - Loss: 2.0864346027374268 \tEpoch time elapsed: 0:00:15\n",
      "[Train] Epoch 5 [36/36] - Loss: 2.115767478942871 \tProgress [100.0%] \tEpoch time elapsed: 0:01:204\n",
      "[Val] [21/21] - Loss: 2.06844162940979 \tEpoch time elapsed: 0:00:1543\n",
      "Training completed! Loss: 2.115767478942871 \tTotal time elapsed: 0:07:56Training: ReferenceHead\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './dataset/images/train/ReferenceHead'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-ff5fbf71ee3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_multi_ssd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'v5_SGD-StepLR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SSD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-4feb8fb4c7a2>\u001b[0m in \u001b[0;36mtrain_multi_ssd\u001b[1;34m(model_name, model_type, epochs, components)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mdataset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSketchDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./dataset'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_component\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mdataset_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSketchDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./dataset'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_component\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         data_loader = torch.utils.data.DataLoader(\n",
      "\u001b[1;32m<ipython-input-93-4feb8fb4c7a2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root_dir, set_type, single_component, combined)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_component\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/images/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mset_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msingle_component\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/images/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mset_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msingle_component\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/labels/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './dataset/images/train/ReferenceHead'"
     ]
    }
   ],
   "source": [
    "train_multi_ssd('v5_SGD-StepLR', 'SSD', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6c8b1f5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-acca922924e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_multi_frcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'v5_SGD-StepLR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Faster-RCNN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Radiobutton'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Reference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-b4daebd639bd>\u001b[0m in \u001b[0;36mtrain_multi_frcnn\u001b[1;34m(model_name, model_type, epochs, components)\u001b[0m\n\u001b[0;32m    184\u001b[0m                                                        gamma=0.1)\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_multi_ssd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-b4daebd639bd>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler, folder_name)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mbatch_nr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-b4daebd639bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mimgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_BINARY_INV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_multi_frcnn('v5_SGD-StepLR', 'Faster-RCNN', 5, components=['Radiobutton', 'Reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f762e6a",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ddb4222",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2 [1268/1268] - Loss: 0.36831164360046387 \tProgress [20.0%] \tEpoch time elapsed: 3:53:1329\n",
      "[Val] [150/150] - Loss: 0.18582996726036072 \tEpoch time elapsed: 0:20:24\n",
      "[Train] Epoch 3 [1268/1268] - Loss: 0.11197745054960251 \tProgress [30.0%] \tEpoch time elapsed: 3:48:5106\n",
      "[Val] [150/150] - Loss: 0.15591025352478027 \tEpoch time elapsed: 0:20:222\n",
      "[Train] Epoch 4 [1268/1268] - Loss: 0.07354184240102768 \tProgress [40.0%] \tEpoch time elapsed: 3:49:2313\n",
      "[Val] [150/150] - Loss: 0.07237938046455383 \tEpoch time elapsed: 0:20:235\n",
      "[Train] Epoch 5 [55/1268] - Loss: 0.01926172338426113 \tProgress [40.43%] \tEpoch time elapsed: 0:09:565"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-16f6b569a916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Faster-RCNN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'All-Comp_v6_SGD-StepLR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-8ee0e1fe52dc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler, folder_name, begin_epoch)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_train = SketchDataset('./dataset', 'train')\n",
    "dataset_val = SketchDataset('./dataset', 'val')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=1, shuffle=True, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "model.train()\n",
    "train_model(model, optimizer, data_loader, data_loader_val, device, 10, 'Faster-RCNN', 'All-Comp_v6_SGD-StepLR', lr_scheduler, begin_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214a5b",
   "metadata": {},
   "source": [
    "### Faster-RCNN Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb9e83",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 [1619/1619] - Loss: 0.13450743522699468 \tProgress [10.0%] \tEpoch time elapsed: 11:03:48\n",
      "[Val] [464/464] - Loss: 0.2626046294270004 \tEpoch time elapsed: 1:15:581\n",
      "[Train] Epoch 2 [68/1619] - Loss: 0.2409842579561231 \tProgress [10.42%] \tEpoch time elapsed: 0:27:567"
     ]
    }
   ],
   "source": [
    "dataset_train = SketchDataset('./dataset_combined', 'train', combined=True, preprocessed=True)\n",
    "dataset_val = SketchDataset('./dataset_combined', 'val', combined=True, preprocessed=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=3, shuffle=True, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cpu') #torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "num_classes = 13\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "train_model(model, optimizer, data_loader, data_loader_val, device, 10, 'Faster-RCNN', 'All-Comp_v9_SGD-StepLR_notpre', lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b8b5f",
   "metadata": {},
   "source": [
    "### SSD train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9230d606",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Eriksson\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 [254/254] - Loss: 1.4496783018112183 \tProgress [6.67%] \tEpoch time elapsed: 0:08:59\n",
      "[Val] [149/149] - Loss: 1.0815856456756592 \tEpoch time elapsed: 0:01:42\n",
      "[Train] Epoch 2 [254/254] - Loss: 0.6185829043388367 \tProgress [13.33%] \tEpoch time elapsed: 0:09:03\n",
      "[Val] [149/149] - Loss: 0.9633197784423828 \tEpoch time elapsed: 0:01:439\n",
      "[Train] Epoch 3 [254/254] - Loss: 0.4855213165283203 \tProgress [20.0%] \tEpoch time elapsed: 0:09:0059\n",
      "[Val] [149/149] - Loss: 0.7084689140319824 \tEpoch time elapsed: 0:01:442\n",
      "[Train] Epoch 4 [254/254] - Loss: 0.5170818567276001 \tProgress [26.67%] \tEpoch time elapsed: 0:09:037\n",
      "[Val] [149/149] - Loss: 0.554409384727478 \tEpoch time elapsed: 0:01:4241\n",
      "[Train] Epoch 5 [254/254] - Loss: 0.2824884057044983 \tProgress [33.33%] \tEpoch time elapsed: 0:09:018\n",
      "[Val] [149/149] - Loss: 0.5354295372962952 \tEpoch time elapsed: 0:01:428\n",
      "[Train] Epoch 6 [254/254] - Loss: 0.23927098512649536 \tProgress [40.0%] \tEpoch time elapsed: 0:08:541\n",
      "[Val] [149/149] - Loss: 0.5381316542625427 \tEpoch time elapsed: 0:01:409\n",
      "[Train] Epoch 7 [254/254] - Loss: 0.2889207899570465 \tProgress [46.67%] \tEpoch time elapsed: 0:08:521\n",
      "[Val] [149/149] - Loss: 0.5394837856292725 \tEpoch time elapsed: 0:01:399\n",
      "[Train] Epoch 8 [254/254] - Loss: 0.399581640958786 \tProgress [53.33%] \tEpoch time elapsed: 0:08:5432\n",
      "[Val] [149/149] - Loss: 0.5457155704498291 \tEpoch time elapsed: 0:01:398\n",
      "[Train] Epoch 9 [254/254] - Loss: 0.6520136594772339 \tProgress [60.0%] \tEpoch time elapsed: 0:08:5653\n",
      "[Val] [149/149] - Loss: 0.5448159575462341 \tEpoch time elapsed: 0:01:400\n",
      "[Train] Epoch 10 [254/254] - Loss: 0.26275813579559326 \tProgress [66.67%] \tEpoch time elapsed: 0:08:52\n",
      "[Val] [149/149] - Loss: 0.5450984835624695 \tEpoch time elapsed: 0:01:400\n",
      "[Train] Epoch 11 [254/254] - Loss: 0.4517441391944885 \tProgress [73.33%] \tEpoch time elapsed: 0:08:565\n",
      "[Val] [149/149] - Loss: 0.5457406640052795 \tEpoch time elapsed: 0:01:407\n",
      "[Train] Epoch 12 [254/254] - Loss: 0.49640926718711853 \tProgress [80.0%] \tEpoch time elapsed: 0:08:562\n",
      "[Val] [149/149] - Loss: 0.5454398989677429 \tEpoch time elapsed: 0:01:386\n",
      "[Train] Epoch 13 [254/254] - Loss: 0.4063023328781128 \tProgress [86.67%] \tEpoch time elapsed: 0:08:529\n",
      "[Val] [149/149] - Loss: 0.5454692840576172 \tEpoch time elapsed: 0:01:408\n",
      "[Train] Epoch 14 [254/254] - Loss: 0.205678790807724 \tProgress [93.33%] \tEpoch time elapsed: 0:08:5958\n",
      "[Val] [149/149] - Loss: 0.5454902052879333 \tEpoch time elapsed: 0:02:029\n",
      "[Train] Epoch 15 [254/254] - Loss: 0.4331454038619995 \tProgress [100.0%] \tEpoch time elapsed: 0:09:230\n",
      "[Val] [149/149] - Loss: 0.5455499291419983 \tEpoch time elapsed: 0:02:053\n",
      "Training completed! Loss: 0.4331454038619995 \tTotal time elapsed: 2:40:47"
     ]
    }
   ],
   "source": [
    "dataset_train = SketchDataset('./dataset', 'train')\n",
    "dataset_val = SketchDataset('./dataset', 'val')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "num_classes = 13\n",
    "in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "\n",
    "model_ssd.to(device)\n",
    "\n",
    "\n",
    "params = [p for p in model_ssd.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0005, \n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "train_model(model_ssd, optimizer, data_loader, data_loader_val, device, 15, 'SSD', 'All_v5_SGD-StepLR', lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9ea74",
   "metadata": {},
   "source": [
    "### Retina train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48e6b75",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 [415/415] - Loss: 0.5971544981002808 \tProgress [6.67%] \tEpoch time elapsed: 2:36:22\n",
      "[Val] [106/106] - Loss: 0.7871170043945312 \tEpoch time elapsed: 0:16:35\n",
      "[Train] Epoch 2 [415/415] - Loss: 0.4725760221481323 \tProgress [13.33%] \tEpoch time elapsed: 2:36:533\n",
      "[Val] [106/106] - Loss: 0.445965051651001 \tEpoch time elapsed: 0:16:2721\n",
      "[Train] Epoch 3 [415/415] - Loss: 0.24580880999565125 \tProgress [20.0%] \tEpoch time elapsed: 2:36:496\n",
      "[Val] [106/106] - Loss: 0.3202489912509918 \tEpoch time elapsed: 0:16:103\n",
      "[Train] Epoch 4 [415/415] - Loss: 0.14127951860427856 \tProgress [26.67%] \tEpoch time elapsed: 2:33:04\n",
      "[Val] [106/106] - Loss: 0.19828936457633972 \tEpoch time elapsed: 0:16:12\n",
      "[Train] Epoch 5 [415/415] - Loss: 0.1403563916683197 \tProgress [33.33%] \tEpoch time elapsed: 2:33:153\n",
      "[Val] [106/106] - Loss: 0.1775968074798584 \tEpoch time elapsed: 0:16:114\n",
      "[Train] Epoch 6 [415/415] - Loss: 0.09419110417366028 \tProgress [40.0%] \tEpoch time elapsed: 2:33:008\n",
      "[Val] [106/106] - Loss: 0.1727815568447113 \tEpoch time elapsed: 0:16:072\n",
      "[Train] Epoch 7 [415/415] - Loss: 0.11307470500469208 \tProgress [46.67%] \tEpoch time elapsed: 2:32:52\n",
      "[Val] [106/106] - Loss: 0.16369161009788513 \tEpoch time elapsed: 0:16:06\n",
      "[Train] Epoch 8 [415/415] - Loss: 0.1284562051296234 \tProgress [53.33%] \tEpoch time elapsed: 2:32:553\n",
      "[Val] [106/106] - Loss: 0.16260042786598206 \tEpoch time elapsed: 0:16:15\n",
      "[Train] Epoch 9 [415/415] - Loss: 0.12968425452709198 \tProgress [60.0%] \tEpoch time elapsed: 2:35:185\n",
      "[Val] [106/106] - Loss: 0.15998968482017517 \tEpoch time elapsed: 0:16:22\n",
      "[Train] Epoch 10 [415/415] - Loss: 0.09321253001689911 \tProgress [66.67%] \tEpoch time elapsed: 2:35:29\n",
      "[Val] [106/106] - Loss: 0.1602097898721695 \tEpoch time elapsed: 0:16:181\n",
      "[Train] Epoch 11 [127/415] - Loss: 0.0802672952413559 \tProgress [68.71%] \tEpoch time elapsed: 0:47:186"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-264bb75c5242>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                                                gamma=0.1)\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_retina\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RetinaNet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'All_v8_SGD-StepLR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-34c12cd29e4d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler, folder_name, mixup, begin_epoch)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_train = SketchDataset('./dataset_combined', 'train', combined=True)\n",
    "dataset_val = SketchDataset('./dataset_combined', 'val', combined=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=3, shuffle=True, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cpu')# if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_retina = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "model_retina.head = torchvision.models.detection.retinanet.RetinaNetHead(256, 9, 13)\n",
    "\n",
    "model_retina.to(device)\n",
    "\n",
    "\n",
    "params = [p for p in model_retina.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "train_model(model_retina, optimizer, data_loader, data_loader_val, device, 15, 'RetinaNet', 'All_v8_SGD-StepLR', lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984dd5dc",
   "metadata": {},
   "source": [
    "## Optical character recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78becb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "874be976",
   "metadata": {},
   "source": [
    "## Post processing engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce632dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
