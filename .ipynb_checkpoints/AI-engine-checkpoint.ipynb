{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20389c2a",
   "metadata": {},
   "source": [
    "# AI Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0bc09",
   "metadata": {},
   "source": [
    "# Import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b644fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as transform\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "sys.path.insert(0, './pytorchutils/')\n",
    "from pytorchutils.engine import train_one_epoch, evaluate\n",
    "import pytorchutils.utils\n",
    "import pytorchutils.multibox_loss\n",
    "\n",
    "#!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "\n",
    "# Calculates the Intersection Over Union for two specified bounding boxes\n",
    "def calc_iou(bb1, bb2):\n",
    "    # Get the coordinates of the intersecting box\n",
    "    inter_x = max(bb1[0], bb2[0])\n",
    "    inter_y = max(bb1[1], bb2[1])\n",
    "    inter_x2 = min(bb1[2], bb2[2])\n",
    "    inter_y2 = min(bb1[3], bb2[3])\n",
    "    \n",
    "    if inter_x2 < inter_x or inter_y2 < inter_y:\n",
    "        return 0.0\n",
    "    \n",
    "    inter_area = (inter_x2 - inter_x) * (inter_y2 - inter_y)\n",
    "\n",
    "    # If intersection area is or lower than 0 we dont have an intersection\n",
    "    #if inter_area <= 0:\n",
    "    #    return 0.0\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "    iou = inter_area / float(bb1_area + bb2_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "def calc_intersection(bb1, bb2):\n",
    "    inter_x = max(bb1[0], bb2[0])\n",
    "    inter_y = max(bb1[1], bb2[1])\n",
    "    inter_x2 = min(bb1[2], bb2[2])\n",
    "    inter_y2 = min(bb1[3], bb2[3])\n",
    "    \n",
    "    if inter_x2 < inter_x or inter_y2 < inter_y:\n",
    "        return 0.0\n",
    "    \n",
    "    return (inter_x2 - inter_x) * (inter_y2 - inter_y)\n",
    "\n",
    "# Calculates the area of a bounding box\n",
    "def calc_area(bb):\n",
    "    return (bb[2] - bb[0]) * (bb[3] - bb[1])\n",
    "\n",
    "# Padds a bounding box by a specific number, doubles the padding if text is specified\n",
    "def pad_bb(bb, pad, text=False):\n",
    "    x,y,x2,y2 = bb\n",
    "    if text:\n",
    "        return [x-pad*2, y-pad, x2+pad*2, y2+pad]\n",
    "    return [x-pad, y-pad, x2+pad, y2+pad]\n",
    "\n",
    "# Returns the smallest bounding box between two specified boxes\n",
    "def return_smallest(bb1, bb2):\n",
    "    bb1_x,bb1_y,bb1_x2,bb1_y2 = bb1\n",
    "    bb2_x,bb2_y,bb2_x2,bb2_y2 = bb2\n",
    "    bb1_size = (bb1_x2-bb1_x)*(bb1_y2-bb1_y)\n",
    "    bb2_size = (bb2_x2-bb2_x)*(bb2_y2-bb2_y)\n",
    "    \n",
    "    return bb2 if bb1_size > bb2_size else bb1\n",
    "\n",
    "# Gets the bounding boxes from an image by processing the image\n",
    "def get_bbs_from_image(im, clean=True, pad=0, text=False):\n",
    "    imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bbs = []\n",
    "    for cntr in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cntr)\n",
    "        #cv2.rectangle(im, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        if x > 40 and y > 40 and y < im.shape[:2][1]-40 and x < im.shape[:2][0]-40:\n",
    "            bbs.append([x,y,x+w,y+h])\n",
    "                \n",
    "    t_bbs = []\n",
    "    [t_bbs.append(x) for x in bbs if x not in t_bbs]\n",
    "    \n",
    "    t_bbs = remove_small_bb_list(t_bbs, 300)\n",
    "    combined_bbs = combine_bb_list(t_bbs, pad=pad, text=text)\n",
    "    \n",
    "    if clean:\n",
    "        return clean_bb_list(combined_bbs, pad=pad)\n",
    "    \n",
    "    return combined_bbs\n",
    "\n",
    "def combine_bb_list(bb_list, pad=0, text=False):\n",
    "    bbs = bb_list.copy()\n",
    "    iou_non_zero = True\n",
    "    while iou_non_zero:\n",
    "        iou_non_zero = False\n",
    "        for i in range(len(bbs)-1):\n",
    "            for c in range(i, len(bbs)):\n",
    "                if bbs[i] == bbs[c]:\n",
    "                    continue\n",
    "                    \n",
    "                iou = calc_iou(pad_bb(bbs[i], pad, text=text), bbs[c])\n",
    "                \n",
    "                if iou != 0:\n",
    "                    iou_non_zero = True\n",
    "                    bb = combine_bb(bbs[i], bbs[c])\n",
    "                    bb1 = bbs[i].copy()\n",
    "                    bb2 = bbs[c].copy()\n",
    "                    \n",
    "                    bbs.remove(bb1)\n",
    "                    bbs.remove(bb2)\n",
    "                    bbs.append(bb)\n",
    "                    break;\n",
    "            if iou_non_zero:\n",
    "                break;\n",
    "    return bbs\n",
    "\n",
    "def combine_bb(bb1, bb2):\n",
    "    bb1_x,bb1_y,bb1_x2,bb1_y2 = bb1\n",
    "    bb2_x,bb2_y,bb2_x2,bb2_y2 = bb2\n",
    "\n",
    "    if bb2_x < bb1_x:\n",
    "        bb1_x = bb2_x\n",
    "    if bb2_y < bb1_y:\n",
    "        bb1_y = bb2_y\n",
    "    if bb2_x2 > bb1_x2:\n",
    "        bb1_x2 = bb2_x2\n",
    "    if bb2_y2 > bb1_y2:\n",
    "        bb1_y2 = bb2_y2\n",
    "        \n",
    "    return [bb1_x, bb1_y, bb1_x2, bb1_y2]\n",
    "\n",
    "def clean_bb_list(bb_list, pad=0, text=False):\n",
    "    bbs = bb_list.copy()\n",
    "    iou_non_zero = True\n",
    "    while iou_non_zero:\n",
    "        iou_non_zero = False\n",
    "        for i in range(len(bbs)):\n",
    "            if i == len(bbs)-1:\n",
    "                break;\n",
    "                \n",
    "            iou = calc_iou(pad_bb(bbs[i], pad, text=text), bbs[i+1])\n",
    "\n",
    "            if iou == 0:\n",
    "                continue\n",
    "\n",
    "            iou_non_zero = True\n",
    "            bb = return_smallest(bbs[i], bbs[i+1])\n",
    "            bbs.remove(bb)\n",
    "            break;\n",
    "                \n",
    "    return bbs\n",
    "\n",
    "def remove_small_bb_list(bb_list, size):\n",
    "    cleaned_list = []\n",
    "    for bb in bb_list:\n",
    "        x,y,x2,y2 = bb\n",
    "        w = x2-x\n",
    "        h = y2-y\n",
    "        if w*h > size:\n",
    "            cleaned_list.append(bb)\n",
    "            \n",
    "    return cleaned_list\n",
    "\n",
    "# Normalizes a pixel specific bounding box [x, y, x2, y2] to normalized bounding box [x, y, w, h]\n",
    "def normalize_bb(bb, shape):\n",
    "    h_img,w_img = shape\n",
    "    x,y,x2,y2 = bb\n",
    "    norm_w,norm_h = [(x2-x)/w_img, (y2-y)/h_img]\n",
    "    return [((x+x2)/2)/w_img, ((y+y2)/2)/h_img, norm_w, norm_h]\n",
    "\n",
    "# Denormalizes a normalized bounding box [x, y, w, h] to pixel specific bounding box [x, y, x2, y2]\n",
    "def denormalize_bb(bb, shape):\n",
    "    h_img,w_img = shape\n",
    "    x,y,w,h = bb\n",
    "    x_min,y_min = [int(x*w_img-(w*w_img)/2), int(y*h_img-(h*h_img)/2)]\n",
    "    return [x_min, y_min, x_min+int(w*w_img), y_min+int(h*h_img)]\n",
    "\n",
    "# Stringifies a bounding box for output\n",
    "def bb_to_str(bb):\n",
    "    return str(bb[0])+' '+str(bb[1])+' '+str(bb[2])+' '+str(bb[3])\n",
    "\n",
    "# Destringifies a bounding box\n",
    "def str_to_bb(bb_str):\n",
    "    str_arr = bb_str.split(' ')\n",
    "    return [float(str_arr[0]), float(str_arr[1]), float(str_arr[2]), float(str_arr[3]), float(str_arr[4])]\n",
    "\n",
    "# Generates dataset structure by generating boundingbox labels, spliting data into train and validition sets\n",
    "# also providing the found boundingboxes for verification of labeling being successfull \n",
    "def generate_dataset(root_folder, labels=[], split_components=True, train_val_ratio=0.8):\n",
    "    os.mkdir('./'+root_folder+'_generated/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/train/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/val/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/bbs/')\n",
    "    os.mkdir('./'+root_folder+'_generated/labels/')\n",
    "\n",
    "    if (split_components):    \n",
    "        for component in os.listdir('./'+root_folder):\n",
    "            os.mkdir('./'+root_folder+'_generated/images/train/'+component+'/')\n",
    "            os.mkdir('./'+root_folder+'_generated/images/val/'+component+'/')\n",
    "            images = os.listdir('./'+root_folder+'/'+component)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i]\n",
    "                img_type = 'val' if i > math.floor(len(images)*train_val_ratio) else 'train'\n",
    "                im = cv2.imread('./'+root_folder+'/'+component+'/'+image)\n",
    "                cv2.imwrite('./'+root_folder+'_generated/images/'+img_type+'/'+component+'/'+image, im)\n",
    "                bbs = get_bbs_from_image(im, clean=True, pad=30, text=True)\n",
    "                bbs_str = '' \n",
    "                for bb in bbs:\n",
    "                    bbs_str += str(labels[component])+' '+bb_to_str(normalize_bb(bb, im.shape[:2]))+'\\n'\n",
    "                    x,y,x2,y2 = pad_bb(bb, 5)\n",
    "                    cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.imwrite('./'+root_folder+'_generated/images/bbs/'+image, im)\n",
    "                f = open('./'+root_folder+'_generated/labels/'+image[:-3]+\"txt\", \"a\")\n",
    "                f.write(bbs_str[:-1])\n",
    "                f.close()\n",
    "    else:\n",
    "        images = os.listdir('./'+root_folder)\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            img_type = 'val' if i > math.floor(len(images)*train_val_ratio) else 'train'\n",
    "            im = cv2.imread('./'+root_folder+'/'+image)\n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/'+img_type+'/'+image, im)\n",
    "            bbs = get_bbs_from_image(im, clean=True, pad=30, text=True)\n",
    "            \n",
    "            bbs_str = '' \n",
    "            c = 0\n",
    "            for bb in bbs:\n",
    "                c = c + 1\n",
    "                bbs_str += str(c)+' '+bb_to_str(normalize_bb(bb, im.shape[:2]))+'\\n'\n",
    "                x,y,x2,y2 = pad_bb(bb, 5)\n",
    "                cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(im, str(c), (int((x+x2)/2)-50,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/bbs/'+image, im)\n",
    "            f = open('./'+root_folder+'_generated/labels/'+image[:-3]+\"txt\", \"a\")\n",
    "            f.write(bbs_str[:-1])\n",
    "            f.close()\n",
    "            \n",
    "def predict_and_save(model, root_dir, save_dir, labels=[], threshold=0.5, IoU=0, mask=False):\n",
    "    # Check if the 4th final character is a dot aka if the input directory is a file\n",
    "    if root_dir[-4] == '.':\n",
    "        im = root_dir\n",
    "        img = cv2.imread(im)\n",
    "        cv2_img = cv2.imread(im)\n",
    "        if mask:\n",
    "            imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "        tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "        predictions = model(tensor_img)\n",
    "        print(predictions)\n",
    "        dont_print_id = []\n",
    "        for i in range(len(predictions[0]['boxes'])):\n",
    "            score = predictions[0]['scores'][i].item()\n",
    "            if IoU > 0:\n",
    "                if i in dont_print_id:\n",
    "                    continue\n",
    "                bb1 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                for c in range(i, len(predictions[0]['boxes'])):\n",
    "                    bb2 = predictions[0]['boxes'][c].detach().numpy()\n",
    "                    if calc_intersection(bb1, bb2) > calc_area(bb1)*IoU:\n",
    "                        dont_print_id.append(c)\n",
    "            if score > threshold:\n",
    "                x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "                \n",
    "                if len(labels) > 1:\n",
    "                    cv2.putText(cv2_img, labels[predictions[0]['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imwrite(save_dir+(root_dir.split(\"/\")[-1]), cv2_img)\n",
    "    else:\n",
    "        for image in os.listdir(root_dir):\n",
    "            im = root_dir+image\n",
    "            img = Image.open(im)\n",
    "            cv2_img = cv2.imread(im)\n",
    "            tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "            tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "            predictions = model(tensor_img)\n",
    "            print(predictions)\n",
    "            for i in range(len(predictions[0]['boxes'])):\n",
    "                score = predictions[0]['scores'][i].item()\n",
    "                if score > threshold:\n",
    "                    x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                    if len(labels) > 1:\n",
    "                        cv2.putText(cv2_img, labels[predictions[0]['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imwrite(save_dir+image, cv2_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8b588",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a357a8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IMG20220127120913'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'IMG20220127120628'\n",
    "'IMG20220127120506'\n",
    "'IMG20220127120913'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17f5b030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = cv2.imread('./whiteboard website.jpg')\n",
    "bbs = get_bbs_from_image(im, clean=True, pad=10, text=True)\n",
    "\n",
    "for bb in bbs:\n",
    "    x,y,x2,y2 = pad_bb(bb, 5)\n",
    "    cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "#r = 800 / float(im.shape[:2][1])\n",
    "#im = cv2.resize(im, (int(im.shape[:2][0] * r), 800))\n",
    "cv2.imwrite('whiteboard website BB.jpg', im)\n",
    "#cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7ab0709",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-86700aaa86f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtime_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgenerate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'temp/big_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_before\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-6031ce4905e2>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[1;34m(root_folder, labels, split_components, train_val_ratio)\u001b[0m\n\u001b[0;32m    222\u001b[0m                     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_bb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_generated/images/bbs/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_generated/labels/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbs_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = {data[i]:i for i in range(len(data))}\n",
    "time_before = time.time()\n",
    "generate_dataset('temp/big_data', labels)\n",
    "print(str(datetime.timedelta(seconds=round(time.time()-time_before))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82aceb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = cv2.imread('./new_more_dataset/Checkbox/IMG20220209145109.jpg')\n",
    "#bbs = get_bbs_from_image(im, clean=True, pad=30, text=True)\n",
    "\n",
    "#f = open(\"test_bb.txt\", \"w\")\n",
    "#for bb in bbs:\n",
    "#    norm_bb = normalize_bb(bb, im.shape[:2])\n",
    "#    print(bb,'->',norm_bb)\n",
    "#    f.write(\"0 \" + bb_to_str(norm_bb) + ('\\n' if bbs.index(bb) != len(bbs)-1 else ''))\n",
    "#\n",
    "#f.close()\n",
    "\n",
    "f = open(\"./new_more_dataset_bb/labels/IMG20220209145109.txt\", \"r\")\n",
    "for bb in f.read().split('\\n'):\n",
    "    denorm_bb = denormalize_bb(str_to_bb(bb)[1:], im.shape[:2])\n",
    "    x,y,w,h = pad_bb(denorm_bb, 5)\n",
    "    cv2.rectangle(im, (x, y), (w, h), (0, 0, 255), 2)\n",
    "    cv2.putText(im, '1', (int((x+w)/2)-50,int((y+h)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "    \n",
    "f.close()\n",
    "cv2.imwrite('checkbox test.jpg', im)\n",
    "#cv2.waitKey()\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32330d79",
   "metadata": {},
   "source": [
    "### Yolo formating to box testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4cf16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669 1609 3106 1953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = cv2.imread('./image_unsorted/IMG20220129155542.jpg')\n",
    "#h,w = im.shape[:2]\n",
    "label = [8, 0.556466, 0.402322, 0.479053, 0.086066]\n",
    "x,y,w,h = denormalize_bb(label[1:], im.shape[:2])\n",
    "print(x,y,w,h)\n",
    "cv2.rectangle(im, (x,y), (w, h), (0,0,255), 2)\n",
    "cv2.imwrite('test2.jpg', im)\n",
    "#cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b3bcd",
   "metadata": {},
   "source": [
    "### Mark all in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "6780ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = os.listdir('./image_unsorted')\n",
    "for image in images:\n",
    "    im = cv2.imread('./image_unsorted/'+image)\n",
    "    bbs = get_bbs_from_image(im, clean=True, pad=30, text=True)\n",
    "    for bb in bbs:\n",
    "        x,y,x2,y2 = pad_bb(bb, 5)\n",
    "        cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "    cv2.imwrite('./image_unsorted_bb/'+image, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a61841",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319601f0",
   "metadata": {},
   "source": [
    "### Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542b14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNSketchDataset(Dataset):\n",
    "    def __init__(self, root_dir, set_type, single_component=False):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.component_names = []\n",
    "        if not single_component:\n",
    "            for component in os.listdir(root_dir+\"/images/\"+set_type):\n",
    "                self.component_names.append(component)\n",
    "                for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+component):\n",
    "                    self.images.append(root_dir+\"/images/\"+set_type+\"/\"+component+\"/\"+image)\n",
    "                    self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "        else:\n",
    "            self.component_names.append(single_component)\n",
    "            for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+single_component):\n",
    "                self.images.append(root_dir+\"/images/\"+set_type+\"/\"+single_component+\"/\"+image)\n",
    "                self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "            \n",
    "        self.root = root_dir\n",
    "        self.single_component = single_component\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im = cv2.imread(self.images[idx])\n",
    "        imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        #img = cv2.cvtColor(img_tresh, cv2.COLOR_GRAY2RGB)\n",
    "        #normImg = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "        f = open(self.labels[idx], \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "\n",
    "        N = len(data)\n",
    "        boxes = torch.zeros([N, 4], dtype=torch.float)\n",
    "        labels = torch.zeros([N], dtype=torch.int64)\n",
    "        areas = torch.zeros([N])\n",
    "        \n",
    "        for i in range(N):\n",
    "            bb = denormalize_bb(str_to_bb(data[i])[1:], img.shape[:2])\n",
    "            boxes[i][0],boxes[i][1],boxes[i][2],boxes[i][3] = bb\n",
    "            areas[i] = calc_area(bb)\n",
    "            \n",
    "            if not self.single_component:\n",
    "                labels[i] = int(data[i][0])+1\n",
    "                continue\n",
    "                \n",
    "            labels[i] = 1\n",
    "                \n",
    "        return transform.to_tensor(img), {'boxes':boxes, 'labels':labels, 'image_id':torch.LongTensor([idx]), 'area':areas, 'iscrowd':torch.zeros([N], dtype=torch.int64)}\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler=False):\n",
    "    writer = SummaryWriter()\n",
    "    total_time = time.time()\n",
    "    \n",
    "    date = datetime.datetime.now().strftime(\"%b-%d_%H-%M\")\n",
    "    if not os.path.exists('./models/'+model_type+'/'+date+'/'):\n",
    "        os.mkdir('./models/'+model_type+'/'+date+'/')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_time = time.time()\n",
    "        epoch_loss = []\n",
    "        batch_nr = 0\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            batch_time = time.time()\n",
    "            \n",
    "            # Send them to device if using GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            pred = model(images, targets)\n",
    "            losses = sum(loss for loss in pred.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss.append(losses.item())\n",
    "            \n",
    "            writer.add_scalars(model_type+'_'+model_name, {\n",
    "                'train_loss': losses.item(),\n",
    "            }, epoch*len(data_loader)+batch_nr)\n",
    "            \n",
    "            batch_nr = batch_nr + 1\n",
    "            print(\n",
    "                '\\r[Train] Epoch {} [{}/{}] - Loss: {} \\tProgress [{}%] \\tEpoch time elapsed: {}'.format(\n",
    "                    epoch+1, batch_nr, len(data_loader), losses.item(), round(((epoch/num_epochs)+(1/num_epochs*batch_nr/len(data_loader)))*100, 2), str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "        \n",
    "            \n",
    "        writer.add_scalars(model_type+'_'+model_name, {\n",
    "            'avg_epoch_loss': numpy.average(epoch_loss),\n",
    "        }, (epoch+1))\n",
    "            \n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        print()\n",
    "        evaluate_model(model, data_loader_val, device, writer, model_type, model_name, epoch+1)\n",
    "        #model.train()\n",
    "        print()\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, './models/'+model_type+'/'+date+'/'+model_name+'-'+str(epoch+1)+'.pt')\n",
    "\n",
    "    print(\n",
    "        '\\rTraining completed! Loss: {} \\tTotal time elapsed: {}'.format(\n",
    "            losses.item(), str(datetime.timedelta(seconds=round(time.time()-total_time)))\n",
    "        ),\n",
    "        end=''\n",
    "    )\n",
    "    \n",
    "def evaluate_model(model, data_loader, device, writer, model_type, model_name, epoch):\n",
    "    with torch.no_grad():\n",
    "        epoch_time = time.time()\n",
    "        avg_loss = []\n",
    "        batch_nr = 0\n",
    "        for images, targets in data_loader:\n",
    "            # Send them to device if using GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            pred = model(images, targets)\n",
    "            losses = sum(loss for loss in pred.values())\n",
    "            avg_loss.append(losses.item())\n",
    "            \n",
    "            batch_nr = batch_nr + 1\n",
    "            print(\n",
    "                '\\r[Val] [{}/{}] - Loss: {} \\tEpoch time elapsed: {}'.format(\n",
    "                    batch_nr, len(data_loader), losses.item(), str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "\n",
    "        writer.add_scalars(model_type+'_'+model_name, {\n",
    "            'val_loss': numpy.average(avg_loss),\n",
    "        }, epoch)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fb9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Eriksson\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 [3/72] - Loss: 0.35160717368125916 \tProgress [0.6%] \tEpoch time elapsed: 0:02:19"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7907153fe5f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                                                gamma=0.1)\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Faster-RCNN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'All-Comp_v4_SGD-StepLR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-b96874db7d86>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     91\u001b[0m                                      .format(degen_bb, target_idx))\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\models\\_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\ops\\misc.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#dataset_train = FasterRCNNSketchDataset('./dataset', 'train')\n",
    "#dataset_val = FasterRCNNSketchDataset('./dataset', 'val')\n",
    "\n",
    "dataset_train = FasterRCNNSketchDataset('./dataset', 'train')\n",
    "dataset_val = FasterRCNNSketchDataset('./dataset', 'val')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "        collate_fn=pytorchutils.utils.collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "        collate_fn=pytorchutils.utils.collate_fn)\n",
    "\n",
    "device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 13\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "train_model(model, optimizer, data_loader, data_loader_val, device, 7, 'Faster-RCNN', 'All-Comp_v4_SGD-StepLR', lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df2df1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cryslacks\\anaconda3\\envs\\nnlm\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[1498.8979,  440.7491, 2635.7700,  925.8327],\n",
      "        [1671.9338, 1396.2307, 2609.1558, 1840.4751],\n",
      "        [ 384.7604, 2191.0439, 1274.4561, 2615.8643],\n",
      "        [1751.2051, 2046.9060, 2542.6919, 2358.1477],\n",
      "        [ 232.9498, 1143.3949, 1021.7910, 1524.4823],\n",
      "        [1674.9076, 2003.4613, 2608.4773, 2747.4641],\n",
      "        [ 248.2850, 1527.6628,  940.2410, 1858.7635],\n",
      "        [  45.3643,  689.6697, 1078.6929, 1179.3794],\n",
      "        [1730.3732, 2401.5906, 2543.0100, 2734.2966],\n",
      "        [ 211.7554, 1162.8032, 1062.4457, 1917.1985],\n",
      "        [1700.6423, 2238.8347, 2578.4087, 2591.6472],\n",
      "        [ 120.0673,  992.3790, 1072.0443, 1627.6317],\n",
      "        [  90.8544,  721.5524, 1092.3401,  818.2288],\n",
      "        [ 100.1335,  728.2725, 1125.5175,  949.4650],\n",
      "        [1656.1731, 1784.4149, 2516.7114, 2143.3918],\n",
      "        [1541.0342,  411.2225, 2771.5647, 2294.4272],\n",
      "        [ 265.9842, 1421.7349, 1119.7715, 1751.7324],\n",
      "        [1722.5110, 2288.2554, 2556.0334, 2466.1270],\n",
      "        [1606.8375, 1354.7185, 2654.7952, 2177.7903],\n",
      "        [1399.8278,  709.7514, 2681.8364, 1486.5886],\n",
      "        [ 332.0152, 1195.7855,  988.2740, 1404.3541],\n",
      "        [ 468.3018, 1571.0648,  850.1440, 1845.7115],\n",
      "        [1857.7495, 2160.8169, 2439.1013, 2793.5286],\n",
      "        [ 244.7714, 1337.2170, 1043.9062, 1626.5159],\n",
      "        [1613.2151, 1195.4814, 2859.9163, 3015.4175],\n",
      "        [ 285.4179, 1575.6722,  991.7654, 1755.2289],\n",
      "        [1579.7604, 1867.2361, 2696.2583, 2394.1260],\n",
      "        [1511.4487, 1482.1511, 2835.2605, 1938.1968],\n",
      "        [  73.7020,  630.0781, 1139.8887,  800.2811],\n",
      "        [ 352.5238, 1423.5781, 1011.1290, 1597.4172],\n",
      "        [ 157.5493,  698.8376, 1041.1086,  779.5274],\n",
      "        [1543.2097,  603.9009, 1914.8488,  873.5396]],\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9989, 0.9988, 0.9977, 0.9934, 0.9880, 0.9756, 0.9607, 0.9575, 0.9423,\n",
      "        0.8951, 0.8594, 0.7634, 0.6152, 0.5738, 0.4545, 0.4015, 0.3881, 0.3776,\n",
      "        0.2097, 0.1956, 0.1812, 0.1720, 0.1484, 0.1422, 0.1413, 0.1310, 0.1228,\n",
      "        0.0966, 0.0866, 0.0827, 0.0586, 0.0547], grad_fn=<IndexBackward0>)}]\n",
      "Faster-RCNN scores for bbs: 0.9989145994186401\n",
      "Faster-RCNN scores for bbs: 0.9987718462944031\n",
      "Faster-RCNN scores for bbs: 0.997650682926178\n",
      "Faster-RCNN scores for bbs: 0.9933786392211914\n",
      "Faster-RCNN scores for bbs: 0.9880159497261047\n",
      "Faster-RCNN scores for bbs: 0.9756360054016113\n",
      "Faster-RCNN scores for bbs: 0.9606714248657227\n",
      "Faster-RCNN scores for bbs: 0.9575106501579285\n",
      "Faster-RCNN scores for bbs: 0.9423048496246338\n",
      "Faster-RCNN scores for bbs: 0.8951419591903687\n",
      "Faster-RCNN scores for bbs: 0.8593669533729553\n",
      "Faster-RCNN scores for bbs: 0.7633788585662842\n",
      "Faster-RCNN scores for bbs: 0.6151607632637024\n",
      "Faster-RCNN scores for bbs: 0.5737649202346802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "im =\"./components.jpg\"\n",
    "image = \"components.jpg\"\n",
    "#label = \"./dataset/labels/\"+(image.split('.')[0]+'.txt')\n",
    "imgR = cv2.imread(im)\n",
    "cv2_img = cv2.imread(im)\n",
    "imgray = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "predictions = model(tensor_img)\n",
    "print(predictions)\n",
    "\n",
    "for i in range(len(predictions[0]['boxes'])):\n",
    "    score = predictions[0]['scores'][i].item()\n",
    "    if score > 0.5:\n",
    "        x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "        cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "        cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        print('Faster-RCNN scores for bbs:', score)\n",
    "cv2.imwrite('./results/input-rcnn/treshold-'+image, cv2_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93ef565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cryslacks\\anaconda3\\envs\\nnlm\\lib\\site-packages\\ipykernel_launcher.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[1783.3864, 2026.4620, 2546.8032, 2330.3635],\n",
      "        [ 500.4083,  875.5958, 1492.7535, 1217.0627],\n",
      "        [1757.9944,  900.5966, 2455.7290, 1256.8989],\n",
      "        [ 531.2827, 1385.3551, 1605.5140, 3323.6848],\n",
      "        [ 410.3445, 1207.7856, 2723.2495, 3191.3757],\n",
      "        [ 474.0057,  183.6731, 1663.3750, 1212.9236],\n",
      "        [ 499.0638, 1353.7314, 1672.7955, 3171.4692],\n",
      "        [ 603.2289,  242.6476, 1480.7758,  622.3920],\n",
      "        [1690.5858, 1610.7550, 2904.0471, 1857.0385],\n",
      "        [1756.4015,  906.1067, 2541.8386, 1250.3099],\n",
      "        [ 531.0112,  316.0464, 1669.2383, 1282.9100],\n",
      "        [ 481.7568, 1260.1385, 2934.2300, 3167.8040],\n",
      "        [1234.6787,  834.4679, 2935.7051, 1981.5819],\n",
      "        [1678.0809,  963.8422, 2850.2180, 2378.3411],\n",
      "        [ 459.6611,  158.2962, 1693.2533,  999.1198],\n",
      "        [ 557.6180,  877.3840, 1577.7971, 1226.6521],\n",
      "        [1743.1876, 1590.8253, 1949.9871, 1802.0271],\n",
      "        [1729.7051, 1600.1277, 1932.2109, 1796.7493],\n",
      "        [1723.4525, 2019.7620, 2572.7141, 2319.9211],\n",
      "        [ 392.8522,  231.9633, 1788.4442,  675.7661],\n",
      "        [ 416.0484,  359.8306, 1594.5645, 1173.8726],\n",
      "        [ 820.2412,  847.4986, 2952.9036, 2232.9758],\n",
      "        [ 634.8475,  819.5422, 2986.5437, 2300.1265],\n",
      "        [ 420.2785,  531.7069, 1624.7976, 1226.1165],\n",
      "        [1680.3862,  893.2809, 2565.9548, 1292.8253],\n",
      "        [ 602.3056,  705.4509, 3000.0000, 2071.3198],\n",
      "        [1428.1472,  813.4855, 2818.4355, 2008.7467],\n",
      "        [ 578.7705,  352.1784, 1585.2825, 1200.7748],\n",
      "        [ 468.8338,  266.0504, 1781.4519, 1480.2249],\n",
      "        [ 472.8528,  540.7029, 1649.4489,  935.5018],\n",
      "        [ 684.2202,  754.4755, 2863.7253, 2086.0100],\n",
      "        [1694.7684,  771.8354, 2570.4126, 1283.9604],\n",
      "        [ 849.2490,  891.4049, 3000.0000, 2217.6150],\n",
      "        [ 404.7646,  999.5413, 3000.0000, 2601.5618],\n",
      "        [1464.4255, 1568.4082, 2912.5771, 2325.4319],\n",
      "        [ 498.6249,  169.9341, 1724.1072,  685.5804],\n",
      "        [1686.3745, 2028.1132, 2540.1643, 2331.5945],\n",
      "        [ 552.9155,  201.4301, 1449.9869,  591.6924],\n",
      "        [ 696.0364,  385.2326, 1547.4641,  606.8030],\n",
      "        [2255.3218, 1614.5984, 2894.6819, 1834.4065],\n",
      "        [ 442.4022,  127.4167, 1519.1536, 1226.6531],\n",
      "        [1373.3882, 1030.2148, 2848.0779, 3118.2566],\n",
      "        [ 530.5947,  241.3784, 1157.7843,  559.0330],\n",
      "        [1748.7394, 1573.9962, 1936.6510, 1803.3209],\n",
      "        [ 337.4285,  191.0168, 1651.9768,  694.0548],\n",
      "        [ 222.3963,  310.6924, 1586.9049,  918.2378],\n",
      "        [1729.2389, 1587.0150, 1942.5085, 1807.5367],\n",
      "        [ 627.2432,  230.8289, 1428.6338,  605.7408],\n",
      "        [ 249.0015,  925.6638, 1572.5885, 1285.4615]],\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([ 7,  2,  7,  4,  5,  2,  5,  7,  7,  8,  6,  4,  7,  7,  7,  6,  8,  7,\n",
      "         8, 10,  4,  4,  5, 10,  5,  2,  5,  3, 10,  2,  6, 10, 10,  7,  7,  2,\n",
      "         6,  5,  7,  7,  5,  7,  8, 10,  6, 10,  5, 10, 10]), 'scores': tensor([0.9611, 0.9464, 0.8645, 0.7796, 0.5030, 0.4556, 0.4291, 0.4105, 0.3572,\n",
      "        0.2991, 0.2915, 0.2670, 0.2358, 0.2207, 0.2188, 0.1800, 0.1777, 0.1743,\n",
      "        0.1585, 0.1327, 0.1311, 0.1270, 0.1249, 0.1188, 0.0986, 0.0981, 0.0971,\n",
      "        0.0942, 0.0937, 0.0897, 0.0863, 0.0860, 0.0854, 0.0762, 0.0742, 0.0736,\n",
      "        0.0725, 0.0711, 0.0681, 0.0674, 0.0658, 0.0649, 0.0625, 0.0586, 0.0532,\n",
      "        0.0529, 0.0513, 0.0505, 0.0502], grad_fn=<IndexBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "model_all.eval()\n",
    "f = open('./dataset/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "predict_and_save(model_all, './whiteboard website.jpg', './results/all-rcnn/treshold/', labels=labels, threshold=0.5, IoU=0.3, mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b8b5f",
   "metadata": {},
   "source": [
    "### SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2e702e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDSketchDataset(Dataset):\n",
    "    def __init__(self, root_dir, set_type, single_component=False):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.component_names = []\n",
    "        if not single_component:\n",
    "            for component in os.listdir(root_dir+\"/images/\"+set_type):\n",
    "                self.component_names.append(component)\n",
    "                for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+component):\n",
    "                    self.images.append(root_dir+\"/images/\"+set_type+\"/\"+component+\"/\"+image)\n",
    "                    self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "        else:\n",
    "            self.component_names.append(single_component)\n",
    "            for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+single_component):\n",
    "                self.images.append(root_dir+\"/images/\"+set_type+\"/\"+single_component+\"/\"+image)\n",
    "                self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "            \n",
    "        self.root = root_dir\n",
    "        self.single_component = single_component\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.images[idx])\n",
    "        #imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        #ret, img_tresh = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        #img = cv2.cvtColor(img_tresh, cv2.COLOR_GRAY2RGB)\n",
    "        #normImg = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "        f = open(self.labels[idx], \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "\n",
    "        N = len(data)\n",
    "        boxes = torch.zeros([N, 4], dtype=torch.float)\n",
    "        labels = torch.zeros([N], dtype=torch.int64)\n",
    "        areas = torch.zeros([N])\n",
    "        \n",
    "        for i in range(N):\n",
    "            bb = denormalize_bb(str_to_bb(data[i])[1:], img.shape[:2])\n",
    "            boxes[i][0],boxes[i][1],boxes[i][2],boxes[i][3] = bb\n",
    "            \n",
    "            if not self.single_component:\n",
    "                labels[i] = int(data[i][0])+1\n",
    "                continue\n",
    "                \n",
    "            labels[i] = 1\n",
    "                \n",
    "        return transform.to_tensor(img), {'boxes':boxes, 'labels':labels}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9230d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 [6/72] - Loss: nan \tProgress [1.19%] \tEpoch time elapsed: 0:00:46: 0:00:39:00:30"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-f2a9680946c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m                                                gamma=0.1)\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ssd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SSD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AllComp_v3_SGD-StepLR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-98-069a7b0c2da7>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mbatch_nr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-3f0af3024a9d>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'boxes'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_ssd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# backward compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#dataset_train = FasterRCNNSketchDataset('./dataset', 'train')\n",
    "#dataset_val = FasterRCNNSketchDataset('./dataset', 'val')\n",
    "\n",
    "dataset_train = SSDSketchDataset('./dataset', 'train')\n",
    "dataset_val = SSDSketchDataset('./dataset', 'val')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "        collate_fn=pytorchutils.utils.collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "        collate_fn=pytorchutils.utils.collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "num_classes = 13\n",
    "in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "\n",
    "model_ssd.to(device)\n",
    "\n",
    "\n",
    "params = [p for p in model_ssd.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "train_model(model_ssd, optimizer, data_loader, data_loader_val, device, 7, 'SSD', 'AllComp_v3_SGD-StepLR', lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5294132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cryslacks\\anaconda3\\envs\\nnlm\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 893.3196, 1647.6664, 2066.8818, 2961.3760],\n",
      "        [1756.8842, 1239.7880, 2076.8975, 1519.0928],\n",
      "        [ 791.0107, 1241.5090, 2697.6719, 2285.1575],\n",
      "        [1053.7938,    0.0000, 1488.4951, 1137.0680],\n",
      "        [2144.0461, 1058.5895, 2431.6250, 1755.1636],\n",
      "        [2361.3345,  961.9185, 2580.6604, 1573.9338],\n",
      "        [1109.5116,  939.8868, 1727.2297, 1578.8533],\n",
      "        [1984.7820, 1090.2424, 2211.6326, 1639.1440],\n",
      "        [2145.4233, 2433.6357, 2665.4685, 3517.1460],\n",
      "        [ 951.5978,  299.9469, 1718.7595,  870.7869],\n",
      "        [1057.4376,  492.3156, 1273.1948, 2936.8867],\n",
      "        [1163.5031,  162.9524, 1477.0153, 2667.8718],\n",
      "        [1005.8318, 1389.4165, 1240.4116, 3302.3110],\n",
      "        [1276.8878,    0.0000, 1710.3306,  890.1877],\n",
      "        [2208.7209,  588.4589, 2589.9365, 1778.2502],\n",
      "        [1200.8909, 1624.9567, 2564.8635, 2391.3796],\n",
      "        [ 812.0310,  802.6419, 1193.2656, 3180.7698],\n",
      "        [ 131.1717, 2241.9607,  358.6216, 3605.3259],\n",
      "        [1387.2264,  575.1398, 1652.3640, 2440.4763],\n",
      "        [1422.2126,  500.1151, 2268.7141, 3103.7085],\n",
      "        [1430.8237, 1992.4369, 1720.5560, 3611.2478],\n",
      "        [  26.5180, 2128.3062,  264.2825, 3596.9026],\n",
      "        [1259.0437, 1083.1219, 1458.8600, 2390.2854],\n",
      "        [2539.1357,  752.7930, 2797.8564, 1277.0354],\n",
      "        [2253.6584, 1884.0869, 2716.9534, 3056.6340],\n",
      "        [1179.5001, 2070.8389, 1460.4958, 3691.0920],\n",
      "        [ 960.2253, 2110.2710, 1186.2812, 3871.1147],\n",
      "        [ 829.7023,  683.2523, 1298.0469, 1379.4966],\n",
      "        [ 743.0276,  825.4794, 1087.0846, 1167.3024],\n",
      "        [1421.6748, 1186.4325, 1855.7098, 1328.8206],\n",
      "        [2096.8940, 1572.0779, 2352.9180, 1941.9838],\n",
      "        [ 825.0604, 2629.9116, 1285.3290, 3994.5247],\n",
      "        [2443.2969, 1137.0922, 2750.6931, 1752.7661],\n",
      "        [2472.7388,  682.7998, 2764.7798, 1776.8094],\n",
      "        [2251.8965, 1246.6669, 2524.8589, 1828.3649],\n",
      "        [1173.1132, 1225.2253, 2425.2480, 1863.4531],\n",
      "        [ 854.9512,   83.1979, 1425.0710,  769.6146],\n",
      "        [1893.9224,    0.0000, 2294.0449,  275.4036],\n",
      "        [ 619.1427, 1264.9066, 2016.8179, 2124.7786],\n",
      "        [1848.9080, 1132.3075, 3000.0000, 2348.7007],\n",
      "        [ 912.9169,  404.8904, 1355.7343, 1898.3667],\n",
      "        [  54.0511,  890.6824,  979.4089, 1546.9966],\n",
      "        [1901.0265, 2510.8784, 2488.3970, 3423.5854],\n",
      "        [ 145.1408, 1217.5519,  962.4916, 2778.9836],\n",
      "        [2312.7527,  432.0325, 2663.7568, 1503.0167],\n",
      "        [   0.0000,    0.0000,  772.1875, 1541.3987],\n",
      "        [2380.8181, 2271.1521, 2791.9839, 3284.6489],\n",
      "        [ 188.3139, 1079.4490,  634.4277, 2570.0198],\n",
      "        [1832.8147, 1063.2355, 2443.8914, 1773.9866],\n",
      "        [ 553.0533, 2215.0479,  963.2460, 2712.3672],\n",
      "        [2636.3950,  790.3350, 2953.3169, 1191.9143],\n",
      "        [ 380.3159,    0.0000,  895.9460, 1752.7681],\n",
      "        [1689.0505,    0.0000, 3000.0000,  810.6546],\n",
      "        [1348.5457, 2053.7012, 1593.3528, 3379.5071],\n",
      "        [ 173.4218,  784.0463,  852.1547, 1328.3352],\n",
      "        [1406.6461, 1075.8169, 1955.2273, 2829.9287],\n",
      "        [1585.1975, 2004.5676, 1951.1041, 3551.3459],\n",
      "        [1836.7795,   36.2727, 2225.3916,  513.2771],\n",
      "        [1456.5115,  318.2043, 1726.6754,  853.2755],\n",
      "        [ 557.7909,  806.5946, 1009.0491, 1107.2102],\n",
      "        [ 761.4766,  858.3209, 1624.7394, 1452.2432],\n",
      "        [ 745.7358, 1227.3464, 1031.4100, 3253.0835],\n",
      "        [ 550.2538,  311.7033,  934.2555, 2800.4475],\n",
      "        [ 784.7618, 1445.3843, 1879.0718, 2339.9351],\n",
      "        [1223.2546, 1085.5048, 1807.0510, 1279.9255],\n",
      "        [   0.0000, 2506.3965,  180.9558, 3480.1355],\n",
      "        [1208.6471,  203.6168, 1593.8435, 1498.1134],\n",
      "        [   0.0000, 2106.3586,   49.4573, 3389.6895],\n",
      "        [ 990.6943,  679.0754, 1673.2965, 1370.9323],\n",
      "        [1445.3834, 3093.0337, 1957.5048, 4000.0000],\n",
      "        [   0.0000, 1691.5474, 2992.2920, 2163.8867],\n",
      "        [1992.1167,  753.4099, 2532.1909, 1656.3878],\n",
      "        [1758.5862,  954.2836, 2420.6448, 3219.6750],\n",
      "        [1622.1107, 3151.3987, 2195.9543, 4000.0000],\n",
      "        [   0.0000, 2192.5039,  128.1529, 3096.1553],\n",
      "        [ 377.1253, 2192.5835, 1296.8477, 2567.8142],\n",
      "        [ 804.3375,  753.6677, 1164.6971, 1069.0818],\n",
      "        [ 170.4153, 2885.9084, 3000.0000, 3339.6326],\n",
      "        [ 143.9671, 2679.2092,  441.1383, 3866.1025],\n",
      "        [ 263.8758, 3045.9836,  666.6451, 3944.9275],\n",
      "        [ 463.2971, 2946.5596, 1109.9877, 4000.0000],\n",
      "        [2018.6780,    0.0000, 2343.6594,  599.8404],\n",
      "        [ 906.7042,    0.0000, 2193.2073, 3360.8828],\n",
      "        [2455.7310, 1665.0416, 2856.3479, 2778.6436],\n",
      "        [1218.5605,  367.2725, 1735.8748, 2054.0674],\n",
      "        [ 799.0668, 2226.1516, 1101.3954, 2838.3071],\n",
      "        [1397.1949,  231.0540, 1826.0444, 1448.8051],\n",
      "        [2045.3286, 1735.9517, 2485.4172, 3131.6018],\n",
      "        [2396.8960, 1272.1705, 2742.5156, 2165.2122],\n",
      "        [ 206.5140, 1827.0930, 2460.9817, 2312.2529],\n",
      "        [1113.4764, 1477.3121, 2704.7983, 2027.8745],\n",
      "        [  60.1398, 1968.1052,  805.1129, 2975.2996],\n",
      "        [2310.3545, 1302.9520, 2748.4023, 1923.2057],\n",
      "        [1525.9510,    0.0000, 1906.1089,  676.5175],\n",
      "        [ 485.6914,  892.4293, 1295.6633, 1485.2163],\n",
      "        [1493.6115,   95.1203, 1782.8538,  726.2224],\n",
      "        [ 690.9641,    0.0000, 1252.9397, 2294.7781],\n",
      "        [ 134.4852, 1511.7437, 2640.5662, 2106.4651],\n",
      "        [ 423.5755, 1046.7512,  865.9014, 2608.0837],\n",
      "        [   0.0000, 1613.5465, 1548.2496, 2258.6445],\n",
      "        [1213.9473, 1299.6299, 1365.4235, 2869.2642],\n",
      "        [ 634.0268,  703.5605,  995.0809, 1313.7010],\n",
      "        [ 742.3041,    0.0000, 1307.2601, 1149.5927],\n",
      "        [ 716.2104, 1373.9850, 1088.1661, 1721.8323],\n",
      "        [1240.2745, 3515.5647, 1693.2728, 4000.0000],\n",
      "        [ 279.5244,  486.4597,  814.7908, 2479.6541],\n",
      "        [2218.5959, 1485.2493, 2545.5872, 2159.2163],\n",
      "        [ 522.2022, 1928.4543, 1152.6003, 2286.5354],\n",
      "        [2162.1416,   89.1858, 2652.0757, 1314.4047],\n",
      "        [2108.6423,    0.0000, 2486.0542,  733.1858],\n",
      "        [1938.9471, 1083.5011, 2721.9780, 1949.1941],\n",
      "        [1523.6719, 3508.1978, 1981.2860, 4000.0000],\n",
      "        [   0.0000, 2146.8147,  253.6077, 2811.9009],\n",
      "        [1730.6543,    0.0000, 2127.4717,  307.6913],\n",
      "        [1911.5619, 2574.5286, 2460.8215, 3594.6833],\n",
      "        [1703.3478, 1938.1370, 2349.2161, 3297.6394],\n",
      "        [1535.6714, 1433.5083, 2120.2559, 1557.7325],\n",
      "        [1927.3123, 3361.4734, 2882.5876, 4000.0000],\n",
      "        [1505.7520, 1923.2570, 2719.7163, 2492.1511],\n",
      "        [2315.5854, 1933.3931, 2924.2305, 2804.4001],\n",
      "        [2446.1851,    0.0000, 2856.5188, 1426.6062],\n",
      "        [   0.0000, 1176.3665, 1333.9464, 1815.1820],\n",
      "        [1784.5039, 1456.3037, 2285.4683, 2523.1331],\n",
      "        [   0.0000, 2150.9421,  411.7560, 2830.5923],\n",
      "        [  24.9305, 1465.9524, 1851.2522, 1960.8093],\n",
      "        [ 682.5298, 3291.9253, 1326.0715, 4000.0000],\n",
      "        [2562.1560, 1298.4983, 2896.0171, 2029.4727],\n",
      "        [   0.0000, 3065.4189, 3000.0000, 3492.9553],\n",
      "        [ 396.0948, 2543.3071,  998.9795, 2965.7000],\n",
      "        [1899.3263, 3387.6882, 2445.0833, 4000.0000],\n",
      "        [1380.9463, 1716.9307, 2562.6111, 3231.9062],\n",
      "        [ 328.7797, 3292.1829,  913.1715, 4000.0000],\n",
      "        [1331.9995, 3493.9985, 1601.1240, 4000.0000],\n",
      "        [ 943.8538, 1031.8728, 2240.3127, 1728.6954],\n",
      "        [1419.8757, 1139.5461, 2223.7563, 1701.5483],\n",
      "        [1546.3875, 1985.1361, 1803.9100, 3217.7200],\n",
      "        [ 913.1940, 1109.4523, 2867.5806, 1529.8771],\n",
      "        [1360.4286,  915.7373, 1913.2643, 1543.0427],\n",
      "        [1027.5178, 1640.9912, 1885.9685, 2808.2563],\n",
      "        [1592.5463,   81.9863, 1993.0952,  525.3762],\n",
      "        [ 646.3632,  105.4462, 1307.2012, 1149.5194],\n",
      "        [1637.5981, 1192.2134, 2141.3594, 3206.1514],\n",
      "        [2574.9663, 1145.1398, 2932.8535, 1704.9790],\n",
      "        [2167.2703,    0.0000, 2560.2251,  546.4927],\n",
      "        [   0.0000, 1198.5813, 2584.4521, 2131.3237],\n",
      "        [1727.6027, 1464.8011, 3000.0000, 1855.2797],\n",
      "        [2213.0796,  869.3651, 2448.6101, 1427.8657],\n",
      "        [1812.8339,  296.7292, 2616.0305, 1123.1097],\n",
      "        [ 867.7172,  211.9732, 1851.8281, 2303.3081],\n",
      "        [2106.7632,  351.1994, 2381.1167,  567.0735],\n",
      "        [1882.1813,  309.8801, 2249.7781,  892.1613],\n",
      "        [ 382.1832, 2771.1243,  806.6176, 3918.1929],\n",
      "        [ 744.1750, 2583.1936, 1381.4060, 3588.8354],\n",
      "        [1023.0813, 1292.4851, 2291.0159, 3647.4316],\n",
      "        [ 958.0730,    0.0000, 1666.4293,  577.8560],\n",
      "        [1781.1360, 1544.1968, 1992.0961, 1735.0342],\n",
      "        [1922.3733,  718.3815, 2250.0747, 1166.1359],\n",
      "        [1687.4120, 1662.3047, 3000.0000, 2013.0497],\n",
      "        [1502.9730, 1767.6569, 3000.0000, 2603.3237],\n",
      "        [ 715.9023,    0.0000, 1204.6644,  286.4944],\n",
      "        [   0.0000, 1797.8406,  614.6454, 2701.2576],\n",
      "        [1588.1548,  270.3739, 1874.6532,  693.2590],\n",
      "        [2190.7476, 1307.7882, 2455.8979, 1486.7267],\n",
      "        [ 492.4359, 2711.8621, 1192.0942, 3307.0334],\n",
      "        [ 722.2527, 1159.6327, 2735.6226, 1553.2999],\n",
      "        [ 971.1456,  468.6713, 1219.6323,  763.1903],\n",
      "        [ 771.6394, 1151.8141, 1576.6587, 1627.6843],\n",
      "        [ 331.6602,    0.0000, 2273.0051, 1442.1755],\n",
      "        [ 442.8494,    0.0000,  888.2600,  350.6964],\n",
      "        [1306.4873, 1175.6281, 1459.5741, 3573.3284],\n",
      "        [   0.0000, 1883.8916,  543.7906, 2307.7766],\n",
      "        [1837.4016,    0.0000, 2377.8464, 1845.0500],\n",
      "        [   0.0000, 1290.9214,  285.8368, 1926.7059],\n",
      "        [1557.0542,  968.7194, 1771.2441, 3297.0522],\n",
      "        [ 561.3322,  234.2501,  920.4227, 1227.6082],\n",
      "        [ 209.8511, 2816.3801, 1017.5568, 3329.8567],\n",
      "        [ 256.2737, 1925.9861,  934.4807, 2235.4631],\n",
      "        [   0.0000, 2793.9607, 3000.0000, 3108.9631],\n",
      "        [1979.8083, 1959.7253, 2197.5403, 2218.4951],\n",
      "        [1254.8025, 2943.2898, 1647.1055, 3946.6858],\n",
      "        [ 864.0669,  399.5326, 1821.4208, 1622.5013],\n",
      "        [ 794.6071, 3074.1167, 1301.8147, 3785.2358],\n",
      "        [1534.7610, 3596.9050, 2163.5093, 4000.0000],\n",
      "        [1162.2090, 1584.0623, 1696.1602, 1862.3536],\n",
      "        [  44.8828, 2151.4246, 2728.6794, 2472.7412],\n",
      "        [2392.3552,  675.4734, 2558.0393, 1243.8755],\n",
      "        [   0.0000, 1900.6617, 1407.3679, 2347.9038],\n",
      "        [ 946.0255, 3089.4534, 1444.4580, 4000.0000],\n",
      "        [1382.9332,  370.3366, 1581.2157,  778.6830],\n",
      "        [ 584.5237,  943.7255,  946.9177, 1438.6119],\n",
      "        [1280.2078, 1298.3986, 1876.3708, 2202.3159],\n",
      "        [ 200.6777, 3531.7439,  351.8022, 4000.0000],\n",
      "        [1969.7950,    0.0000, 2829.9150, 2194.2554],\n",
      "        [   0.0000, 1640.2551,  420.0305, 2479.3306],\n",
      "        [   0.0000, 1919.3519, 2625.3564, 2843.4280],\n",
      "        [1528.2269, 2006.0273, 1730.8254, 2286.0791],\n",
      "        [1954.0216, 1338.3226, 2201.2502, 1527.4276],\n",
      "        [ 358.0247, 2527.5020, 1778.8914, 3135.7717],\n",
      "        [2281.6863,  983.5799, 2620.2576, 1374.2633],\n",
      "        [2315.2031,    0.0000, 3000.0000,  241.5447]],\n",
      "       grad_fn=<StackBackward0>), 'scores': tensor([0.9998, 0.9978, 0.9977, 0.9976, 0.9973, 0.9969, 0.9967, 0.9967, 0.9964,\n",
      "        0.9963, 0.9945, 0.9941, 0.9941, 0.9941, 0.9940, 0.9938, 0.9932, 0.9931,\n",
      "        0.9928, 0.9927, 0.9926, 0.9924, 0.9923, 0.9921, 0.9909, 0.9906, 0.9901,\n",
      "        0.9894, 0.9894, 0.9888, 0.9878, 0.9875, 0.9873, 0.9865, 0.9862, 0.9859,\n",
      "        0.9851, 0.9848, 0.9844, 0.9840, 0.9837, 0.9829, 0.9826, 0.9822, 0.9818,\n",
      "        0.9818, 0.9815, 0.9813, 0.9809, 0.9809, 0.9804, 0.9804, 0.9795, 0.9788,\n",
      "        0.9787, 0.9787, 0.9785, 0.9781, 0.9780, 0.9773, 0.9771, 0.9762, 0.9761,\n",
      "        0.9757, 0.9754, 0.9754, 0.9735, 0.9730, 0.9707, 0.9691, 0.9687, 0.9669,\n",
      "        0.9666, 0.9659, 0.9659, 0.9648, 0.9644, 0.9642, 0.9641, 0.9625, 0.9604,\n",
      "        0.9599, 0.9598, 0.9597, 0.9597, 0.9594, 0.9573, 0.9572, 0.9566, 0.9553,\n",
      "        0.9546, 0.9546, 0.9546, 0.9544, 0.9542, 0.9521, 0.9518, 0.9513, 0.9505,\n",
      "        0.9502, 0.9500, 0.9496, 0.9493, 0.9484, 0.9479, 0.9474, 0.9449, 0.9431,\n",
      "        0.9424, 0.9419, 0.9410, 0.9402, 0.9400, 0.9394, 0.9361, 0.9358, 0.9349,\n",
      "        0.9343, 0.9341, 0.9332, 0.9331, 0.9320, 0.9309, 0.9291, 0.9267, 0.9266,\n",
      "        0.9257, 0.9256, 0.9247, 0.9228, 0.9209, 0.9204, 0.9195, 0.9188, 0.9184,\n",
      "        0.9172, 0.9169, 0.9168, 0.9160, 0.9138, 0.9135, 0.9123, 0.9097, 0.9097,\n",
      "        0.9077, 0.9074, 0.9069, 0.9053, 0.9045, 0.9039, 0.9034, 0.9028, 0.9027,\n",
      "        0.9022, 0.9018, 0.9010, 0.9000, 0.8984, 0.8979, 0.8978, 0.8972, 0.8972,\n",
      "        0.8971, 0.8967, 0.8957, 0.8956, 0.8949, 0.8948, 0.8939, 0.8938, 0.8937,\n",
      "        0.8929, 0.8918, 0.8913, 0.8878, 0.8878, 0.8871, 0.8863, 0.8862, 0.8854,\n",
      "        0.8846, 0.8844, 0.8841, 0.8834, 0.8830, 0.8823, 0.8821, 0.8821, 0.8819,\n",
      "        0.8802, 0.8801, 0.8768, 0.8760, 0.8755, 0.8754, 0.8744, 0.8740, 0.8731,\n",
      "        0.8708, 0.8690], grad_fn=<IndexBackward0>), 'labels': tensor([ 3, 12,  3, 11, 12, 12, 10, 12, 11,  1, 11, 11, 11, 11, 11,  3, 11, 11,\n",
      "        11,  8, 11, 11, 11, 12, 11, 11, 11, 12, 12, 12, 12, 11, 12, 11, 12,  3,\n",
      "         1, 12, 10,  8, 10,  3, 11,  7, 11,  1, 11, 10, 11, 10, 12,  1,  5, 11,\n",
      "         3, 10, 11, 12, 12, 12, 10, 11,  7,  6, 12, 11, 11, 11, 12, 10,  4, 11,\n",
      "        10, 10, 11,  3, 12,  5, 11, 11, 11, 12,  8, 11,  8,  9, 11, 11, 12,  4,\n",
      "         4, 10, 11, 11, 10, 12,  1,  5, 10,  4, 11,  3, 11, 12,  6,  7, 12,  9,\n",
      "        11,  3, 10,  6, 11, 12,  7, 11, 12,  5,  3,  3, 11,  4, 11,  9,  4, 11,\n",
      "        12,  5,  5,  7,  3, 11, 11,  5, 10, 11,  4, 10, 10, 12,  1, 10, 12, 12,\n",
      "         4,  3, 12, 11,  8,  7, 12, 12,  3,  3,  1, 12, 12,  3,  5, 10, 11, 12,\n",
      "         7,  5,  5, 12, 10, 12, 10, 11,  9,  8, 11, 10,  9,  5,  9,  5,  9, 10,\n",
      "        10, 10,  7,  1,  4, 12,  4, 11, 12, 12,  1, 12,  8, 10,  4,  1,  7,  5,\n",
      "         5, 10])}]\n",
      "Faster-RCNN scores for bbs: 0.9998362064361572\n",
      "Faster-RCNN scores for bbs: 0.9978328347206116\n",
      "Faster-RCNN scores for bbs: 0.9976717829704285\n",
      "Faster-RCNN scores for bbs: 0.9976285099983215\n",
      "Faster-RCNN scores for bbs: 0.9972656965255737\n",
      "Faster-RCNN scores for bbs: 0.9968578815460205\n",
      "Faster-RCNN scores for bbs: 0.9967383742332458\n",
      "Faster-RCNN scores for bbs: 0.9967231154441833\n",
      "Faster-RCNN scores for bbs: 0.9964428544044495\n",
      "Faster-RCNN scores for bbs: 0.9963129162788391\n",
      "Faster-RCNN scores for bbs: 0.9945259094238281\n",
      "Faster-RCNN scores for bbs: 0.9941390752792358\n",
      "Faster-RCNN scores for bbs: 0.9940669536590576\n",
      "Faster-RCNN scores for bbs: 0.9940606951713562\n",
      "Faster-RCNN scores for bbs: 0.9940271377563477\n",
      "Faster-RCNN scores for bbs: 0.9938310384750366\n",
      "Faster-RCNN scores for bbs: 0.9932126998901367\n",
      "Faster-RCNN scores for bbs: 0.9930644631385803\n",
      "Faster-RCNN scores for bbs: 0.9927932024002075\n",
      "Faster-RCNN scores for bbs: 0.9927307963371277\n",
      "Faster-RCNN scores for bbs: 0.9926062822341919\n",
      "Faster-RCNN scores for bbs: 0.9923884272575378\n",
      "Faster-RCNN scores for bbs: 0.9922877550125122\n",
      "Faster-RCNN scores for bbs: 0.9920658469200134\n",
      "Faster-RCNN scores for bbs: 0.9908924102783203\n",
      "Faster-RCNN scores for bbs: 0.9905802607536316\n",
      "Faster-RCNN scores for bbs: 0.9900962114334106\n",
      "Faster-RCNN scores for bbs: 0.9894256591796875\n",
      "Faster-RCNN scores for bbs: 0.9893787503242493\n",
      "Faster-RCNN scores for bbs: 0.9887554049491882\n",
      "Faster-RCNN scores for bbs: 0.9877820611000061\n",
      "Faster-RCNN scores for bbs: 0.9874513745307922\n",
      "Faster-RCNN scores for bbs: 0.9873319268226624\n",
      "Faster-RCNN scores for bbs: 0.9864612221717834\n",
      "Faster-RCNN scores for bbs: 0.9861554503440857\n",
      "Faster-RCNN scores for bbs: 0.9859305620193481\n",
      "Faster-RCNN scores for bbs: 0.985137939453125\n",
      "Faster-RCNN scores for bbs: 0.9847841262817383\n",
      "Faster-RCNN scores for bbs: 0.9844251871109009\n",
      "Faster-RCNN scores for bbs: 0.9839972257614136\n",
      "Faster-RCNN scores for bbs: 0.983665943145752\n",
      "Faster-RCNN scores for bbs: 0.9829117059707642\n",
      "Faster-RCNN scores for bbs: 0.9825648665428162\n",
      "Faster-RCNN scores for bbs: 0.9821826815605164\n",
      "Faster-RCNN scores for bbs: 0.9817890524864197\n",
      "Faster-RCNN scores for bbs: 0.9817870855331421\n",
      "Faster-RCNN scores for bbs: 0.9815375804901123\n",
      "Faster-RCNN scores for bbs: 0.981279730796814\n",
      "Faster-RCNN scores for bbs: 0.9808696508407593\n",
      "Faster-RCNN scores for bbs: 0.9808525443077087\n",
      "Faster-RCNN scores for bbs: 0.980418860912323\n",
      "Faster-RCNN scores for bbs: 0.980368435382843\n",
      "Faster-RCNN scores for bbs: 0.979459822177887\n",
      "Faster-RCNN scores for bbs: 0.9788317680358887\n",
      "Faster-RCNN scores for bbs: 0.97870934009552\n",
      "Faster-RCNN scores for bbs: 0.9786513447761536\n",
      "Faster-RCNN scores for bbs: 0.978509783744812\n",
      "Faster-RCNN scores for bbs: 0.978135347366333\n",
      "Faster-RCNN scores for bbs: 0.9779995083808899\n",
      "Faster-RCNN scores for bbs: 0.9772621393203735\n",
      "Faster-RCNN scores for bbs: 0.9771126508712769\n",
      "Faster-RCNN scores for bbs: 0.9762427806854248\n",
      "Faster-RCNN scores for bbs: 0.9760605692863464\n",
      "Faster-RCNN scores for bbs: 0.9756637811660767\n",
      "Faster-RCNN scores for bbs: 0.9753790497779846\n",
      "Faster-RCNN scores for bbs: 0.9753639698028564\n",
      "Faster-RCNN scores for bbs: 0.973502516746521\n",
      "Faster-RCNN scores for bbs: 0.973045289516449\n",
      "Faster-RCNN scores for bbs: 0.970696747303009\n",
      "Faster-RCNN scores for bbs: 0.9691309928894043\n",
      "Faster-RCNN scores for bbs: 0.9687180519104004\n",
      "Faster-RCNN scores for bbs: 0.9668909907341003\n",
      "Faster-RCNN scores for bbs: 0.9665709137916565\n",
      "Faster-RCNN scores for bbs: 0.9659223556518555\n",
      "Faster-RCNN scores for bbs: 0.9659080505371094\n",
      "Faster-RCNN scores for bbs: 0.9647891521453857\n",
      "Faster-RCNN scores for bbs: 0.9643741846084595\n",
      "Faster-RCNN scores for bbs: 0.9641799926757812\n",
      "Faster-RCNN scores for bbs: 0.9641205072402954\n",
      "Faster-RCNN scores for bbs: 0.9624980092048645\n",
      "Faster-RCNN scores for bbs: 0.9603527188301086\n",
      "Faster-RCNN scores for bbs: 0.9599400758743286\n",
      "Faster-RCNN scores for bbs: 0.9598345160484314\n",
      "Faster-RCNN scores for bbs: 0.9597055912017822\n",
      "Faster-RCNN scores for bbs: 0.9596736431121826\n",
      "Faster-RCNN scores for bbs: 0.9594332575798035\n",
      "Faster-RCNN scores for bbs: 0.9572954773902893\n",
      "Faster-RCNN scores for bbs: 0.9571919441223145\n",
      "Faster-RCNN scores for bbs: 0.9566054344177246\n",
      "Faster-RCNN scores for bbs: 0.955265462398529\n",
      "Faster-RCNN scores for bbs: 0.9546314477920532\n",
      "Faster-RCNN scores for bbs: 0.95458984375\n",
      "Faster-RCNN scores for bbs: 0.9545777440071106\n",
      "Faster-RCNN scores for bbs: 0.954441249370575\n",
      "Faster-RCNN scores for bbs: 0.9542001485824585\n",
      "Faster-RCNN scores for bbs: 0.952078640460968\n",
      "Faster-RCNN scores for bbs: 0.9517703652381897\n",
      "Faster-RCNN scores for bbs: 0.9512792825698853\n",
      "Faster-RCNN scores for bbs: 0.9504976868629456\n",
      "Faster-RCNN scores for bbs: 0.950186550617218\n",
      "Faster-RCNN scores for bbs: 0.9500030875205994\n",
      "Faster-RCNN scores for bbs: 0.9496065974235535\n",
      "Faster-RCNN scores for bbs: 0.9493445754051208\n",
      "Faster-RCNN scores for bbs: 0.9483559727668762\n",
      "Faster-RCNN scores for bbs: 0.9479491710662842\n",
      "Faster-RCNN scores for bbs: 0.9473559856414795\n",
      "Faster-RCNN scores for bbs: 0.9449019432067871\n",
      "Faster-RCNN scores for bbs: 0.9430837035179138\n",
      "Faster-RCNN scores for bbs: 0.9424300789833069\n",
      "Faster-RCNN scores for bbs: 0.9418738484382629\n",
      "Faster-RCNN scores for bbs: 0.9409857988357544\n",
      "Faster-RCNN scores for bbs: 0.9401677250862122\n",
      "Faster-RCNN scores for bbs: 0.9400115013122559\n",
      "Faster-RCNN scores for bbs: 0.9393911361694336\n",
      "Faster-RCNN scores for bbs: 0.9361371397972107\n",
      "Faster-RCNN scores for bbs: 0.9357975125312805\n",
      "Faster-RCNN scores for bbs: 0.9349304437637329\n",
      "Faster-RCNN scores for bbs: 0.9342994689941406\n",
      "Faster-RCNN scores for bbs: 0.9341042041778564\n",
      "Faster-RCNN scores for bbs: 0.9331733584403992\n",
      "Faster-RCNN scores for bbs: 0.9331218600273132\n",
      "Faster-RCNN scores for bbs: 0.9319671392440796\n",
      "Faster-RCNN scores for bbs: 0.93092942237854\n",
      "Faster-RCNN scores for bbs: 0.9291011691093445\n",
      "Faster-RCNN scores for bbs: 0.926729679107666\n",
      "Faster-RCNN scores for bbs: 0.926618218421936\n",
      "Faster-RCNN scores for bbs: 0.9256826043128967\n",
      "Faster-RCNN scores for bbs: 0.925603449344635\n",
      "Faster-RCNN scores for bbs: 0.9246722459793091\n",
      "Faster-RCNN scores for bbs: 0.9228023886680603\n",
      "Faster-RCNN scores for bbs: 0.9208996891975403\n",
      "Faster-RCNN scores for bbs: 0.9204421639442444\n",
      "Faster-RCNN scores for bbs: 0.9195050597190857\n",
      "Faster-RCNN scores for bbs: 0.9187524914741516\n",
      "Faster-RCNN scores for bbs: 0.9184008240699768\n",
      "Faster-RCNN scores for bbs: 0.9172263145446777\n",
      "Faster-RCNN scores for bbs: 0.9169245362281799\n",
      "Faster-RCNN scores for bbs: 0.916803777217865\n",
      "Faster-RCNN scores for bbs: 0.9160265326499939\n",
      "Faster-RCNN scores for bbs: 0.9137557744979858\n",
      "Faster-RCNN scores for bbs: 0.9134959578514099\n",
      "Faster-RCNN scores for bbs: 0.9122606515884399\n",
      "Faster-RCNN scores for bbs: 0.9097422957420349\n",
      "Faster-RCNN scores for bbs: 0.9096528887748718\n",
      "Faster-RCNN scores for bbs: 0.9076658487319946\n",
      "Faster-RCNN scores for bbs: 0.90744948387146\n",
      "Faster-RCNN scores for bbs: 0.9068925976753235\n",
      "Faster-RCNN scores for bbs: 0.9053210616111755\n",
      "Faster-RCNN scores for bbs: 0.9045034050941467\n",
      "Faster-RCNN scores for bbs: 0.903930127620697\n",
      "Faster-RCNN scores for bbs: 0.9034193158149719\n",
      "Faster-RCNN scores for bbs: 0.9027633666992188\n",
      "Faster-RCNN scores for bbs: 0.9026825428009033\n",
      "Faster-RCNN scores for bbs: 0.9022320508956909\n",
      "Faster-RCNN scores for bbs: 0.9018447399139404\n",
      "Faster-RCNN scores for bbs: 0.9009507894515991\n",
      "Faster-RCNN scores for bbs: 0.899976909160614\n",
      "Faster-RCNN scores for bbs: 0.8984400629997253\n",
      "Faster-RCNN scores for bbs: 0.8979192972183228\n",
      "Faster-RCNN scores for bbs: 0.8978185653686523\n",
      "Faster-RCNN scores for bbs: 0.8972421884536743\n",
      "Faster-RCNN scores for bbs: 0.8972171545028687\n",
      "Faster-RCNN scores for bbs: 0.8971492052078247\n",
      "Faster-RCNN scores for bbs: 0.8966513276100159\n",
      "Faster-RCNN scores for bbs: 0.8956677913665771\n",
      "Faster-RCNN scores for bbs: 0.8956331014633179\n",
      "Faster-RCNN scores for bbs: 0.8948991298675537\n",
      "Faster-RCNN scores for bbs: 0.8948203325271606\n",
      "Faster-RCNN scores for bbs: 0.8939095139503479\n",
      "Faster-RCNN scores for bbs: 0.8937793374061584\n",
      "Faster-RCNN scores for bbs: 0.8937316536903381\n",
      "Faster-RCNN scores for bbs: 0.8929213881492615\n",
      "Faster-RCNN scores for bbs: 0.8917789459228516\n",
      "Faster-RCNN scores for bbs: 0.8912559747695923\n",
      "Faster-RCNN scores for bbs: 0.8878133296966553\n",
      "Faster-RCNN scores for bbs: 0.8877692222595215\n",
      "Faster-RCNN scores for bbs: 0.8870987296104431\n",
      "Faster-RCNN scores for bbs: 0.8862890601158142\n",
      "Faster-RCNN scores for bbs: 0.8862234950065613\n",
      "Faster-RCNN scores for bbs: 0.885377824306488\n",
      "Faster-RCNN scores for bbs: 0.8846331238746643\n",
      "Faster-RCNN scores for bbs: 0.8844462037086487\n",
      "Faster-RCNN scores for bbs: 0.8841246366500854\n",
      "Faster-RCNN scores for bbs: 0.8834113478660583\n",
      "Faster-RCNN scores for bbs: 0.8829723596572876\n",
      "Faster-RCNN scores for bbs: 0.8822768330574036\n",
      "Faster-RCNN scores for bbs: 0.8821431398391724\n",
      "Faster-RCNN scores for bbs: 0.8820582032203674\n",
      "Faster-RCNN scores for bbs: 0.8819457292556763\n",
      "Faster-RCNN scores for bbs: 0.8801610469818115\n",
      "Faster-RCNN scores for bbs: 0.880071759223938\n",
      "Faster-RCNN scores for bbs: 0.8767629265785217\n",
      "Faster-RCNN scores for bbs: 0.8759927153587341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faster-RCNN scores for bbs: 0.8755373954772949\n",
      "Faster-RCNN scores for bbs: 0.875408947467804\n",
      "Faster-RCNN scores for bbs: 0.8743588924407959\n",
      "Faster-RCNN scores for bbs: 0.8739598393440247\n",
      "Faster-RCNN scores for bbs: 0.8731203675270081\n",
      "Faster-RCNN scores for bbs: 0.8707557320594788\n",
      "Faster-RCNN scores for bbs: 0.8690065741539001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ssd.eval()\n",
    "\n",
    "im =\"./components.jpg\"\n",
    "image =\"components.jpg\"\n",
    "#label = \"./dataset/labels/\"+(image.split('.')[0]+'.txt')\n",
    "img = cv2.imread(im)\n",
    "cv2_img = cv2.imread(im)\n",
    "#imgray = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "#ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "predictions = model_ssd(tensor_img)\n",
    "\n",
    "#pred = model_ssd(dataset_train[0][0])\n",
    "print(predictions)\n",
    "\n",
    "for i in range(len(predictions[0]['boxes'])):\n",
    "    score = predictions[0]['scores'][i].item()\n",
    "    if score > 0.5:\n",
    "        x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "        cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "        cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        print('Faster-RCNN scores for bbs:', score)\n",
    "cv2.imwrite('./results/all-ssd/'+image, cv2_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520f142",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112a0b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=13, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=52, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_all = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "num_classes = 13\n",
    "in_features = model_all.roi_heads.box_predictor.cls_score.in_features\n",
    "model_all.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model_all.to(device)\n",
    "\n",
    "params = [p for p in model_all.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, weight_decay=0.0005)\n",
    "\n",
    "checkpoint = torch.load('./models/Faster-RCNN/All_v4/All-Comp_v4_SGD-StepLR-7.pt')\n",
    "model_all.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model_all.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984dd5dc",
   "metadata": {},
   "source": [
    "## Optical character recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d961e028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchvision.models.detection.ssd' from 'C:\\\\Users\\\\Cryslacks\\\\anaconda3\\\\envs\\\\nnlm\\\\lib\\\\site-packages\\\\torchvision\\\\models\\\\detection\\\\ssd.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models.detection.ssd\n",
    "torchvision.models.detection.ssd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874be976",
   "metadata": {},
   "source": [
    "## Post processing engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb089a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=False):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = numpy.clip(numpy.random.beta(alpha, alpha), 0.4, 0.6)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y.numpy(), y[index].numpy()\n",
    "    mixedup_bboxes = []\n",
    "    for bbox, s_bbox in zip(y_a, y_b):\n",
    "        mixedup_bboxes.append([bbox,s_bbox])\n",
    "        \n",
    "    return mixed_x, index, torch.tensor(mixedup_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cccf255",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9c34eb8f5069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#image /= 255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixup_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#print(mix.numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#cv2.imshow('img', mix.numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "im1 = cv2.imread('./new_more_dataset/Checkbox/IMG20220209145109.jpg')\n",
    "im2 = cv2.imread('./new_more_dataset/Button/IMG20220209143703.jpg')\n",
    "im3 = cv2.imread('./new_more_dataset/Header/IMG20220209144530.jpg')\n",
    "im4 = cv2.imread('./new_more_dataset/List/IMG20220209145958_01.jpg')\n",
    "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(numpy.float32)\n",
    "#image /= 255.0\n",
    "\n",
    "mix = mixup_data(torch.tensor(numpy.array([im1, im2])), [[1],[2]])\n",
    "#print(mix.numpy())\n",
    "#cv2.imshow('img', mix.numpy())\n",
    "cv2.imwrite('./Looool1.jpg', mix[0].numpy()[0])\n",
    "cv2.imwrite('./Looool2.jpg', mix[0].numpy()[1])\n",
    "cv2.imwrite('./Looool3.jpg', mix[0].numpy()[2])\n",
    "cv2.imwrite('./Looool4.jpg', mix[0].numpy()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "112914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy right of https://github.com/sunshiding/ssd-pytorch-custom/blob/7d036770a6ec616fa7374bc8c798a3fd05888b33/layers/modules/multibox_loss.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + aLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by a which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 device):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.use_gpu = False if device.type == 'cpu' else True\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = [0.1, 0.2]\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "            targets (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold, truths, defaults, self.variance, labels,\n",
    "                  loc_t, conf_t, idx)\n",
    "        loc_t = loc_t.to(self.device)\n",
    "        conf_t = conf_t.to(self.device)\n",
    "        # wrap targets\n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c[pos.view(-1, 1)] = 0  # filter out pos boxes for now UPDATED\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.view(1, -1).long().sum(1, keepdim=True) # UPDATED\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + aLloc(x,l,g)) / N\n",
    "\n",
    "        N = num_pos.data.sum().float() # UPDATED\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce632dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
