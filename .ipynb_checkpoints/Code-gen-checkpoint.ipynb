{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee42a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Create function files for every component\n",
    "# 2. Create main structure of main.tsx file\n",
    "# 3. Go through the result json string and form the site\n",
    "#\n",
    "# TODO OUTSIDE: \n",
    "# - Change comp_definitions to a json structure\n",
    "# - Ask the supervisor regarding the depth of my \"scientific\" part\n",
    "# \n",
    "# Functions:\n",
    "#   Name: Component():\n",
    "#   Arguments: IDX, Text and Reference text\n",
    "#   Output: Imports, Variables and Objects (All strings)\n",
    "#\n",
    "#   Definitions for arguments:\n",
    "#     IDX, a integer that represents the number of times this component has been represented (used for variables and naming)\n",
    "#     Text, a string containing the text that the component may contain\n",
    "#     Reference text, a string containing the text that may be referenced to the component\n",
    "#   Definitions for output: \n",
    "#     Imports, a string containing all imports that are required to be made to support the component\n",
    "#     Variables, a string containing all variables that are required to be made to support the component\n",
    "#     Objects, a string containing all the objects that are required to be made to view the component\n",
    "#\n",
    "# https://mui.com/components/data-grid/components/#main-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from importlib import reload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('code-gen/')\n",
    "\n",
    "def generate_code(json_str, code_gen_name, path_to_save, import_indentation=False):\n",
    "    json_dict = json.loads(json_str)\n",
    "    result_txt = ''\n",
    "    indentation = 0\n",
    "    code_gen = __import__(code_gen_name)\n",
    "    reload(code_gen) # For debug purposes right now\n",
    "    imports, var_begin, obj_begin, obj_end = code_gen.core()\n",
    "    str_code,components,variables = iterate_rows(json_dict, [], [], code_gen_name)\n",
    "    \n",
    "    comp_imports = code_gen.create_imports(components)\n",
    "    \n",
    "    if import_indentation:\n",
    "        indentation, imports = fix_indentation(indentation, imports)\n",
    "        indentation, comp_imports = fix_indentation(indentation, comp_imports)\n",
    "    \n",
    "    if ''.join(imports.split(' ')) != '':\n",
    "        result_txt += imports+'\\n'\n",
    "    if ''.join(comp_imports.split(' ')) != '':\n",
    "        result_txt += comp_imports+'\\n'\n",
    "    \n",
    "    indentation, var_begin = fix_indentation(indentation, var_begin)\n",
    "    result_txt += var_begin+'\\n'\n",
    "\n",
    "    for comp_var in variables:\n",
    "        indentation, comp_var_str = fix_indentation(indentation, comp_var)\n",
    "        result_txt += comp_var_str+'\\n'\n",
    "    result_txt += '\\n'  \n",
    "    \n",
    "    indentation, obj_begin = fix_indentation(indentation, obj_begin)\n",
    "    indentation, str_code = fix_indentation(indentation, str_code)\n",
    "    indentation, obj_end = fix_indentation(indentation, obj_end)\n",
    "\n",
    "    result_txt += obj_begin+'\\n'\n",
    "    result_txt += str_code+'\\n'\n",
    "    result_txt += obj_end\n",
    "    \n",
    "    f = open(path_to_save, 'w')\n",
    "    f.write(result_txt)\n",
    "    f.close()\n",
    "\n",
    "def fix_indentation(curr_indentation, curr_str):\n",
    "    inden_increase = ['{', '(', '>', '[']\n",
    "    inden_decrease = ['}', ')', '</', ']', '/>']\n",
    "    \n",
    "    str_arr = curr_str.split('\\n')\n",
    "    for i in range(len(str_arr)):\n",
    "        b_indented = False\n",
    "        first_two = str_arr[i][0:2]\n",
    "            \n",
    "        # Check if we need to decrease indentation\n",
    "        if first_two in inden_decrease:\n",
    "            curr_indentation -= 1\n",
    "        elif len(first_two) == 2 and first_two[0] in inden_decrease:\n",
    "            curr_indentation -= 1\n",
    "        else:\n",
    "            if i == 0:\n",
    "                str_arr[i] = curr_indentation*'  '+str_arr[i]\n",
    "                b_indented = True\n",
    "            else:                \n",
    "                # Check if we need to increase indentation\n",
    "                has_html_closing = len(str_arr[i-1].split('</')) > 1\n",
    "                has_html_ending = len(str_arr[i-1].split('/>')) > 1\n",
    "                last_two = str_arr[i-1][-2:]\n",
    "                if last_two in inden_decrease:\n",
    "                    pass\n",
    "                elif has_html_closing:\n",
    "                    pass\n",
    "                elif has_html_ending:\n",
    "                    pass\n",
    "                elif last_two[1] in inden_increase:\n",
    "                    curr_indentation += 1\n",
    "        \n",
    "        if not b_indented:   \n",
    "            str_arr[i] = curr_indentation*'  '+str_arr[i]\n",
    "    \n",
    "        # Check if we need to decrease or decrease next indentation\n",
    "        if i == len(str_arr)-1:\n",
    "            first_two = ''.join(str_arr[i].split(' '))[0:2]\n",
    "            if first_two in inden_decrease:\n",
    "                pass\n",
    "            elif first_two[0] in inden_decrease:\n",
    "                pass\n",
    "            else:\n",
    "                last_two = str_arr[i][-2:]\n",
    "                if last_two in inden_decrease:\n",
    "                    pass\n",
    "                elif last_two[1] in inden_increase:\n",
    "                    curr_indentation += 1\n",
    "        \n",
    "    new_str = '\\n'.join([str(e) for e in str_arr])\n",
    "    return curr_indentation, new_str\n",
    "\n",
    "# Iterates through the rows of the json dict, saving the components found to be able to have correct imports\n",
    "def iterate_rows(json_dict, components, variables, code_gen_name, recurrent=False):\n",
    "    str_code = \"\"\n",
    "    exec(\"import \"+code_gen_name)\n",
    "    reference_functions = {}\n",
    "    \n",
    "    if \"objects\" in json_dict.keys():\n",
    "        for i in range(len(json_dict[\"objects\"])):\n",
    "            obj = json_dict[\"objects\"][str(i)]\n",
    "            try:\n",
    "                api_code, func_name = eval(code_gen_name+'.'+obj+'()')\n",
    "                variables.append(api_code)\n",
    "                reference_functions[obj] = func_name\n",
    "            except AttributeError:\n",
    "                print('[Warning] Tried to load unsupported object \"'+obj+'\"')\n",
    "    \n",
    "    for i in range(len(json_dict[\"rows\"])):\n",
    "        row = json_dict[\"rows\"][str(i)]\n",
    "        \n",
    "        if 'component' in row.keys():\n",
    "            comp_name = row['component']\n",
    "            print('Found component:', comp_name)\n",
    "            components.append(comp_name)\n",
    "            \n",
    "            comp_idx = components.count(comp_name)\n",
    "            comp_text = ''\n",
    "            comp_reference = ''\n",
    "            \n",
    "            if 'text' in row.keys():\n",
    "                comp_text = row['text']\n",
    "            if 'reference' in row.keys():\n",
    "                comp_reference = row['reference']\n",
    "                \n",
    "            if comp_reference in reference_functions.keys():\n",
    "                comp_vars,comp_code = eval(code_gen_name+'.'+comp_name+'('+str(comp_idx)+', \"'+comp_text+'\",\"'+reference_functions[comp_reference]+'\", True)')\n",
    "            else:\n",
    "                comp_vars,comp_code = eval(code_gen_name+'.'+comp_name+'('+str(comp_idx)+', \"'+comp_text+'\",\"'+comp_reference+'\")')\n",
    "                \n",
    "            if comp_vars != '':\n",
    "                variables.append(comp_vars)\n",
    "            if not recurrent:\n",
    "                begin_row,end_row = eval(code_gen_name+'.row(1)')\n",
    "                str_code += begin_row[0]+'\\n'+comp_code+'\\n'+end_row[0]+'\\n'\n",
    "            else:\n",
    "                str_code += comp_code+'\\n'\n",
    "        elif 'cols' in row.keys():\n",
    "            comp_codes = []\n",
    "            for c in range(len(row['cols'])):\n",
    "                col = row['cols'][str(c)]\n",
    "                \n",
    "                if 'component' in col.keys():\n",
    "                    comp_name = col['component']\n",
    "                    print('Found component in column:', comp_name)\n",
    "                    components.append(comp_name)\n",
    "\n",
    "                    comp_idx = components.count(comp_name)\n",
    "                    comp_text = ''\n",
    "                    comp_reference = ''\n",
    "                    \n",
    "                    if 'text' in col.keys():\n",
    "                        comp_text = col['text']\n",
    "                    if 'reference' in col.keys():\n",
    "                        comp_reference = col['reference']\n",
    "                         \n",
    "                    if comp_reference in reference_functions.keys():\n",
    "                        comp_vars,comp_code = eval(code_gen_name+'.'+comp_name+'('+str(comp_idx)+', \"'+comp_text+'\",\"'+reference_functions[comp_reference]+'\", True)')\n",
    "                    else:\n",
    "                        comp_vars,comp_code = eval(code_gen_name+'.'+comp_name+'('+str(comp_idx)+', \"'+comp_text+'\",\"'+comp_reference+'\")')\n",
    "\n",
    "                    if comp_vars != '':\n",
    "                        variables.append(comp_vars)\n",
    "                    comp_codes.append(comp_code)\n",
    "                elif 'rows' in col.keys():\n",
    "                    str_code2, components2, variables2 = iterate_rows(col, components, variables, code_gen_name, True)\n",
    "                    comp_codes.append(str_code2)\n",
    "                    \n",
    "            begin_row,end_row = eval(code_gen_name+'.row('+str(len(row['cols']))+')')\n",
    "            str_code += begin_row[0]+'\\n'\n",
    "            for c in range(len(row['cols'])):\n",
    "                str_code += begin_row[c+1]+'\\n'+comp_codes[c]+'\\n'+end_row[c]+\"\\n\"\n",
    "            str_code = str_code[:-1]\n",
    "            str_code += end_row[len(end_row)-1]+'\\n'\n",
    "\n",
    "    return str_code[:-1], components, variables             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb59320",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_tests = {'doublelist_datagrid_complex':'{\"rows\": {\"0\": {\"cols\": {\"0\": {\"rows\": {\"0\": {\"component\": \"Datagrid\"}, \"1\": {\"component\": \"Dropdown\"}, \"2\": {\"component\": \"Checkbox\"}}}, \"1\": {\"rows\": {\"0\": {\"component\": \"List\"}, \"1\": {\"component\": \"Input\"}}}, \"2\": {\"component\": \"List\"}}}}}',\n",
    "                  'loginwebsite_highcaps':'{\"rows\": {\"0\": {\"component\": \"Header\", \"text\": \"WEBSITE\"}, \"1\": {\"component\": \"Input\"}, \"2\": {\"cols\": {\"0\": {\"component\": \"Input\"}, \"1\": {\"component\": \"Checkbox\", \"text\": \"save\"}}}, \"3\": {\"component\": \"Button\", \"text\": \"LOGIN\", \"reference\": \"Loin\"}},\"objects\":{\"0\":\"Loin\"}}',\n",
    "                  'middlecomp_highcaps':'{\"rows\": {\"0\": {\"component\": \"Paragraph\", \"text\": \"TExT\"}, \"1\": {\"cols\": {\"0\": {\"rows\": {\"0\": {\"component\": \"Radiobutton\", \"text\": \"6o06l E\"}, \"1\": {\"component\": \"Dropdown\"}, \"2\": {\"component\": \"Combobox\"}}}, \"1\": {\"component\": \"List\"}, \"2\": {\"rows\": {\"0\": {\"component\": \"Input\"}, \"1\": {\"component\": \"Datagrid\"}}}}}, \"2\": {\"component\": \"Combobox\"}}}',\n",
    "                  'test_doublelist':'{\"rows\": {\"0\": {\"component\": \"Paragraph\", \"text\": \"its\"}, \"1\": {\"cols\": {\"0\": {\"component\": \"List\"}, \"1\": {\"rows\": {\"0\": {\"component\": \"Checkbox\"}, \"1\": {\"component\": \"Combobox\"}, \"2\": {\"component\": \"Paragraph\", \"text\": \"ten\"}}}, \"2\": {\"component\": \"List\"}}}, \"2\": {\"cols\": {\"0\": {\"component\": \"Combobox\"}, \"1\": {\"component\": \"Paragraph\", \"text\": \"test\"}}}}}',\n",
    "                  'words_highcaps':'{\"rows\": {\"0\": {\"cols\": {\"0\": {\"component\": \"Paragraph\", \"text\": \"Words\"}, \"1\": {\"component\": \"Paragraph\", \"text\": \"PARAGRAPH\"}}}, \"1\": {\"cols\": {\"0\": {\"component\": \"Datagrid\"}, \"1\": {\"rows\": {\"0\": {\"component\": \"Checkbox\", \"text\": \"SAVE\"}, \"1\": {\"component\": \"Radiobutton\"}, \"2\": {\"component\": \"Radiobutton\"}}}}}, \"2\": {\"cols\": {\"0\": {\"component\": \"Combobox\"}, \"1\": {\"component\": \"Paragraph\", \"text\": \"search\"}}}},\"objects\":{\"0\":\"Loin\"}}'}\n",
    "reference_tests = lambda y : '{\"rows\": {\"0\": {\"component\": \"'+y+'\", \"text\": \"Test\", \"reference\": \"Test\"}},\"objects\":{\"0\":\"Test\"}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5948eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"rows\": {\"0\": {\"cols\": {\"0\": {\"component\": \"Paragraph\", \"text\": \"Words\"}, \"1\": {\"component\": \"Paragraph\", \"text\": \"PARAGRAPH\"}}}, \"1\": {\"cols\": {\"0\": {\"component\": \"Datagrid\"}, \"1\": {\"rows\": {\"0\": {\"component\": \"Checkbox\", \"text\": \"SAVE\"}, \"1\": {\"component\": \"Radiobutton\"}, \"2\": {\"component\": \"Radiobutton\"}}}}}, \"2\": {\"cols\": {\"0\": {\"component\": \"Combobox\"}, \"1\": {\"component\": \"Paragraph\", \"text\": \"search\"}}}},\"objects\":{\"0\":\"Loin\"}}\n"
     ]
    }
   ],
   "source": [
    "print(website_tests['words_highcaps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22dd5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found component: Datagrid\n",
      "Found component: Dropdown\n",
      "Found component: Checkbox\n",
      "Found component: List\n",
      "Found component: Input\n",
      "Found component in column: List\n"
     ]
    }
   ],
   "source": [
    "generate_code(website_tests['doublelist_datagrid_complex'], 'ReactMUI', 'C:/Users/David Eriksson/Desktop/test/src/main.tsx')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8857b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = '{\"rows\": {\"0\": {\"component\": \"Header\", \"text\": \"INTERNET\"}, \"1\": {\"cols\": {\"0\": {\"component\": \"Dropdown\"}, \"1\": {\"component\": \"Combobox\"}}}, \"2\": {\"cols\": {\"0\": {\"rows\": {\"0\": {\"component\": \"Checkbox\", \"text\": \"SAVE\"}, \"1\": {\"component\": \"Button\"}}}, \"1\": {\"component\": \"List\"}}}}, \"objects\": {}}'\n",
    "\n",
    "generate_code(thing, 'ReactMUI', 'C:/Users/David Eriksson/Desktop/test/src/main.tsx')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2402ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found component: List\n"
     ]
    }
   ],
   "source": [
    "generate_code(website_tests['doublelist_datagrid_complex'], 'HTML_JS', 'C:/Users/David Eriksson/Desktop/test/src/website.html', import_indentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c41adcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error unsupported component\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exec(\"import ReactMUI\")\n",
    "try:\n",
    "    eval('ReactMUI.Test(0, \"test\", \"\")')\n",
    "except AttributeError:\n",
    "    print('Error unsupported component')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692da63",
   "metadata": {},
   "source": [
    "Website 1                |Website 2\n",
    "-------------------------|-------------------------\n",
    "![](./code-gen/loginwebsite_highcaps.jpg)  |  ![](./code-gen/whiteboard_website.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "baa54c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mAP]\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:14:15\n",
      "Values @90:\n",
      "16.72012166570391\n",
      "\n",
      "Header \t\t-> 0.0\n",
      "Input \t\t-> 31.964823546973648\n",
      "Button \t\t-> 0\n",
      "List \t\t-> 24.747474747474747\n",
      "Datagrid \t-> 39.15629824720734\n",
      "Dropdown \t-> 25.060606060606055\n",
      "Combobox \t-> 32.2038567493113\n",
      "Checkbox \t-> 0.0\n",
      "Radiobutton \t-> 0.0\n",
      "Reference \t-> 23.030679213834294\n",
      "Object \t\t-> 35.232679643265044\n",
      "ReferenceHead \t-> 5.965163445478406\n",
      "Image \t\t-> 0.0\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:08:30\n",
      "Values @75:\n",
      "38.09229011410074\n",
      "\n",
      "Header \t\t-> 9.090909090909092\n",
      "Input \t\t-> 45.45454545454545\n",
      "Button \t\t-> 0\n",
      "List \t\t-> 54.54545454545454\n",
      "Datagrid \t-> 54.54545454545454\n",
      "Dropdown \t-> 27.27272727272727\n",
      "Combobox \t-> 63.63636363636363\n",
      "Checkbox \t-> 20.715528490648108\n",
      "Radiobutton \t-> 29.960039960039957\n",
      "Reference \t-> 64.1916176242195\n",
      "Object \t\t-> 54.2251559714795\n",
      "ReferenceHead \t-> 71.56197489146797\n",
      "Image \t\t-> 0.0\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:08:29\n",
      "Values @50:\n",
      "51.053977920575534\n",
      "\n",
      "Header \t\t-> 38.087039223402854\n",
      "Input \t\t-> 45.45454545454545\n",
      "Button \t\t-> 0\n",
      "List \t\t-> 54.54545454545454\n",
      "Datagrid \t-> 54.54545454545454\n",
      "Dropdown \t-> 27.27272727272727\n",
      "Combobox \t-> 63.63636363636363\n",
      "Checkbox \t-> 81.81818181818183\n",
      "Radiobutton \t-> 41.75084175084175\n",
      "Reference \t-> 98.8941001939769\n",
      "Object \t\t-> 62.77646595828414\n",
      "ReferenceHead \t-> 94.92053856824903\n",
      "Image \t\t-> 0.0\n",
      "\n",
      "[Image validation]\n",
      "[Eval] mAP [36/36]\tEpoch time elapsed: 0:11:15\n",
      "Values @90:\n",
      "82.8599392392785\n",
      "\n",
      "Header \t\t-> 90.9090909090909\n",
      "Input \t\t-> 90.9090909090909\n",
      "Button \t\t-> 0\n",
      "List \t\t-> 90.9090909090909\n",
      "Datagrid \t-> 100.0\n",
      "Dropdown \t-> 90.9090909090909\n",
      "Combobox \t-> 90.9090909090909\n",
      "Checkbox \t-> 100.0\n",
      "Radiobutton \t-> 100.0\n",
      "Reference \t-> 90.9090909090909\n",
      "Object \t\t-> 90.9090909090909\n",
      "ReferenceHead \t-> 49.90648283789335\n",
      "Image \t\t-> 90.9090909090909\n",
      "[Eval] mAP [36/36]\tEpoch time elapsed: 0:11:16\n",
      "Values @75:\n",
      "87.41258741258743\n",
      "\n",
      "Header \t\t-> 90.9090909090909\n",
      "Input \t\t-> 90.9090909090909\n",
      "Button \t\t-> 0\n",
      "List \t\t-> 90.9090909090909\n",
      "Datagrid \t-> 100.0\n",
      "Dropdown \t-> 90.9090909090909\n",
      "Combobox \t-> 100.0\n",
      "Checkbox \t-> 100.0\n",
      "Radiobutton \t-> 100.0\n",
      "Reference \t-> 90.9090909090909\n",
      "Object \t\t-> 90.9090909090909\n",
      "ReferenceHead \t-> 100.0\n",
      "Image \t\t-> 90.9090909090909\n",
      "[Eval] mAP [36/36]\tEpoch time elapsed: 0:11:15\n",
      "Values @50:\n",
      "88.10292271830734\n",
      "\n",
      "Header \t\t-> 90.9090909090909\n",
      "Input \t\t-> 99.88344988344988\n",
      "Button \t\t-> 0\n",
      "List \t\t-> 90.9090909090909\n",
      "Datagrid \t-> 100.0\n",
      "Dropdown \t-> 90.9090909090909\n",
      "Combobox \t-> 100.0\n",
      "Checkbox \t-> 100.0\n",
      "Radiobutton \t-> 100.0\n",
      "Reference \t-> 90.9090909090909\n",
      "Object \t\t-> 90.9090909090909\n",
      "ReferenceHead \t-> 100.0\n",
      "Image \t\t-> 90.9090909090909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([90.9090909090909,\n",
       "   90.9090909090909,\n",
       "   0,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   90.9090909090909,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   90.9090909090909,\n",
       "   90.9090909090909,\n",
       "   49.90648283789335,\n",
       "   90.9090909090909],\n",
       "  82.8599392392785),\n",
       " ([90.9090909090909,\n",
       "   90.9090909090909,\n",
       "   0,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   90.9090909090909,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   90.9090909090909],\n",
       "  87.41258741258743),\n",
       " ([90.9090909090909,\n",
       "   99.88344988344988,\n",
       "   0,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   90.9090909090909,\n",
       "   90.9090909090909,\n",
       "   100.0,\n",
       "   90.9090909090909],\n",
       "  88.10292271830734)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,_ = load_frcnn('Apr-11_10-43', 'Image-Retrain_v9_SGD-StepLR-5', num_classes=14)\n",
    "dataset = SketchDataset('./datasets/dataset_image', 'val', combined=True, preprocessed=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "dataset_map = SketchDataset('./datasets/dataset_mAP', '', combined=True)\n",
    "data_loader_map = torch.utils.data.DataLoader(\n",
    "        dataset_map, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "print('[mAP]')\n",
    "calc_multi_map(model, data_loader_map, precentages=[90,75,50], print_res=True)\n",
    "print()\n",
    "print('[Image validation]')\n",
    "calc_multi_map(model, data_loader, precentages=[90,75,50], print_res=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35eb3329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mAP]\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:08:25\n",
      "Values @90:\n",
      "20.809464916496673\n",
      "\n",
      "Header \t\t-> 9.090909090909092\n",
      "Input \t\t-> 34.082100070472165\n",
      "Button \t\t-> 8.318478906714201\n",
      "List \t\t-> 38.277511961722496\n",
      "Datagrid \t-> 40.52033492822966\n",
      "Dropdown \t-> 13.537549407114625\n",
      "Combobox \t-> 24.431818181818183\n",
      "Checkbox \t-> 0.0\n",
      "Radiobutton \t-> 0.0\n",
      "Reference \t-> 16.96230598669623\n",
      "Object \t\t-> 44.13702239789197\n",
      "ReferenceHead \t-> 20.355548066391442\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:08:22\n",
      "Values @75:\n",
      "54.50015927972864\n",
      "\n",
      "Header \t\t-> 11.363636363636363\n",
      "Input \t\t-> 54.54545454545454\n",
      "Button \t\t-> 81.12914862914863\n",
      "List \t\t-> 81.81818181818183\n",
      "Datagrid \t-> 54.54545454545454\n",
      "Dropdown \t-> 36.36363636363637\n",
      "Combobox \t-> 72.72727272727273\n",
      "Checkbox \t-> 52.7578006234084\n",
      "Radiobutton \t-> 20.321012712317057\n",
      "Reference \t-> 76.94864125733264\n",
      "Object \t\t-> 45.29220779220779\n",
      "ReferenceHead \t-> 66.1894639786927\n",
      "[Eval] mAP [27/27]\tEpoch time elapsed: 0:08:20\n",
      "Values @50:\n",
      "70.93305437288545\n",
      "\n",
      "Header \t\t-> 57.7635462623501\n",
      "Input \t\t-> 54.54545454545454\n",
      "Button \t\t-> 81.12914862914863\n",
      "List \t\t-> 81.81818181818183\n",
      "Datagrid \t-> 54.54545454545454\n",
      "Dropdown \t-> 36.36363636363637\n",
      "Combobox \t-> 72.72727272727273\n",
      "Checkbox \t-> 90.9090909090909\n",
      "Radiobutton \t-> 72.72727272727273\n",
      "Reference \t-> 99.83164983164983\n",
      "Object \t\t-> 54.248412983590846\n",
      "ReferenceHead \t-> 94.58753113152241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([9.090909090909092,\n",
       "   34.082100070472165,\n",
       "   8.318478906714201,\n",
       "   38.277511961722496,\n",
       "   40.52033492822966,\n",
       "   13.537549407114625,\n",
       "   24.431818181818183,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   16.96230598669623,\n",
       "   44.13702239789197,\n",
       "   20.355548066391442],\n",
       "  20.809464916496673),\n",
       " ([11.363636363636363,\n",
       "   54.54545454545454,\n",
       "   81.12914862914863,\n",
       "   81.81818181818183,\n",
       "   54.54545454545454,\n",
       "   36.36363636363637,\n",
       "   72.72727272727273,\n",
       "   52.7578006234084,\n",
       "   20.321012712317057,\n",
       "   76.94864125733264,\n",
       "   45.29220779220779,\n",
       "   66.1894639786927],\n",
       "  54.50015927972864),\n",
       " ([57.7635462623501,\n",
       "   54.54545454545454,\n",
       "   81.12914862914863,\n",
       "   81.81818181818183,\n",
       "   54.54545454545454,\n",
       "   36.36363636363637,\n",
       "   72.72727272727273,\n",
       "   90.9090909090909,\n",
       "   72.72727272727273,\n",
       "   99.83164983164983,\n",
       "   54.248412983590846,\n",
       "   94.58753113152241],\n",
       "  70.93305437288545)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,_ = load_frcnn('Mar-09_13-47', 'All-Comp_v9_SGD-StepLR_pre-10')\n",
    "dataset_map = SketchDataset('./datasets/dataset_mAP', '', combined=True)\n",
    "data_loader_map = torch.utils.data.DataLoader(\n",
    "        dataset_map, batch_size=3, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "print('[mAP]')\n",
    "calc_multi_map(model, data_loader_map, precentages=[90,75,50], print_res=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec74d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as transform\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import easyocr\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Calculates the Intersection Over Union for two specified bounding boxes\n",
    "def calc_iou(bb1, bb2):\n",
    "    # Get the coordinates of the intersecting box\n",
    "    inter_x = max(bb1[0], bb2[0])\n",
    "    inter_y = max(bb1[1], bb2[1])\n",
    "    inter_x2 = min(bb1[2], bb2[2])\n",
    "    inter_y2 = min(bb1[3], bb2[3])\n",
    "    \n",
    "    if inter_x2 < inter_x or inter_y2 < inter_y:\n",
    "        return 0.0\n",
    "    \n",
    "    inter_area = (inter_x2 - inter_x) * (inter_y2 - inter_y)\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "#    iou = inter_area / float(max([bb1_area - inter_area, inter_area]))\n",
    "    iou = inter_area / float(bb1_area)\n",
    "    return iou\n",
    "\n",
    "def calc_intersection(bb1, bb2):\n",
    "    inter_x = max(bb1[0], bb2[0])\n",
    "    inter_y = max(bb1[1], bb2[1])\n",
    "    inter_x2 = min(bb1[2], bb2[2])\n",
    "    inter_y2 = min(bb1[3], bb2[3])\n",
    "    \n",
    "    if inter_x2 < inter_x or inter_y2 < inter_y:\n",
    "        return 0.0\n",
    "    \n",
    "    return (inter_x2 - inter_x) * (inter_y2 - inter_y)\n",
    "\n",
    "# Calculates the area of a bounding box\n",
    "def calc_area(bb):\n",
    "    return (bb[2] - bb[0]) * (bb[3] - bb[1])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Padds a bounding box by a specific number, doubles the padding if text is specified\n",
    "def pad_bb(bb, pad, text=False):\n",
    "    x,y,x2,y2 = bb\n",
    "    if text:\n",
    "        return [x-pad*2, y-pad, x2+pad*2, y2+pad]\n",
    "    return [x-pad, y-pad, x2+pad, y2+pad]\n",
    "\n",
    "# Returns the smallest bounding box between two specified boxes\n",
    "def return_smallest(bb1, bb2):\n",
    "    bb1_x,bb1_y,bb1_x2,bb1_y2 = bb1\n",
    "    bb2_x,bb2_y,bb2_x2,bb2_y2 = bb2\n",
    "    bb1_size = (bb1_x2-bb1_x)*(bb1_y2-bb1_y)\n",
    "    bb2_size = (bb2_x2-bb2_x)*(bb2_y2-bb2_y)\n",
    "    \n",
    "    return bb2 if bb1_size > bb2_size else bb1d\n",
    "\n",
    "# Gets the bounding boxes from an image by processing the image\n",
    "def get_bbs_from_image(im, clean=True, pad=0, text=False, ignore_padding=10, combine_all=False):\n",
    "    imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bbs = []\n",
    "    for cntr in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cntr)\n",
    "        #cv2.rectangle(im, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        if x > ignore_padding and y > ignore_padding and x < im.shape[:2][1]-ignore_padding and y < im.shape[:2][0]-ignore_padding:\n",
    "            bbs.append([x,y,x+w,y+h])\n",
    "                \n",
    "    t_bbs = []\n",
    "    [t_bbs.append(x) for x in bbs if x not in t_bbs]\n",
    "    \n",
    "    t_bbs = remove_small_bb_list(t_bbs, 10000)\n",
    "    combined_bbs = combine_bb_list(t_bbs, pad=pad, text=text)\n",
    "    \n",
    "    if clean:\n",
    "        combined_bbs = clean_bb_list(combined_bbs, pad=pad)\n",
    "    \n",
    "    if combine_all:\n",
    "        temp_bb = combined_bbs[0]\n",
    "        for bb in combined_bbs:\n",
    "            if temp_bb[0] > bb[0]:\n",
    "                temp_bb[0] = bb[0]\n",
    "            if temp_bb[1] > bb[1]:\n",
    "                temp_bb[1] = bb[1]\n",
    "            if temp_bb[2] < bb[2]:\n",
    "                temp_bb[2] = bb[2]\n",
    "            if temp_bb[3] < bb[3]:\n",
    "                temp_bb[3] = bb[3]\n",
    "        return [temp_bb]\n",
    "    return combined_bbs\n",
    "\n",
    "def combine_bb_list(bb_list, pad=0, text=False):\n",
    "    bbs = bb_list.copy()\n",
    "    iou_non_zero = True\n",
    "    while iou_non_zero:\n",
    "        iou_non_zero = False\n",
    "        for i in range(len(bbs)-1):\n",
    "            for c in range(i, len(bbs)):\n",
    "                if bbs[i] == bbs[c]:\n",
    "                    continue\n",
    "                    \n",
    "                iou = calc_iou(pad_bb(bbs[i], pad, text=text), bbs[c])\n",
    "                \n",
    "                if iou != 0:\n",
    "                    iou_non_zero = True\n",
    "                    bb = combine_bb(bbs[i], bbs[c])\n",
    "                    bb1 = bbs[i].copy()\n",
    "                    bb2 = bbs[c].copy()\n",
    "                    \n",
    "                    bbs.remove(bb1)\n",
    "                    bbs.remove(bb2)\n",
    "                    bbs.append(bb)\n",
    "                    break;\n",
    "            if iou_non_zero:\n",
    "                break;\n",
    "    return bbs\n",
    "\n",
    "def combine_bb(bb1, bb2):\n",
    "    bb1_x,bb1_y,bb1_x2,bb1_y2 = bb1\n",
    "    bb2_x,bb2_y,bb2_x2,bb2_y2 = bb2\n",
    "\n",
    "    if bb2_x < bb1_x:\n",
    "        bb1_x = bb2_x\n",
    "    if bb2_y < bb1_y:\n",
    "        bb1_y = bb2_y\n",
    "    if bb2_x2 > bb1_x2:\n",
    "        bb1_x2 = bb2_x2\n",
    "    if bb2_y2 > bb1_y2:\n",
    "        bb1_y2 = bb2_y2\n",
    "        \n",
    "    return [bb1_x, bb1_y, bb1_x2, bb1_y2]\n",
    "\n",
    "def clean_bb_list(bb_list, pad=0, text=False):\n",
    "    bbs = bb_list.copy()\n",
    "    iou_non_zero = True\n",
    "    while iou_non_zero:\n",
    "        iou_non_zero = False\n",
    "        for i in range(len(bbs)):\n",
    "            if i == len(bbs)-1:\n",
    "                break;\n",
    "                \n",
    "            iou = calc_iou(pad_bb(bbs[i], pad, text=text), bbs[i+1])\n",
    "\n",
    "            if iou == 0:\n",
    "                continue\n",
    "\n",
    "            iou_non_zero = True\n",
    "            bb = return_smallest(bbs[i], bbs[i+1])\n",
    "            bbs.remove(bb)\n",
    "            break;\n",
    "                \n",
    "    return bbs\n",
    "\n",
    "def remove_small_bb_list(bb_list, size):\n",
    "    cleaned_list = []\n",
    "    for bb in bb_list:\n",
    "        x,y,x2,y2 = bb\n",
    "        w = x2-x\n",
    "        h = y2-y\n",
    "        if w*h > size:\n",
    "            cleaned_list.append(bb)\n",
    "            \n",
    "    return cleaned_list\n",
    "\n",
    "# Normalizes a pixel specific bounding box [x, y, x2, y2] to normalized bounding box [x, y, w, h]\n",
    "def normalize_bb(bb, shape):\n",
    "    h_img,w_img = shape\n",
    "    x,y,x2,y2 = bb\n",
    "    norm_w,norm_h = [(x2-x)/w_img, (y2-y)/h_img]\n",
    "    return [((x+x2)/2)/w_img, ((y+y2)/2)/h_img, norm_w, norm_h]\n",
    "\n",
    "# Denormalizes a normalized bounding box [x, y, w, h] to pixel specific bounding box [x, y, x2, y2]\n",
    "def denormalize_bb(bb, shape):\n",
    "    h_img,w_img = shape\n",
    "    x,y,w,h = bb\n",
    "    x_min,y_min = [int(x*w_img-(w*w_img)/2), int(y*h_img-(h*h_img)/2)]\n",
    "    return [x_min, y_min, x_min+int(w*w_img), y_min+int(h*h_img)]\n",
    "\n",
    "# Stringifies a bounding box for output\n",
    "def bb_to_str(bb):\n",
    "    return str(bb[0])+' '+str(bb[1])+' '+str(bb[2])+' '+str(bb[3])\n",
    "\n",
    "# Destringifies a bounding box\n",
    "def str_to_bb(bb_str):\n",
    "    str_arr = bb_str.split(' ')\n",
    "    return [float(str_arr[0]), float(str_arr[1]), float(str_arr[2]), float(str_arr[3]), float(str_arr[4])]\n",
    "\n",
    "# Generates dataset structure by generating boundingbox labels, spliting data into train and validition sets\n",
    "# also providing the found boundingboxes for verification of labeling being successfull \n",
    "def generate_dataset(root_folder, labels=[], split_components=True, train_val_ratio=0.8, combine_all=False):\n",
    "    os.mkdir('./'+root_folder+'_generated/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/train/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/val/')\n",
    "    os.mkdir('./'+root_folder+'_generated/images/bbs/')\n",
    "    os.mkdir('./'+root_folder+'_generated/labels/')\n",
    "\n",
    "    if (split_components):    \n",
    "        for component in os.listdir('./'+root_folder):\n",
    "            os.mkdir('./'+root_folder+'_generated/images/train/'+component+'/')\n",
    "            os.mkdir('./'+root_folder+'_generated/images/val/'+component+'/')\n",
    "            images = os.listdir('./'+root_folder+'/'+component)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i]\n",
    "                img_type = 'val' if i > math.floor(len(images)*train_val_ratio) else 'train'\n",
    "                im = cv2.imread('./'+root_folder+'/'+component+'/'+image)\n",
    "                cv2.imwrite('./'+root_folder+'_generated/images/'+img_type+'/'+component+'/'+image, im)\n",
    "                bbs = get_bbs_from_image(im, clean=True, pad=30, text=True, combine_all=combine_all)\n",
    "                bbs_str = '' \n",
    "                for bb in bbs:\n",
    "                    bbs_str += str(labels[component])+' '+bb_to_str(normalize_bb(bb, im.shape[:2]))+'\\n'\n",
    "                    x,y,x2,y2 = pad_bb(bb, 5)\n",
    "                    cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.imwrite('./'+root_folder+'_generated/images/bbs/'+image, im)\n",
    "                f = open('./'+root_folder+'_generated/labels/'+image[:-3]+\"txt\", \"a\")\n",
    "                f.write(bbs_str[:-1])\n",
    "                f.close()\n",
    "    else:\n",
    "        images = os.listdir('./'+root_folder)\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            img_type = 'val' if i > math.floor(len(images)*train_val_ratio) else 'train'\n",
    "            im = cv2.imread('./'+root_folder+'/'+image)\n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/'+img_type+'/'+image, im)\n",
    "            bbs = get_bbs_from_image(im, clean=True, pad=30, text=True, combine_all=combine_all)\n",
    "            \n",
    "            bbs_str = '' \n",
    "            c = 0\n",
    "            for bb in bbs:\n",
    "                c = c + 1\n",
    "                bbs_str += str(c)+' '+bb_to_str(normalize_bb(bb, im.shape[:2]))+'\\n'\n",
    "                x,y,x2,y2 = pad_bb(bb, 5)\n",
    "                cv2.rectangle(im, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(im, str(c), (int((x+x2)/2)-50,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imwrite('./'+root_folder+'_generated/images/bbs/'+image, im)\n",
    "            f = open('./'+root_folder+'_generated/labels/'+image[:-3]+\"txt\", \"a\")\n",
    "            f.write(bbs_str[:-1])\n",
    "            f.close()\n",
    "            \n",
    "def threshold_output(prediction, threshold=0.5):\n",
    "    output = {'boxes':[], 'scores':[], 'labels':[]}\n",
    "    for i in range(len(prediction['scores'])):\n",
    "        if prediction['scores'][i] > threshold:\n",
    "            output['boxes'].append(prediction['boxes'][i])\n",
    "            output['scores'].append(prediction['scores'][i])\n",
    "            output['labels'].append(prediction['labels'][i])\n",
    "    return output\n",
    "\n",
    "def interpol_precision(precision, fptp):\n",
    "    inter_prec = []\n",
    "    curr_prec = precision[0]\n",
    "    for i in range(len(precision)):\n",
    "        if fptp[i]:\n",
    "            curr_prec = precision[i]\n",
    "        inter_prec.append(curr_prec)\n",
    "    return inter_prec\n",
    "\n",
    "def calc_ap(precision, recall, fptp):\n",
    "    inter_prec = interpol_precision(precision, fptp)\n",
    "    AP = 0\n",
    "    inter_prec.append(0)\n",
    "    \n",
    "    p = 0\n",
    "    for i in range(0, 11):\n",
    "        for c in range(p, len(recall)):\n",
    "            if recall[c] < i*0.1:\n",
    "                if p != len(recall):\n",
    "                    p += 1\n",
    "            else:\n",
    "                break\n",
    "        AP += inter_prec[p]\n",
    "    \n",
    "    return AP/11\n",
    "\n",
    "def calc_map(model, data_loader, device, num_classes, IoU=0.5):\n",
    "    precision = [[] for k in range(0,num_classes)]\n",
    "    recall = [[] for k in range(0,num_classes)]\n",
    "    scores = [[] for k in range(0,num_classes)]\n",
    "    fptp = [[] for k in range(0,num_classes)] # 0 = false positive, 1 = true positive\n",
    "    fptp_p = [1 for k in range(0,num_classes)] # fptp pointer\n",
    "    tpfn = [0 for k in range(0,num_classes)] # true positive false negative counter\n",
    "    AP = [0 for k in range(0,num_classes)]\n",
    "    mAP = 0\n",
    "    predictions = []\n",
    "    batch_nr = 0\n",
    "    epoch_time = time.time()\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        batch_nr += 1\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        preds = model(images)\n",
    "        for i in range(len(preds)):\n",
    "            thres = preds[i]\n",
    "            for c in range(len(thres['labels'])):\n",
    "                b_label = False\n",
    "                for g in range(len(targets[i]['labels'])):\n",
    "                    if device.type == 'cpu':\n",
    "                        iou = calc_iou(targets[i]['boxes'][g].detach().numpy(), thres['boxes'][c].detach().numpy())\n",
    "                    else:\n",
    "                        iou = calc_iou(targets[i]['boxes'][g].cpu().detach().numpy(), thres['boxes'][c].cpu().detach().numpy())\n",
    "                    if iou > IoU and thres['labels'][c].item() == targets[i]['labels'][g].item():\n",
    "                        fptp[thres['labels'][c].item()].append(1)\n",
    "                        scores[thres['labels'][c].item()].append(thres['scores'][c].item())\n",
    "                        b_label = True\n",
    "                        break;\n",
    "                if not b_label:\n",
    "                        fptp[thres['labels'][c].item()].append(0)\n",
    "                        scores[thres['labels'][c].item()].append(thres['scores'][c].item())\n",
    "                        tpfn[thres['labels'][c].item()] += 1\n",
    "\n",
    "        for tar in targets:\n",
    "            for lab in tar['labels']:\n",
    "                tpfn[lab.item()] += 1\n",
    "        \n",
    "        print(\n",
    "            '\\r[Eval] mAP [{}/{}]\\tEpoch time elapsed: {}'.format(\n",
    "                batch_nr, len(data_loader), str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "            ),\n",
    "            end=''\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_fptp = copy.deepcopy(fptp)\n",
    "    # Sort the arrays, highly inefficient sort O(n^2)\n",
    "    for i in range(num_classes):\n",
    "        for x in range(len(scores[i])):\n",
    "            idx = -1\n",
    "            highest_score = 0\n",
    "            for c in range(len(scores[i])):\n",
    "                if highest_score <= scores[i][c]:\n",
    "                    highest_score = scores[i][c]\n",
    "                    idx = c\n",
    "\n",
    "            if idx != -1:\n",
    "                t_fptp[i][x] = fptp[i][idx]\n",
    "                scores[i][idx] = -1\n",
    "            \n",
    "    for i in range(len(t_fptp)):\n",
    "        for c in range(1,len(t_fptp[i])+1):\n",
    "            precision[i].append(sum(t_fptp[i][:c])/len(t_fptp[i][:c]))\n",
    "            recall[i].append(sum(t_fptp[i][:c])/tpfn[i])\n",
    "    \n",
    "    for i in range(len(recall)):\n",
    "        if precision[i] != []:\n",
    "            AP[i] = calc_ap(precision[i], recall[i], t_fptp[i])*100\n",
    "            mAP += AP[i]\n",
    "    mAP = mAP/(num_classes-1)\n",
    "    return AP[1:], mAP\n",
    "            \n",
    "def predict_and_save(model, root_dir, save_dir, labels=[], threshold=0.5, IoU=0, mask=False, unique_name='', skip_component=''):\n",
    "    # Check if the 4th final character is a dot aka if the input directory is a file\n",
    "    if root_dir[-4] == '.':\n",
    "        im = root_dir\n",
    "        img = cv2.imread(im)\n",
    "        cv2_img = cv2.imread(im)\n",
    "        if mask:\n",
    "            imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "        tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "        predictions = model(tensor_img)\n",
    "        dont_print_id = []\n",
    "        for i in range(len(predictions[0]['boxes'])):\n",
    "            score = predictions[0]['scores'][i].item()\n",
    "            if IoU > 0:\n",
    "                if i in dont_print_id:\n",
    "                    continue\n",
    "                bb1 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                for c in range(i, len(predictions[0]['boxes'])):\n",
    "                    bb2 = predictions[0]['boxes'][c].detach().numpy()\n",
    "                    if calc_intersection(bb1, bb2) > calc_area(bb1)*IoU:\n",
    "                        if labels[predictions[0]['labels'][i].item()-1] != skip_component and labels[predictions[0]['labels'][c].item()-1] == skip_component: \n",
    "                            dont_print_id.append(c)\n",
    "            if score > threshold:\n",
    "                x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "                \n",
    "                if len(labels) > 1:\n",
    "                    cv2.putText(cv2_img, labels[predictions[0]['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imwrite(save_dir+unique_name+(root_dir.split(\"/\")[-1]), cv2_img)\n",
    "    else:\n",
    "        for image in os.listdir(root_dir):\n",
    "            im = root_dir+image\n",
    "            img = Image.open(im)\n",
    "            cv2_img = cv2.imread(im)\n",
    "            tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "            tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "            predictions = model(tensor_img)\n",
    "            for i in range(len(predictions[0]['boxes'])):\n",
    "                score = predictions[0]['scores'][i].item()\n",
    "                if score > threshold:\n",
    "                    x,y,x2,y2 = predictions[0]['boxes'][i].detach().numpy()\n",
    "                    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                    if len(labels) > 1:\n",
    "                        cv2.putText(cv2_img, labels[predictions[0]['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imwrite(save_dir+unique_name+image, cv2_img)\n",
    "\n",
    "def save_best_bb(pred1, pred2, IoU=0.5):\n",
    "    remove_pred1_ids = []\n",
    "    remove_pred2_ids = []\n",
    "    \n",
    "    # Go through pred1 checking which elements to remove and also pred2\n",
    "    for i in range(len(pred1['boxes'])):\n",
    "        score1 = pred1['scores'][i].item()\n",
    "        if pred1['boxes'][i].device.type == 'cpu':\n",
    "            bb1 = pred1['boxes'][i].detach().numpy()\n",
    "        else:\n",
    "            bb1 = pred1['boxes'][i].cpu().detach().numpy()\n",
    "        for c in range(len(pred2['boxes'])):\n",
    "            score2 = pred2['scores'][c].item()\n",
    "            \n",
    "            if pred2['boxes'][c].device.type == 'cpu':\n",
    "                bb2 = pred2['boxes'][c].detach().numpy()\n",
    "            else:\n",
    "                bb2 = pred2['boxes'][c].cpu().detach().numpy()\n",
    "            intersect = calc_intersection(bb1, bb2)\n",
    "            if intersect > calc_area(bb1)*IoU or intersect > calc_area(bb2)*IoU:\n",
    "                if score1 > score2:\n",
    "                    remove_pred2_ids.append(c)\n",
    "                else:\n",
    "                    remove_pred1_ids.append(i)\n",
    "            \n",
    "    # Remove the elements that should not be in the refined final list\n",
    "    final_pred = {'boxes':[], 'scores':[], 'labels':[]}\n",
    "    for i in range(len(pred1['boxes'])):\n",
    "        if i not in remove_pred1_ids:\n",
    "            final_pred['boxes'].append(pred1['boxes'][i])\n",
    "            final_pred['scores'].append(pred1['scores'][i])\n",
    "            final_pred['labels'].append(pred1['labels'][i])\n",
    "    for i in range(len(pred2['boxes'])):\n",
    "        if i not in remove_pred2_ids:\n",
    "            final_pred['boxes'].append(pred2['boxes'][i])\n",
    "            final_pred['scores'].append(pred2['scores'][i])\n",
    "            final_pred['labels'].append(pred2['labels'][i])\n",
    "    \n",
    "    return final_pred\n",
    "    \n",
    "def frcnn_load_singular_models(model_name, components, root_dir):\n",
    "    models = []\n",
    "    device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    for component in components:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "        num_classes = 2\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        model.to(device)\n",
    "        checkpoint = torch.load(root_dir+'/'+component+'/'+model_name+'.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        \n",
    "    return models\n",
    "\n",
    "\n",
    "def ssd_load_singular_models(model_name, components, root_dir, device):\n",
    "    models = []\n",
    "\n",
    "    for component in components:\n",
    "        model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "        num_classes = 2\n",
    "        in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "        model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "        \n",
    "        model_ssd.to(device)\n",
    "        \n",
    "        checkpoint = torch.load(root_dir+'/'+component+'/'+model_name+'.pt')\n",
    "        model_ssd.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_ssd.eval()\n",
    "        models.append(model_ssd)\n",
    "        \n",
    "    return models\n",
    "\n",
    "def predict_text(image_filename):\n",
    "    reader = easyocr.Reader(['en'],gpu = False) # load once only in memory.\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    rotation_degrees = [0,1,-1]\n",
    "    boxes = []\n",
    "    words = []\n",
    "    scores = []\n",
    "\n",
    "    for rotation in rotation_degrees:\n",
    "        image = cv2.imread(image_filename)\n",
    "        image_center = tuple(numpy.array(image.shape[1::-1]) / 2)\n",
    "        rot_mat = cv2.getRotationMatrix2D(image_center, rotation, 1.0)\n",
    "        rotated = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        # sharp the edges or image.\n",
    "        gray = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_kernel = numpy.array([[-1,-1,-1], [-1,15,-1], [-1,-1,-1]])\n",
    "        sharpen = cv2.filter2D(gray, -1, sharpen_kernel)\n",
    "        thresh = cv2.threshold(sharpen, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "        \n",
    "        r_easy_ocr=reader.readtext(thresh)\n",
    "        \n",
    "        for pred in r_easy_ocr:\n",
    "            box,word,score = pred\n",
    "            x,y = box[0]\n",
    "            x2,y2 = box[2]\n",
    "            box = [int(x),int(y),int(x2),int(y2)]\n",
    "            if len(boxes) < 1:\n",
    "                boxes.append([box])\n",
    "                words.append([word])\n",
    "                scores.append([score])\n",
    "            else:\n",
    "                b_found = False\n",
    "                for i in range(len(boxes)):\n",
    "                    if calc_iou(box, boxes[i][0]) > 0.5 or calc_iou(boxes[i][0], box) > 0.5:\n",
    "                        b_found = True\n",
    "                        boxes[i].append(box)\n",
    "                        words[i].append(word)\n",
    "                        scores[i].append(score)\n",
    "                        \n",
    "                if not b_found:\n",
    "                    boxes.append([box])\n",
    "                    words.append([word])\n",
    "                    scores.append([score])\n",
    "    \n",
    "    t_words = copy.deepcopy(words)\n",
    "    t_boxes = copy.deepcopy(boxes)\n",
    "    t_scores = copy.deepcopy(boxes)\n",
    "    # Sort the arrays, highly inefficient sort O(n^2)\n",
    "    for i in range(len(words)):\n",
    "        for x in range(len(scores[i])):\n",
    "            idx = -1\n",
    "            highest_score = 0\n",
    "            for c in range(len(scores[i])):\n",
    "                if highest_score <= scores[i][c]:\n",
    "                    highest_score = scores[i][c]\n",
    "                    idx = c\n",
    "\n",
    "            if idx != -1:\n",
    "                t_words[i][x] = words[i][idx]\n",
    "                t_boxes[i][x] = boxes[i][idx]\n",
    "                t_scores[i][x] = scores[i][idx]\n",
    "                scores[i][idx] = -1\n",
    "                \n",
    "    corr_words = []\n",
    "    for i in range(len(t_words)):\n",
    "        corr_words.append(spell.correction(t_words[i][0]))\n",
    "        \n",
    "    boxes = []\n",
    "    for box in t_boxes:\n",
    "        boxes.append(box[0])\n",
    "        \n",
    "    scores = []\n",
    "    for score in t_scores:\n",
    "        scores.append(score[0])\n",
    "\n",
    "    return {'boxes':boxes, 'words':corr_words, 'scores':scores}\n",
    "\n",
    "\n",
    "def predict_model(model, image, IoU=0.5, disregard_comp=[], priority_comp=[]):\n",
    "    model.eval()\n",
    "    img = cv2.imread(image)\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "    tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "    tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "    \n",
    "    pred = model(tensor_img)\n",
    "    pred = {'boxes':pred[0]['boxes'].detach().numpy(), 'scores':pred[0]['scores'].detach().numpy(), 'labels':pred[0]['labels'].detach().numpy()}\n",
    "    \n",
    "    remove_arr = []\n",
    "    for i in range(len(pred['boxes'])):\n",
    "        for c in range(len(pred['boxes'])):\n",
    "            if i == c:\n",
    "                continue\n",
    "            if calc_iou(pred['boxes'][i], pred['boxes'][c]) > IoU:\n",
    "                if i in remove_arr or c in remove_arr:\n",
    "                    continue\n",
    "                \n",
    "                in_disregard = pred['labels'][i] in disregard_comp or pred['labels'][c] in disregard_comp\n",
    "                in_priority  = pred['labels'][i] in priority_comp  or pred['labels'][c] in priority_comp\n",
    "                if not in_priority and in_disregard and pred['labels'][i] != pred['labels'][c]:\n",
    "                    continue\n",
    "                \n",
    "                if not in_priority:\n",
    "                    if pred['scores'][i] > pred['scores'][c]:\n",
    "                        remove_arr.append(c)\n",
    "                    else:\n",
    "                        remove_arr.append(i)\n",
    "                else:\n",
    "                    if pred['labels'][i] in priority_comp:\n",
    "                        remove_arr.append(c)\n",
    "                    else:\n",
    "                        remove_arr.append(i)\n",
    "\n",
    "    remove_arr = list(dict.fromkeys(remove_arr))\n",
    "    remove_arr.sort(reverse=True) # To get the highest id first\n",
    "    \n",
    "    new_boxes = pred['boxes']\n",
    "    new_scores = pred['scores']\n",
    "    new_labels = pred['labels']\n",
    "    \n",
    "    for idx in remove_arr:\n",
    "        new_boxes  = numpy.delete(new_boxes, idx, 0)\n",
    "        new_scores = numpy.delete(new_scores, idx, 0)\n",
    "        new_labels = numpy.delete(new_labels, idx, 0)\n",
    "        \n",
    "    if len(remove_arr) > 0:\n",
    "        return {'boxes':new_boxes, 'scores':new_scores, 'labels':new_labels}\n",
    "    return pred\n",
    "\n",
    "def predict_models(models, images, IoU=0.5):\n",
    "    predictions = [0 for k in range(len(images))]\n",
    "    for i in range(len(images)):\n",
    "        preds = []\n",
    "        for model in models:\n",
    "            pred = model([images[i]])\n",
    "            preds.append(pred[0])\n",
    "\n",
    "        for c in range(len(preds)):\n",
    "            preds[c]['labels'] *= (c+1)\n",
    "\n",
    "        final_pred = preds[0]\n",
    "        for c in range(1, len(preds)):\n",
    "            final_pred = save_best_bb(final_pred, preds[c], IoU=IoU)\n",
    "        predictions[i] = final_pred\n",
    "    return predictions\n",
    "    \n",
    "def predict_and_save_models(models, root_dir, save_dir, labels=[], threshold=0.5, IoU=0.5, mask=False):\n",
    "    if root_dir[-4] == '.':\n",
    "        im = root_dir\n",
    "        img = cv2.imread(im)\n",
    "        cv2_img = cv2.imread(im)\n",
    "        if mask:\n",
    "            imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        tensor_img = torch.tensor(transform.to_tensor(img))\n",
    "        tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "        predictions = []\n",
    "        for model in models:\n",
    "            predictions.append(model(tensor_img)[0])\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            predictions[i]['labels'] *= (i+1)\n",
    "            \n",
    "        final_pred = predictions[0]\n",
    "        for i in range(1, len(predictions)):\n",
    "            final_pred = save_best_bb(final_pred, predictions[i], IoU=IoU)\n",
    "            \n",
    "        for i in range(len(final_pred['boxes'])):\n",
    "            score = final_pred['scores'][i].item()\n",
    "            if score > threshold:\n",
    "                x,y,x2,y2 = final_pred['boxes'][i].detach().numpy()\n",
    "                cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                if len(labels) > 1:\n",
    "                    cv2.putText(cv2_img, labels[final_pred['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imwrite(save_dir+(root_dir.split(\"/\")[-1]), cv2_img)\n",
    "    else:\n",
    "        for image in os.listdir(root_dir):\n",
    "            im = root_dir+image\n",
    "            img = Image.open(im)\n",
    "            cv2_img = cv2.imread(im)\n",
    "            tensor_img = transform.to_tensor(img)\n",
    "            tensor_img = torch.reshape(tensor_img, (1, tensor_img.size(0), tensor_img.size(1), tensor_img.size(2)))\n",
    "\n",
    "            predictions = []\n",
    "            for model in models:\n",
    "                predictions.append(model(tensor_img)[0])\n",
    "\n",
    "            for i in range(len(predictions)):\n",
    "                predictions[i]['labels'] *= (i+1)\n",
    "\n",
    "            final_pred = predictions[0]\n",
    "            for i in range(1, len(predictions)):\n",
    "                final_pred = save_best_bb(final_pred, predictions[i], IoU=IoU)\n",
    "\n",
    "            for i in range(len(final_pred['boxes'])):\n",
    "                score = final_pred['scores'][i].item()\n",
    "                if score > threshold:\n",
    "                    x,y,x2,y2 = final_pred['boxes'][i].detach().numpy()\n",
    "                    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "                    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "                    if len(labels) > 1:\n",
    "                        cv2.putText(cv2_img, labels[final_pred['labels'][i].item()-1], (int((x+x2)/2)-250,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imwrite(save_dir+image, cv2_img)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=False):\n",
    "    batch_size = len(x)\n",
    "    if (batch_size < 2):\n",
    "        return x, [0], y\n",
    "    \n",
    "    # Get a random lambda\n",
    "    if alpha > 0:\n",
    "        lam = numpy.clip(numpy.random.beta(alpha, alpha), 0.4, 0.6)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    # convert tensor array to numpy for mixup\n",
    "    t_x = numpy.empty((batch_size, x[0].size()[1], x[0].size()[2]))\n",
    "    for i in range(len(x)):\n",
    "        t_x[i] = x[i].numpy().astype(numpy.float)\n",
    "    \n",
    "    # convert tuple to numpy array for easier indexing\n",
    "    t_y = numpy.empty((batch_size), dtype=numpy.object)\n",
    "    for i in range(len(y)):\n",
    "        t_y[i] = {}\n",
    "        for var in y[i]:\n",
    "            t_y[i][var] = y[i][var].numpy().astype(numpy.double)\n",
    "        \n",
    "    # Get a random set of indicies\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "    # Mix the two images and make them % transparent based on lambda\n",
    "    t_mixed_x = lam * t_x + (1 - lam) * t_x[index, :]\n",
    "    y_a, y_b = t_y, t_y[index]\n",
    "\n",
    "    # Zip together the bounding boxes of the zipped images\n",
    "    mixedup_bboxes = []\n",
    "    i = 0\n",
    "    for bbox, s_bbox in zip(y_a, y_b):\n",
    "        # If the two images zipped are the same, keep one of the boundingbox infos\n",
    "        if (bbox['boxes'][0] == s_bbox['boxes'][0]).all():\n",
    "            mix_len = len(bbox['boxes'])\n",
    "            mixedup_bboxes.append({'boxes':torch.zeros([mix_len, 4], dtype=torch.double), 'labels':torch.zeros([mix_len], dtype=torch.int64), 'image_id':torch.zeros([1], dtype=torch.int64), 'area':torch.zeros([mix_len], dtype=torch.float), 'iscrowd':torch.zeros([mix_len], dtype=torch.int64)})\n",
    "            for c in range(len(bbox['boxes'])):\n",
    "                mixedup_bboxes[i]['boxes'][c] = torch.from_numpy(bbox['boxes'][c])\n",
    "                mixedup_bboxes[i]['labels'][c] = bbox['labels'][c]\n",
    "                mixedup_bboxes[i]['area'][c] = bbox['area'][c]\n",
    "                mixedup_bboxes[i]['iscrowd'][c] = bbox['iscrowd'][c]\n",
    "            i += 1\n",
    "            continue;\n",
    "            \n",
    "        mix_len = len(bbox['boxes'])+len(s_bbox['boxes'])\n",
    "        mixedup_bboxes.append({'boxes':torch.zeros([mix_len, 4], dtype=torch.double), 'labels':torch.zeros([mix_len], dtype=torch.int64), 'image_id':torch.zeros([1], dtype=torch.int64), 'area':torch.zeros([mix_len], dtype=torch.float), 'iscrowd':torch.zeros([mix_len], dtype=torch.int64)})\n",
    "        for c in range(len(bbox['boxes'])):\n",
    "            mixedup_bboxes[i]['boxes'][c] = torch.from_numpy(bbox['boxes'][c])\n",
    "            mixedup_bboxes[i]['labels'][c] = bbox['labels'][c]\n",
    "            mixedup_bboxes[i]['area'][c] = bbox['area'][c]\n",
    "            mixedup_bboxes[i]['iscrowd'][c] = bbox['iscrowd'][c]\n",
    "        \n",
    "        for j in range(len(s_bbox['boxes'])):\n",
    "            mixedup_bboxes[i]['boxes'][c+j+1] = torch.from_numpy(s_bbox['boxes'][j])\n",
    "            mixedup_bboxes[i]['labels'][c+j+1] = s_bbox['labels'][j]\n",
    "            mixedup_bboxes[i]['area'][c+j+1] = s_bbox['area'][j]\n",
    "            mixedup_bboxes[i]['iscrowd'][c+j+1] = s_bbox['iscrowd'][j]\n",
    "            \n",
    "        mixedup_bboxes[i]['image_id'][0] = bbox['image_id'][0]\n",
    "        i += 1\n",
    "        \n",
    "    mixed_x = []\n",
    "    for v in t_mixed_x:\n",
    "        mixed_x.append(torch.FloatTensor([v]))\n",
    "    \n",
    "    return mixed_x, index, tuple(mixedup_bboxes)\n",
    "\n",
    "# Generates new combined data based on how many components it can fit into the screen without overlapping too much\n",
    "def generate_combined_data(root_folder, max_nr_components=3, IoU=0.05, start_index_filename=0, component_folder=''):\n",
    "    max_nr_components -= 1\n",
    "    os.mkdir('./'+root_folder+'_combined_'+str(max_nr_components)+'/')\n",
    "    os.mkdir('./'+root_folder+'_combined_'+str(max_nr_components)+'/images/')\n",
    "    os.mkdir('./'+root_folder+'_combined_'+str(max_nr_components)+'/labels/')\n",
    "    components = os.listdir('./'+root_folder+'/images')\n",
    "    \n",
    "    if component_folder == '':\n",
    "        component_folder = root_folder\n",
    "        \n",
    "    images = []\n",
    "    components = os.listdir('./'+component_folder+'/images')\n",
    "    for component in components:\n",
    "        images.append(os.listdir('./'+component_folder+'/images/'+component))\n",
    "\n",
    "    root_images = []\n",
    "    root_components = os.listdir('./'+root_folder+'/images')    \n",
    "    for component in root_components:\n",
    "        root_images.append(os.listdir('./'+root_folder+'/images/'+component))\n",
    "\n",
    "    for i in range(len(root_components)):\n",
    "        print('Making dataset for:',root_components[i])\n",
    "        for img in root_images[i]:\n",
    "            image = []\n",
    "            labels = []\n",
    "            im = cv2.imread('./'+root_folder+'/images/'+root_components[i]+'/'+img)\n",
    "            imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            ret, image = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "            f = open('./'+root_folder+'/labels/'+img[:-3]+'txt', 'r')\n",
    "            data = f.read().split('\\n')\n",
    "            f.close()\n",
    "            for line in data:\n",
    "                labels.append(str_to_bb(line))\n",
    "\n",
    "            nr_components = 0\n",
    "            components_checked = [i]\n",
    "            for c in range(len(images)):\n",
    "                if max_nr_components == nr_components:\n",
    "                    break;\n",
    "\n",
    "                choices = []\n",
    "                for y in range(len(components)):\n",
    "                    if not y in components_checked:\n",
    "                        choices.append(y)\n",
    "                if len(choices) == 0:\n",
    "                    break;\n",
    "                c = numpy.random.choice(choices)\n",
    "                components_checked.append(c)\n",
    "                \n",
    "                idxs = [y for y in range(len(images[c]))]\n",
    "                random.shuffle(idxs)\n",
    "                for y in idxs:\n",
    "                    f = open('./'+component_folder+'/labels/'+images[c][y][:-3]+'txt', 'r')\n",
    "                    data = f.read().split('\\n')\n",
    "                    f.close()\n",
    "\n",
    "                    intersects = False\n",
    "                    new_labels = labels.copy()\n",
    "                    for line in data:\n",
    "                        line_bb = str_to_bb(line)\n",
    "                        denorm_line_bb = denormalize_bb(line_bb[1:], image.shape[:2])\n",
    "                        for bb in labels:\n",
    "                            denorm_bb = denormalize_bb(bb[1:], image.shape[:2])\n",
    "                            if calc_iou(denorm_bb, denorm_line_bb) > IoU or calc_iou(denorm_line_bb, denorm_bb) > IoU:\n",
    "                                intersects = True\n",
    "                                break;\n",
    "                        if intersects:\n",
    "                            break;\n",
    "                        new_labels.append(line_bb)\n",
    "                    if intersects:\n",
    "                        continue;\n",
    "\n",
    "                    labels = new_labels.copy()\n",
    "                    im2 = cv2.imread('./'+component_folder+'/images/'+components[c]+'/'+images[c][y])\n",
    "                    imgray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "                    ret, image2 = cv2.threshold(imgray2, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "                    image = image+image2\n",
    "                    nr_components += 1\n",
    "                    break;\n",
    "\n",
    "            str_labels = ''\n",
    "            for bb in labels:\n",
    "                str_labels += str(int(bb[0]))+' '+bb_to_str(bb[1:])+'\\n'\n",
    "            cv2.imwrite('./'+root_folder+'_combined_'+str(max_nr_components)+'/images/'+str(start_index_filename)+'.jpg', image)\n",
    "            f = open('./'+root_folder+'_combined_'+str(max_nr_components)+'/labels/'+str(start_index_filename)+'.txt', 'w')\n",
    "            f.write(str_labels[:-1])\n",
    "            f.close()\n",
    "            start_index_filename += 1\n",
    "            \n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, root_dir, set_type, single_component=False, combined=False, preprocessed=False):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.component_names = []\n",
    "        if combined:\n",
    "            for image in os.listdir(root_dir+\"/images/\"+set_type):\n",
    "                self.images.append(root_dir+\"/images/\"+set_type+'/'+image)\n",
    "                self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "        elif not single_component:\n",
    "            for component in os.listdir(root_dir+\"/images/\"+set_type):\n",
    "                if component == 'Combined':\n",
    "                    continue\n",
    "                self.component_names.append(component)\n",
    "                for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+component):\n",
    "                    self.images.append(root_dir+\"/images/\"+set_type+\"/\"+component+\"/\"+image)\n",
    "                    self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "        else:\n",
    "            self.component_names.append(single_component)\n",
    "            for image in os.listdir(root_dir+\"/images/\"+set_type+\"/\"+single_component):\n",
    "                self.images.append(root_dir+\"/images/\"+set_type+\"/\"+single_component+\"/\"+image)\n",
    "                self.labels.append(root_dir+\"/labels/\"+(image.split('.')[0]+'.txt'))\n",
    "            \n",
    "        self.root = root_dir\n",
    "        self.single_component = single_component\n",
    "        self.combined = combined\n",
    "        self.preprocessed = preprocessed\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im = cv2.imread(self.images[idx])\n",
    "        if not self.preprocessed:\n",
    "            imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            ret, img = cv2.threshold(imgray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        else:\n",
    "            img = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        f = open(self.labels[idx], \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "\n",
    "        N = len(data)\n",
    "        boxes = torch.zeros([N, 4], dtype=torch.double)\n",
    "        labels = torch.zeros([N], dtype=torch.int64)\n",
    "        areas = torch.zeros([N])\n",
    "        \n",
    "        for i in range(N):\n",
    "            bb = denormalize_bb(str_to_bb(data[i])[1:], img.shape[:2])\n",
    "            boxes[i][0],boxes[i][1],boxes[i][2],boxes[i][3] = bb\n",
    "            areas[i] = calc_area(bb)\n",
    "            \n",
    "            if not self.single_component:\n",
    "                labels[i] = int(data[i].split(' ')[0])+1\n",
    "                continue\n",
    "                \n",
    "            labels[i] = 1\n",
    "                \n",
    "        return transform.to_tensor(img), {'boxes':boxes, 'labels':labels, 'image_id':torch.LongTensor([idx]), 'area':areas, 'iscrowd':torch.zeros([N], dtype=torch.int64)}\n",
    "\n",
    "def train_model(model, optimizer, data_loader, data_loader_val, device, num_epochs, model_type, model_name, lr_scheduler=False, folder_name='', mixup=False, begin_epoch=0):\n",
    "    writer = SummaryWriter()\n",
    "    total_time = time.time()\n",
    "    \n",
    "    if folder_name == '':\n",
    "        folder_name = datetime.datetime.now().strftime(\"%b-%d_%H-%M\")\n",
    "    \n",
    "    if not os.path.exists('./models/'+model_type+'/'+folder_name+'/'):\n",
    "        os.mkdir('./models/'+model_type+'/'+folder_name+'/')\n",
    "\n",
    "    for epoch in range(begin_epoch, num_epochs):\n",
    "        epoch_time = time.time()\n",
    "        epoch_loss = []\n",
    "        batch_nr = 0\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            batch_time = time.time()\n",
    "            if mixup:\n",
    "                images, _, targets = mixup_data(images, targets)\n",
    "\n",
    "            # Send them to device if using GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            pred = model(images, targets)\n",
    "            losses = sum(loss for loss in pred.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss.append(losses.item())\n",
    "            \n",
    "            writer.add_scalars(model_type+'_'+model_name, {\n",
    "                'train_loss': losses.item(),\n",
    "            }, epoch*len(data_loader)+batch_nr)\n",
    "            \n",
    "            batch_nr = batch_nr + 1\n",
    "            print_loss = losses.item()\n",
    "            \n",
    "            if batch_nr == epoch+1:\n",
    "                print_loss = numpy.average(epoch_loss)\n",
    "                \n",
    "            print(\n",
    "                '\\r[Train] Epoch {} [{}/{}] - Loss: {} \\tProgress [{}%] \\tEpoch time elapsed: {}'.format(\n",
    "                    epoch+1, batch_nr, len(data_loader), print_loss, round(((epoch/num_epochs)+(1/num_epochs*batch_nr/len(data_loader)))*100, 2), str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "        \n",
    "            \n",
    "        writer.add_scalars(model_type+'_'+model_name, {\n",
    "            'avg_epoch_loss': numpy.average(epoch_loss),\n",
    "        }, (epoch+1))\n",
    "            \n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        print()\n",
    "        evaluate_model(model, data_loader_val, device, writer, model_type, model_name, epoch+1)\n",
    "        #model.train()\n",
    "        print()\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, './models/'+model_type+'/'+folder_name+'/'+model_name+'-'+str(epoch+1)+'.pt')\n",
    "\n",
    "    print(\n",
    "        '\\rTraining completed! Loss: {} \\tTotal time elapsed: {}'.format(\n",
    "            losses.item(), str(datetime.timedelta(seconds=round(time.time()-total_time)))\n",
    "        ),\n",
    "        end=''\n",
    "    )\n",
    "    \n",
    "def evaluate_model(model, data_loader, device, writer, model_type, model_name, epoch):\n",
    "    with torch.no_grad():\n",
    "        epoch_time = time.time()\n",
    "        avg_loss = []\n",
    "        batch_nr = 0\n",
    "        for images, targets in data_loader:\n",
    "            # Send them to device if using GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            pred = model(images, targets)\n",
    "            losses = sum(loss for loss in pred.values())\n",
    "            avg_loss.append(losses.item())\n",
    "            \n",
    "            batch_nr = batch_nr + 1\n",
    "            print_loss = losses.item()\n",
    "            \n",
    "            if batch_nr == epoch+1:\n",
    "                print_loss = numpy.average(avg_loss)\n",
    "            print(\n",
    "                '\\r[Val] [{}/{}] - Loss: {} \\tEpoch time elapsed: {}'.format(\n",
    "                    batch_nr, len(data_loader), print_loss, str(datetime.timedelta(seconds=round(time.time()-epoch_time)))\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "\n",
    "        writer.add_scalars(model_type+'_'+model_name, {\n",
    "            'val_loss': numpy.average(avg_loss),\n",
    "        }, epoch)\n",
    "\n",
    "def train_multi_frcnn(model_name, model_type, epochs, components=[]):\n",
    "    if components == []:\n",
    "        f = open('./dataset/labels.txt', \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "        components = {data[i]:i for i in range(len(data))}\n",
    "\n",
    "    for component in components:\n",
    "        dataset_train = SketchDataset('./dataset', 'train', single_component=component)\n",
    "        dataset_val = SketchDataset('./dataset', 'val', single_component=component)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "                dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "\n",
    "        device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        num_classes = 2\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                       step_size=3,\n",
    "                                                       gamma=0.1)\n",
    "\n",
    "        train_model(model, optimizer, data_loader, data_loader_val, device, epochs, model_type, model_name, lr_scheduler, folder_name=component+'_'+model_name)\n",
    "        \n",
    "def train_multi_ssd(model_name, model_type, epochs, components=[]):\n",
    "    if components == []:\n",
    "        f = open('./dataset/labels.txt', \"r\")\n",
    "        data = f.read().split('\\n')\n",
    "        f.close()\n",
    "        components = {data[i]:i for i in range(len(data))}\n",
    "\n",
    "    for component in components:\n",
    "        print('Training:',component)\n",
    "        print()\n",
    "        dataset_train = SketchDataset('./dataset', 'train', single_component=component)\n",
    "        dataset_val = SketchDataset('./dataset', 'val', single_component=component)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=5, shuffle=True, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "                dataset_val, batch_size=2, shuffle=False, num_workers=0,\n",
    "                collate_fn=collate_fn)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "        num_classes = 2\n",
    "        in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "        model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "\n",
    "        model_ssd.to(device)\n",
    "\n",
    "\n",
    "        params = [p for p in model_ssd.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.00005, \n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        # and a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                       step_size=3,\n",
    "                                                       gamma=0.1)\n",
    "        \n",
    "\n",
    "        train_model(model_ssd, optimizer, data_loader, data_loader_val, device, epochs, model_type, model_name, lr_scheduler, folder_name=component+'_'+model_name)\n",
    "    \n",
    "def load_frcnn(date, model_name, num_classes=13):\n",
    "    device = torch.device('cpu')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, \n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    checkpoint = torch.load('./models/Faster-RCNN/'+date+'/'+model_name+'.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model,optimizer\n",
    "    \n",
    "def load_ssd(date, model_name):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model_ssd = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "\n",
    "    num_classes = 13\n",
    "    in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "    num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "    model_ssd.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "    model_ssd.to(device)\n",
    "\n",
    "    params = [p for p in model_ssd.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, \n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    checkpoint = torch.load('./models/SSD/'+date+'/'+model_name+'.pt', map_location=device)\n",
    "    model_ssd.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model_ssd,optimizer,device\n",
    "\n",
    "def post_process_results(results, comp_defs, labels, min_score=[0.5, 0.5], non_ui_components=[10,11,12]):\n",
    "    bb_res,txt_res = results\n",
    "    \n",
    "    reference_id      = non_ui_components[0]\n",
    "    object_id         = non_ui_components[1]\n",
    "    reference_head_id = non_ui_components[2]\n",
    "\n",
    "    print('Saniticing results')\n",
    "    # 1. First sanitice the results aka remove anything less than min_score\n",
    "    remove_results = []\n",
    "    remove_txt    = []\n",
    "    for i in range(len(bb_res['boxes'])):\n",
    "        if bb_res['scores'][i] < min_score[0]:\n",
    "            remove_results.append(i)\n",
    "            \n",
    "    for i in range(len(txt_res['boxes'])):\n",
    "        if len(txt_res['words'][i]) < 2:\n",
    "            remove_txt.append(i)\n",
    "            continue\n",
    "            \n",
    "        if txt_res['scores'][i] < min_score[1]:\n",
    "            remove_txt.append(i)\n",
    "        else:\n",
    "            for c in range(len(bb_res['boxes'])):\n",
    "                # If we have a larger text than the object it intersects with we want to remove it\n",
    "                # This is because words can be interpereted as components in the sketch\n",
    "                txt_box = txt_res['boxes'][i].copy()\n",
    "                if bb_res['labels'][c] in [8,9]:\n",
    "                    txt_box[0] += 200\n",
    "                fully_overlap = calc_iou(bb_res['boxes'][c], txt_box) == 1\n",
    "                if fully_overlap and c not in remove_results:\n",
    "                    remove_results.append(c)\n",
    "\n",
    "    remove_results.sort(reverse=True) # To get the highest id first\n",
    "    remove_txt.sort(reverse=True) # To get the highest id first\n",
    "    \n",
    "    saniticed_bb_results = copy.deepcopy(bb_res)\n",
    "    for idx in remove_results:\n",
    "        saniticed_bb_results['boxes']  = numpy.delete(saniticed_bb_results['boxes'], idx, 0)\n",
    "        saniticed_bb_results['scores'] = numpy.delete(saniticed_bb_results['scores'], idx, 0)\n",
    "        saniticed_bb_results['labels'] = numpy.delete(saniticed_bb_results['labels'], idx, 0)\n",
    "        \n",
    "    saniticed_txt_results = copy.deepcopy(txt_res)\n",
    "    for idx in remove_txt:\n",
    "        saniticed_txt_results['boxes']  = numpy.delete(saniticed_txt_results['boxes'], idx, 0)\n",
    "        saniticed_txt_results['scores'] = numpy.delete(saniticed_txt_results['scores'], idx, 0)\n",
    "        saniticed_txt_results['words'] = numpy.delete(saniticed_txt_results['words'], idx, 0)\n",
    "        \n",
    "    print('Removed', len(remove_results), 'sketch objects and',len(remove_txt),'text objects')\n",
    "\n",
    "    word_to_idx = {}\n",
    "    len_before_add = len(saniticed_bb_results['boxes'])\n",
    "    # 1.1 Check if any string doesnt overlap at for a new component to be created\n",
    "    for i in range(len(saniticed_txt_results['boxes'])):\n",
    "        t_found_overlap = False\n",
    "        for c in range(len(saniticed_bb_results['boxes'])):\n",
    "            paragraphBigger = calc_iou(saniticed_txt_results['boxes'][i], saniticed_bb_results['boxes'][c]) < calc_iou(saniticed_bb_results['boxes'][c], saniticed_txt_results['boxes'][i])\n",
    "            if not paragraphBigger and calc_iou(saniticed_txt_results['boxes'][i], saniticed_bb_results['boxes'][c]) > 0.2:\n",
    "                t_found_overlap = True\n",
    "                break\n",
    "    \n",
    "        if not t_found_overlap:\n",
    "            saniticed_bb_results['boxes'] = numpy.append(saniticed_bb_results['boxes'], [saniticed_txt_results['boxes'][i]], axis = 0)\n",
    "            saniticed_bb_results['scores'] = numpy.append(saniticed_bb_results['scores'], [saniticed_txt_results['scores'][i]], axis = 0)\n",
    "            saniticed_bb_results['labels'] = numpy.append(saniticed_bb_results['labels'], [saniticed_txt_results['words'][i]], axis = 0)            \n",
    "            word_to_idx[calc_area(saniticed_bb_results['boxes'][len(saniticed_bb_results['boxes'])-1])] = saniticed_txt_results['words'][i]\n",
    "            print('Words added:',saniticed_txt_results['words'][i])\n",
    "    \n",
    "    print('Added',len(saniticed_bb_results['boxes'])-len_before_add, 'text components')\n",
    "    \n",
    "    # 2. Resolve which texts are paragraphs and which are additions to a certain component\n",
    "    remove_txt = []\n",
    "    for i in range(len(saniticed_txt_results['boxes'])):\n",
    "        t_cover_precentage = []\n",
    "        t_found_component = False\n",
    "        t_self_idx = -1\n",
    "        for c_def in comp_defs[len(comp_defs)-1][4]:\n",
    "            for c in range(len(saniticed_bb_results['boxes'])):\n",
    "                txt_box = saniticed_txt_results['boxes'][i].copy()\n",
    "                txt_box[0] -= 100\n",
    "                curr_iou = calc_iou(txt_box, saniticed_bb_results['boxes'][c])\n",
    "                if saniticed_bb_results['labels'][c] == saniticed_txt_results['words'][i]:\n",
    "                    t_self_idx = c\n",
    "                    if not t_found_component:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                if t_found_component:\n",
    "                    continue;\n",
    "                    \n",
    "                if curr_iou > 0.0:\n",
    "                    t_cover_precentage.append((c, curr_iou))\n",
    "                    try:\n",
    "                        lbl = int(saniticed_bb_results['labels'][c])\n",
    "                    except ValueError:\n",
    "                        lbl = 13\n",
    "                    if lbl-1 != c_def:\n",
    "                        continue;\n",
    "                    if comp_defs[lbl-1][0] or comp_defs[lbl-1][2]:\n",
    "                        word_to_idx[calc_area(saniticed_bb_results['boxes'][c])] = saniticed_txt_results['words'][i]\n",
    "                        print('Found text for a component: ',labels[lbl-1], '->', saniticed_txt_results['words'][i])\n",
    "                        t_found_component = True\n",
    "                        if t_self_idx != -1:\n",
    "                            break\n",
    "                        \n",
    "            if t_found_component:\n",
    "                if t_self_idx != -1:\n",
    "                    remove_txt.append(t_self_idx)\n",
    "                break;   \n",
    "        \n",
    "        if not t_found_component and len(t_cover_precentage) > 0:\n",
    "            t_cover_precentage.sort(key=lambda y: y[1], reverse=True)\n",
    "            t_changed_component = False\n",
    "            for idx,iou in t_cover_precentage:\n",
    "                try:\n",
    "                    idx_lbl = int(saniticed_bb_results['labels'][idx])-1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                for c_def in comp_defs[idx_lbl][4]:\n",
    "                    if comp_defs[c_def][0] or comp_defs[c_def][2]:\n",
    "                        saniticed_bb_results['labels'][idx] = c_def+1\n",
    "                        word_to_idx[calc_area(saniticed_bb_results['boxes'][idx])] = saniticed_txt_results['words'][i]\n",
    "                        print('Found text overlapping a unsupported component, component changed from: ',labels[idx_lbl], 'to', labels[c_def], '->', saniticed_txt_results['words'][i])\n",
    "                        t_changed_component = True\n",
    "                        break;\n",
    "                if t_changed_component:\n",
    "                    break;\n",
    "                        \n",
    "                \n",
    "    \n",
    "    remove_txt.sort(reverse=True) # To get the highest id first\n",
    "    for idx in remove_txt:\n",
    "        saniticed_bb_results['boxes']  = numpy.delete(saniticed_bb_results['boxes'], idx, 0)\n",
    "        saniticed_bb_results['scores'] = numpy.delete(saniticed_bb_results['scores'], idx, 0)\n",
    "        saniticed_bb_results['labels'] = numpy.delete(saniticed_bb_results['labels'], idx, 0)\n",
    "\n",
    "    # 3. Resolve references\n",
    "    references      = []\n",
    "    reference_heads = []\n",
    "    objects         = []\n",
    "    for i in range(len(saniticed_bb_results['boxes'])):\n",
    "        try:\n",
    "            if int(saniticed_bb_results['labels'][i]) == reference_id:\n",
    "                references.append(i)\n",
    "            elif int(saniticed_bb_results['labels'][i]) == reference_head_id:\n",
    "                reference_heads.append(i)\n",
    "            elif int(saniticed_bb_results['labels'][i]) == object_id:\n",
    "                objects.append(i)\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    # If the model has found the heads to the reference we can easily deduce its direction otherwise we need to deduce it manually\n",
    "    ref_to_obj = []\n",
    "    if references == [] and reference_heads != []:\n",
    "        references = reference_heads.copy()\n",
    "        reference_heads = []\n",
    "        \n",
    "    if reference_heads != []:\n",
    "        for i, ref_idx in enumerate(references):\n",
    "            box = saniticed_bb_results['boxes'][ref_idx]\n",
    "            head_box = saniticed_bb_results['boxes'][reference_heads[i]]\n",
    "            p1_ul = [box[0],box[1]]\n",
    "            p1_ur = [box[2],box[1]]\n",
    "            p1_ll = [box[0],box[3]]\n",
    "            p1_lr = [box[2],box[3]]\n",
    "            p1 = [p1_ul, p1_ur, p1_ll, p1_lr]\n",
    "            \n",
    "            head_p = [(head_box[2]-head_box[0])/2+head_box[0], (head_box[3]-head_box[1])/2+head_box[1]]\n",
    "            smallest_distance = 10000000\n",
    "            point = head_p\n",
    "            oppo_point = (-1, -1)\n",
    "            \n",
    "            for c, ref_p in enumerate(p1):\n",
    "                diff = ((((ref_p[0] - head_p[0] )**2) + ((ref_p[1] - head_p[1])**2) )**0.5)\n",
    "                if diff < smallest_distance:\n",
    "                    smallest_distance = diff\n",
    "                    oppo_point = p1[3-c]\n",
    "            \n",
    "            smallest_distance = 10000000  \n",
    "            idx = -1\n",
    "            \n",
    "            for obj_idx in objects:\n",
    "                box = saniticed_bb_results['boxes'][obj_idx]\n",
    "                p2_ul = [box[0],box[1]]\n",
    "                p2_ur = [box[2],box[1]]\n",
    "                p2_ll = [box[0],box[3]]\n",
    "                p2_lr = [box[2],box[3]]\n",
    "                p2 = [p2_ul, p2_ur, p2_ll, p2_lr]\n",
    "                for obj_p in p2:                    \n",
    "                    diff = ((((obj_p[0] - head_p[0] )**2) + ((obj_p[1] - head_p[1])**2) )**0.5)\n",
    "                    if diff < smallest_distance:\n",
    "                        smallest_distance = diff\n",
    "                        idx = obj_idx\n",
    "                            \n",
    "            if idx > -1:\n",
    "                ref_to_obj.append((ref_idx, idx, point, oppo_point))\n",
    "    else:\n",
    "        for ref_idx in references:\n",
    "            box = saniticed_bb_results['boxes'][ref_idx]\n",
    "            p1_ul = [box[0],box[1]]\n",
    "            p1_ur = [box[2],box[1]]\n",
    "            p1_ll = [box[0],box[3]]\n",
    "            p1_lr = [box[2],box[3]]\n",
    "            p1 = [p1_ul, p1_ur, p1_ll, p1_lr]\n",
    "            smallest_distance = 10000000\n",
    "            idx = -1\n",
    "            point = (-1, -1)\n",
    "            oppo_point = (-1, -1)\n",
    "\n",
    "            for obj_idx in objects:\n",
    "                box = saniticed_bb_results['boxes'][obj_idx]\n",
    "                p2_ul = [box[0],box[1]]\n",
    "                p2_ur = [box[2],box[1]]\n",
    "                p2_ll = [box[0],box[3]]\n",
    "                p2_lr = [box[2],box[3]]\n",
    "                p2 = [p2_ul, p2_ur, p2_ll, p2_lr]\n",
    "                for ref_p in p1:\n",
    "                    for obj_p in p2:\n",
    "                        diff = ((((obj_p[0] - ref_p[0] )**2) + ((obj_p[1] - ref_p[1])**2) )**0.5)\n",
    "                        \n",
    "                        if diff < smallest_distance:\n",
    "                            smallest_distance = diff\n",
    "                            idx = obj_idx\n",
    "                            point = ref_p\n",
    "                            oppo_point = p1[3-p1.index(ref_p)]\n",
    "                            \n",
    "            if idx > -1:\n",
    "                ref_to_obj.append((ref_idx, idx, point, oppo_point))\n",
    "    \n",
    "    print('Attempting to resolve references')\n",
    "    for i in range(len(ref_to_obj)):\n",
    "        p = ref_to_obj[i][3]\n",
    "        smallest_distance = 10000000\n",
    "        comp_idx = -1\n",
    "        smallest_point = (-1,-1)\n",
    "        for c in range(len(saniticed_bb_results['boxes'])):\n",
    "            try:\n",
    "                comp_lbl = int(saniticed_bb_results['labels'][c])\n",
    "            except ValueError:\n",
    "                comp_lbl = 13\n",
    "                \n",
    "            if comp_lbl == reference_id or comp_lbl == reference_head_id or comp_lbl == object_id:\n",
    "                continue\n",
    "                \n",
    "            box = saniticed_bb_results['boxes'][c]\n",
    "            p2_ul = [box[0],box[1]]\n",
    "            p2_ur = [box[2],box[1]]\n",
    "            p2_ll = [box[0],box[3]]\n",
    "            p2_lr = [box[2],box[3]]\n",
    "            p2 = [p2_ul, p2_ur, p2_ll, p2_lr]\n",
    "            for comp_p in p2:\n",
    "                diff = ((((comp_p[0] - p[0] )**2) + ((comp_p[1] - p[1])**2) )**0.5)\n",
    "                if diff < smallest_distance:\n",
    "                    smallest_distance = diff\n",
    "                    comp_idx = c\n",
    "                    smallest_point = comp_p\n",
    "    \n",
    "        if comp_idx > -1:\n",
    "            try:\n",
    "                comp_lbl = int(saniticed_bb_results['labels'][comp_idx])-1\n",
    "            except ValueError:\n",
    "                comp_lbl = 12\n",
    "\n",
    "            if comp_defs[comp_lbl][1] or comp_defs[comp_lbl][3]:\n",
    "                ref_to_obj[i] = (comp_idx, ref_to_obj[i][1], ref_to_obj[i][2], ref_to_obj[i][3])\n",
    "            else:\n",
    "                print('Did not find a suitable comp directly, attempting to change closest component')\n",
    "                t_found_other = False\n",
    "                for other_comp in comp_defs[comp_lbl][4]:\n",
    "                    if comp_defs[other_comp][1] or comp_defs[other_comp][3]:\n",
    "                        word_to_idx[calc_area(saniticed_bb_results['boxes'][comp_idx])] = saniticed_bb_results['labels'][comp_idx]\n",
    "                        saniticed_bb_results['labels'][comp_idx] = other_comp+1\n",
    "                        ref_to_obj[i] = (comp_idx, ref_to_obj[i][1], ref_to_obj[i][2], ref_to_obj[i][3])\n",
    "                        t_found_other = True\n",
    "                        break\n",
    "\n",
    "                if not t_found_other:\n",
    "                    ref_to_obj[i] = (-1, ref_to_obj[i][1], ref_to_obj[i][2], ref_to_obj[i][3])\n",
    "                    # Since we did not find a comp that supported references for the closest comp\n",
    "                    # we define the reference as undefined\n",
    "                    \n",
    "    # Clean up word_to_idx, since we used it more as word_to_size\n",
    "    for i in range(len(saniticed_bb_results['boxes'])):\n",
    "        try:\n",
    "            word = word_to_idx[calc_area(saniticed_bb_results['boxes'][i])]\n",
    "            word_to_idx[i] = word\n",
    "            del word_to_idx[calc_area(saniticed_bb_results['boxes'][i])]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    # 4. Sort the array by the biggest objects\n",
    "    idx_to_size = []\n",
    "    for i in range(len(saniticed_bb_results['boxes'])):\n",
    "        idx_to_size.append((i, calc_area(saniticed_bb_results['boxes'][i])))\n",
    "    idx_to_size.sort(key=lambda y: y[1], reverse=True)\n",
    "     \n",
    "    # 5. Top-down walk to get the row/cols of the layout\n",
    "    rows = []\n",
    "    ignore_comps = []\n",
    "    for c in range(len(saniticed_bb_results['boxes'])):\n",
    "        top_element = [0, 1000000, 0, 0]\n",
    "        top_element_idx = -1\n",
    "        # Get the current highest element from the picture\n",
    "        for i,box in enumerate(saniticed_bb_results['boxes']):\n",
    "            if i in ignore_comps:\n",
    "                continue;\n",
    "            try:\n",
    "                if int(saniticed_bb_results['labels'][i]) in non_ui_components:\n",
    "                    continue;\n",
    "            except ValueError:\n",
    "                pass\n",
    "            if box[1] < top_element[1]:\n",
    "                top_element[0] = box[0]\n",
    "                top_element[1] = box[1]\n",
    "                top_element[2] = box[2]\n",
    "                top_element[3] = box[3]\n",
    "                top_element_idx = i\n",
    "        if top_element_idx == -1:\n",
    "            break;\n",
    "        cols = [(top_element_idx, top_element[0])]\n",
    "        # Make so that we cover all the sides of the top element to find anything on that row\n",
    "        top_element[0] = 0\n",
    "        top_element[2] = 1000000\n",
    "        # Now we need to find which elements are on the same row as the current highest element\n",
    "        for i,box in enumerate(saniticed_bb_results['boxes']):\n",
    "            if i == top_element_idx:\n",
    "                continue;\n",
    "            if i in ignore_comps:\n",
    "                continue;\n",
    "            try:\n",
    "                if int(saniticed_bb_results['labels'][i]) in non_ui_components:\n",
    "                    continue;\n",
    "            except ValueError:\n",
    "                pass\n",
    "            if calc_iou(top_element, box) > 0:\n",
    "                cols.append((i, box[0]))\n",
    "        \n",
    "        # Now we got the objects on the same column, now we need to find the biggest of that intersects with a larger portion\n",
    "        for size in idx_to_size:\n",
    "            biggest_idx = -1\n",
    "            for col in cols:\n",
    "                if col[0] == size[0]:\n",
    "                    biggest_idx = size[0]\n",
    "                    break;\n",
    "            if biggest_idx != -1:\n",
    "                break;\n",
    "        biggest_cols = []\n",
    "        biggest_box = numpy.copy(saniticed_bb_results['boxes'][biggest_idx])\n",
    "        biggest_box[0] = 0\n",
    "        biggest_box[2] = 1000000\n",
    "        for i,box in enumerate(saniticed_bb_results['boxes']):\n",
    "            if i in ignore_comps:\n",
    "                continue;\n",
    "            try:\n",
    "                if int(saniticed_bb_results['labels'][i]) in non_ui_components:\n",
    "                    continue;\n",
    "            except ValueError:\n",
    "                pass\n",
    "            if calc_iou(biggest_box, box) > 0:\n",
    "                biggest_cols.append((i, box))\n",
    "        all_cols = []\n",
    "        for col in biggest_cols:            \n",
    "            curr_col = []\n",
    "            curr_box = [0, col[1][1], 1000000, col[1][3]]\n",
    "            for i,box in enumerate(saniticed_bb_results['boxes']):\n",
    "                if i in ignore_comps:\n",
    "                    continue;\n",
    "                try:\n",
    "                    if int(saniticed_bb_results['labels'][i]) in non_ui_components:\n",
    "                        continue;\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                if calc_iou(curr_box, box) > 0:\n",
    "                    curr_col.append((i, box))\n",
    "            if curr_col != []:\n",
    "                all_cols.append(curr_col)\n",
    "            else:\n",
    "                print(\"ERROR, THIS SHOULD NOT BE REACHED! THIS IS SUPER STRANGE SINCE WE SHOULD ALWAYS GET A OVERLAP\")\n",
    "\n",
    "        if len(all_cols) > 1:\n",
    "            remove_idxs = [] # This is because we want to remove entries with more columns than it actually is\n",
    "            longest_intersect_col = 0\n",
    "            \n",
    "            for i in range(len(all_cols)):\n",
    "                len_of_col = 1000\n",
    "                for c in range(len(all_cols[i])):\n",
    "                    curr_box = [0, all_cols[i][c][1][1], 1000000, all_cols[i][c][1][3]]\n",
    "                    overlap = 0\n",
    "                    for y in range(len(all_cols[i])):\n",
    "                        if calc_iou(curr_box, all_cols[i][y][1]) > 0:\n",
    "                            overlap += 1\n",
    "                    if overlap < len_of_col:\n",
    "                        len_of_col = overlap\n",
    "                        \n",
    "                if len_of_col < len(all_cols[i]):\n",
    "                    remove_idxs.append(i)\n",
    "                if len_of_col > longest_intersect_col:\n",
    "                    longest_intersect_col = len_of_col\n",
    "\n",
    "            remove_idxs.sort(reverse=True)\n",
    "            for idx in remove_idxs:\n",
    "                all_cols.pop(idx)\n",
    "                \n",
    "            new_cols = []\n",
    "            for c in range(len(all_cols)):\n",
    "                curr_col = all_cols[c]\n",
    "                curr_col.sort(key=lambda y: y[1][0])\n",
    "                sorted_col = []\n",
    "                for t_col in curr_col:\n",
    "                    sorted_col.append((t_col[0], t_col[1][0], t_col[1][1]))\n",
    "                new_cols.append(sorted_col)\n",
    "                \n",
    "            b_same = True\n",
    "            for col1 in new_cols:\n",
    "                for col2 in new_cols:\n",
    "                    if col1 != col2:\n",
    "                        b_same = False\n",
    "                        break;\n",
    "                if not b_same:\n",
    "                    break;\n",
    "            if b_same:\n",
    "                rows.append(new_cols[0])\n",
    "            else:        \n",
    "                remove_idxs = []\n",
    "                for idx, col in enumerate(new_cols):\n",
    "                    except_idx = new_cols[0:idx]+new_cols[idx+1:len(new_cols)]\n",
    "                    if col in except_idx and not except_idx.index(col) in remove_idxs and not idx in remove_idxs:\n",
    "                        remove_idxs.append(idx)\n",
    "                \n",
    "                remove_idxs.sort(reverse=True)\n",
    "                for idx in remove_idxs:\n",
    "                    new_cols.pop(idx)\n",
    "                    \n",
    "                rows.append((True, new_cols))\n",
    "        else:\n",
    "            a_cols = all_cols[0]\n",
    "            fixed_col = []\n",
    "            for t_col in a_cols:\n",
    "                fixed_col.append((t_col[0], t_col[1][0], t_col[1][1]))\n",
    "            rows.append(fixed_col)\n",
    "        \n",
    "        for acol in all_cols:\n",
    "            for col in acol:\n",
    "                ignore_comps.append(col[0])\n",
    "\n",
    "    # 6. Create json results\n",
    "    print('Creating json formating')\n",
    "    json_dict = {\"rows\":{}, \"objects\":{}}\n",
    "    for idx,ref in enumerate(ref_to_obj):\n",
    "        json_dict[\"objects\"][str(idx)] = word_to_idx[ref[1]]\n",
    "    \n",
    "    for row_idx,row in enumerate(rows):\n",
    "        row_dict = {}\n",
    "        if type(row) is tuple:\n",
    "            # Multiple rows in same row\n",
    "            largest_col = 0\n",
    "            multi_rows = row[1]\n",
    "            for mrow in multi_rows:\n",
    "                if len(mrow) > largest_col:\n",
    "                    largest_col = len(mrow)\n",
    "            columns = []\n",
    "            for col_idx in range(largest_col):\n",
    "                components = []\n",
    "                for mrow_idx in range(len(multi_rows)):                    \n",
    "                    try:\n",
    "                        comp_idx = multi_rows[mrow_idx][col_idx][0]\n",
    "                        comp_height = multi_rows[mrow_idx][col_idx][2]\n",
    "                        if (comp_idx, comp_height) not in components:\n",
    "                            components.append((comp_idx, comp_height))\n",
    "                    except KeyError:\n",
    "                        components.append((-1, 1000000))\n",
    "                    except IndexError:\n",
    "                        components.append((-1, 1000000))\n",
    "                columns.append(components)\n",
    "            # Sort the components in the columns by height and remove height variable\n",
    "            t_columns = []\n",
    "            for col in columns:\n",
    "                if len(col) < 2:\n",
    "                    t_columns.append([col[0][0]])\n",
    "                    continue\n",
    "                col.sort(key=lambda y: y[1])\n",
    "                t_col = []\n",
    "                for col_comp in col:\n",
    "                    t_col.append(col_comp[0])\n",
    "                t_columns.append(t_col)\n",
    "            columns = t_columns\n",
    "            \n",
    "            for col_idx,col in enumerate(columns):\n",
    "                col_dict = {}\n",
    "                for c_row_idx,c_row in enumerate(col):\n",
    "                    multi_row_dict = {}\n",
    "                    comp_idx = int(c_row)\n",
    "                    try:\n",
    "                        comp_label = int(saniticed_bb_results['labels'][comp_idx])\n",
    "                    except ValueError:\n",
    "                        comp_label = 13\n",
    "                    if len(col) == 1:\n",
    "                        col_dict['component'] = labels[comp_label-1]\n",
    "                    multi_row_dict['component'] = labels[comp_label-1]\n",
    "\n",
    "                    try:\n",
    "                        if len(col) == 1:\n",
    "                            col_dict['text'] = word_to_idx[comp_idx]\n",
    "                        multi_row_dict['text'] = word_to_idx[comp_idx]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                    for ref in ref_to_obj:\n",
    "                        if ref[0] == comp_idx:\n",
    "                            try:\n",
    "                                if len(col) == 1:\n",
    "                                    col_dict['reference'] = word_to_idx[ref[1]]\n",
    "                                multi_row_dict['reference'] = word_to_idx[ref[1]]\n",
    "                            except KeyError:\n",
    "                                print('Error could not resolve a reference value to an object, skipping reference')\n",
    "                            break;\n",
    "                    \n",
    "                    if len(col) > 1:\n",
    "                        try:\n",
    "                            col_dict['rows'][str(c_row_idx)] = multi_row_dict\n",
    "                        except KeyError:\n",
    "                            col_dict['rows'] = {}\n",
    "                            col_dict['rows'][str(c_row_idx)] = multi_row_dict\n",
    "\n",
    "                try:\n",
    "                    row_dict['cols'][str(col_idx)] = col_dict\n",
    "                except KeyError:\n",
    "                    row_dict['cols'] = {}\n",
    "                    row_dict['cols'][str(col_idx)] = col_dict\n",
    "        else:\n",
    "            for col_idx,col in enumerate(row):\n",
    "                col_dict = {}\n",
    "                comp_idx = int(col[0])\n",
    "                try:\n",
    "                    comp_label = int(saniticed_bb_results['labels'][comp_idx])\n",
    "                except ValueError:\n",
    "                    comp_label = 13\n",
    "\n",
    "                if len(row) == 1:\n",
    "                    row_dict['component'] = labels[comp_label-1]\n",
    "                col_dict['component'] = labels[comp_label-1]\n",
    "                    \n",
    "                try:\n",
    "                    if len(row) == 1:\n",
    "                        row_dict['text'] = word_to_idx[comp_idx]\n",
    "                    col_dict['text'] = word_to_idx[comp_idx]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                for ref in ref_to_obj:\n",
    "                    if ref[0] == comp_idx:\n",
    "                        try:\n",
    "                            if len(row) == 1:\n",
    "                                row_dict['reference'] = word_to_idx[ref[1]]\n",
    "                            col_dict['reference'] = word_to_idx[ref[1]]\n",
    "                        except KeyError:\n",
    "                            print('Error could not resolve a reference value to an object, skipping reference')\n",
    "                        break;\n",
    "                if len(row) > 1:\n",
    "                    try:\n",
    "                        row_dict['cols'][str(col_idx)] = col_dict\n",
    "                    except KeyError:\n",
    "                        row_dict['cols'] = {}\n",
    "                        row_dict['cols'][str(col_idx)] = col_dict\n",
    "\n",
    "        json_dict['rows'][str(row_idx)] = row_dict\n",
    "    \n",
    "    return json.dumps(json_dict)\n",
    "\n",
    "def get_component_definitions(file_name, labels_to_idx):\n",
    "    # Gets the component definitions by the following format\n",
    "    # must_have_text   must_have_reference    can_have_text    can_have_reference    similarity_to_components[component.id]    \n",
    "    components_definition = []\n",
    "        \n",
    "    f = open('./'+file_name, \"r\")\n",
    "    json_data = json.load(f)\n",
    "    for key in json_data['components'].keys():\n",
    "        if key == \"Paragraph\":\n",
    "            continue\n",
    "        \n",
    "        data = json_data['components'][key]\n",
    "\n",
    "        hasText = type(data['hasText']) is not bool\n",
    "        requiredText = data['hasText'] == 'required'\n",
    "        \n",
    "        hasRef = type(data['hasReference']) is not bool\n",
    "        requiredRef = data['hasReference'] == 'required'\n",
    "        label_idxs = []\n",
    "        for d in data['similarity']:\n",
    "            label_idxs.append(labels_to_idx[d])\n",
    "           \n",
    "        components_definition.append((hasText and requiredText, hasRef and requiredRef, hasText or requiredText, hasRef or requiredRef, label_idxs))\n",
    "    # Since Paragraph is loosely defined we want it to be last at all times:\n",
    "    data = json_data['components']['Paragraph']\n",
    "\n",
    "    hasText = type(data['hasText']) is not bool\n",
    "    requiredText = data['hasText'] == 'required'\n",
    "\n",
    "    hasRef = type(data['hasReference']) is not bool\n",
    "    requiredRef = data['hasReference'] == 'required'\n",
    "    label_idxs = []\n",
    "    for d in data['similarity']:\n",
    "        label_idxs.append(labels_to_idx[d])\n",
    "\n",
    "    components_definition.append((hasText and requiredText, hasRef and requiredRef, hasText or requiredText, hasRef or requiredRef, label_idxs))\n",
    "\n",
    "    return components_definition\n",
    "\n",
    "def calc_multi_map(model, data_loader, precentages=[90,75,50], device=torch.device('cpu'), print_res=True):\n",
    "    model.eval()\n",
    "    num_classes = model.roi_heads.box_predictor.cls_score.out_features\n",
    "    metrics = []\n",
    "    \n",
    "    for precentage in precentages:\n",
    "        metric = calc_map(model, data_loader, device, num_classes, IoU=precentage/100.0)\n",
    "        metrics.append(metric)\n",
    "        if print_res:\n",
    "            print()\n",
    "            print_map_res(metric, precentage, num_classes)\n",
    "    \n",
    "    return metrics\n",
    "            \n",
    "def print_map_res(metric, precentage, num_classes):\n",
    "    f = open('./datasets/labels.txt', \"r\")\n",
    "    data = f.read().split('\\n')\n",
    "    f.close()\n",
    "    labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "    print('Values @'+str(precentage)+':')\n",
    "    print(metric[1])\n",
    "    print()\n",
    "\n",
    "    formating = [2,2,2,2,1,1,1,1,1,1,2,1,2]\n",
    "    for i in range(num_classes-1):\n",
    "        print(labels[i],'\\t'*formating[i]+'->',metric[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e3d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saniticing results\n",
      "Removed 4 sketch objects and 5 text objects\n",
      "Words added: Words\n",
      "Words added: PARAGRAPH\n",
      "Words added: SAVE\n",
      "Added 3 text components\n",
      "Found text for a component:  Checkbox -> SAVE\n",
      "Found text overlapping a unsupported component, component changed from:  Combobox to Button -> search\n",
      "JUMP\n",
      "[]\n",
      "{314736.0: 'Words', 410256.0: 'PARAGRAPH', 156615.0: 'SAVE', 66251.28527024388: 'SAVE', 488751.5873377472: 'search'}\n",
      "ref empty, refhead not:  [1] []\n",
      "Attempting to resolve references\n",
      "Creating json formating\n",
      "\n",
      "{\"rows\": {\"0\": {\"cols\": {\"0\": {\"component\": \"Image\", \"text\": \"Words\"}, \"1\": {\"component\": \"Image\", \"text\": \"PARAGRAPH\"}}}, \"1\": {\"cols\": {\"0\": {\"component\": \"Datagrid\"}, \"1\": {\"rows\": {\"0\": {\"component\": \"Checkbox\", \"text\": \"SAVE\"}, \"1\": {\"component\": \"Radiobutton\"}, \"2\": {\"component\": \"Radiobutton\"}}}}}, \"2\": {\"component\": \"Button\", \"text\": \"search\"}}, \"objects\": {}}\n",
      "\n",
      "0:00:02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "f = open('./datasets/labels.txt', \"r\")\n",
    "data = f.read().split('\\n')\n",
    "f.close()\n",
    "labels_to_idx = {data[i]:i for i in range(len(data))}\n",
    "idx_to_labels = [data[i] for i in range(len(data))]\n",
    "\n",
    "model,_ = load_frcnn('Mar-09_13-47', 'All-Comp_v9_SGD-StepLR_pre-10')\n",
    "\n",
    "images = ['doublelist_datagrid_complex.jpg', 'loginwebsite_highcaps.jpg', 'middlecomp_highcaps.jpg', \n",
    "          'test_doublelist.jpg', 'words_highcaps.jpg']\n",
    "image = './temp/Postprocessing_tests/'+images[4]\n",
    "\n",
    "#prediction = predict_model(model, image, IoU=0.5, disregard_comp=[10,11,12], priority_comp=[11])\n",
    "#txt_prediction = predict_text(image)\n",
    "comp_defs = get_component_definitions('datasets/definitions.json', labels_to_idx)\n",
    "\n",
    "time_total = str(datetime.timedelta(seconds=round(time.time()-time_start)))\n",
    "json_str = post_process_results((prediction, txt_prediction), comp_defs, idx_to_labels, min_score=[0.80, 0.4])\n",
    "print('\\n'+json_str+'\\n')\n",
    "print(time_total)\n",
    "'''\n",
    "cv2_img = cv2.imread(image)\n",
    "\n",
    "for i in range(len(ref_results)):\n",
    "    x1,y1 = ref_results[i][2]\n",
    "    x2,y2 = ref_results[i][3]\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    for c in range(len(bb_results['boxes'])):\n",
    "        if c == ref_results[i][0]:\n",
    "            box = bb_results['boxes'][c]\n",
    "            p1 = [int((box[2]-box[0])/2+box[0]), int((box[3]-box[1])/2+box[1])]\n",
    "        elif c == ref_results[i][1]:\n",
    "            box = bb_results['boxes'][c]\n",
    "            p2 = [int((box[2]-box[0])/2+box[0]), int((box[3]-box[1])/2+box[1])]\n",
    "\n",
    "    cv2.arrowedLine(cv2_img, p1, p2, (255, 255, 255), 4)\n",
    "    cv2.putText(cv2_img, 'Head', (int(x1),int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 0), 5, cv2.LINE_AA)\n",
    "    cv2.putText(cv2_img, 'Tail', (int(x2),int(y2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "for i in range(len(bb_results['boxes'])):\n",
    "    score = bb_results['scores'][i]\n",
    "    label = bb_results['labels'][i]\n",
    "    x,y,x2,y2 = bb_results['boxes'][i]\n",
    "    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "    try:\n",
    "        word = word_to_idx[i]\n",
    "        cv2.putText(cv2_img, '+'+word, (int((x+x2)/2)-200,int((y+y2)/2+150)), cv2.FONT_HERSHEY_SIMPLEX, 5, (255, 255, 0), 5, cv2.LINE_AA)\n",
    "    except KeyError:\n",
    "        word = ''\n",
    "    try:\n",
    "        label = int(label)\n",
    "        cv2.putText(cv2_img, labels[label-1], (int((x+x2)/2)-100,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)   \n",
    "    except ValueError:\n",
    "        cv2.putText(cv2_img, 'str_'+label, (int((x+x2)/2)-100,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "'''\n",
    "#cv2.imwrite(image[:-4]+'_san.jpg', cv2_img)\n",
    "\n",
    "cv2_img = cv2.imread(image)\n",
    "for i in range(len(prediction['boxes'])):\n",
    "    score = prediction['scores'][i]\n",
    "    label = prediction['labels'][i]\n",
    "    x,y,x2,y2 = prediction['boxes'][i]\n",
    "    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "    try:\n",
    "        label = int(label)\n",
    "        cv2.putText(cv2_img, idx_to_labels[label-1], (int((x+x2)/2)-100,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)   \n",
    "    except ValueError:\n",
    "        cv2.putText(cv2_img, 'str_'+label, (int((x+x2)/2)-100,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "\n",
    "cv2.imwrite(image[:-4]+'_bb.jpg', cv2_img)\n",
    "\n",
    "cv2_img = cv2.imread(image)\n",
    "for i in range(len(txt_prediction['boxes'])):\n",
    "    score = txt_prediction['scores'][i]\n",
    "    label = txt_prediction['words'][i]\n",
    "    x,y,x2,y2 = txt_prediction['boxes'][i]\n",
    "    cv2.rectangle(cv2_img, (int(x), int(y)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "    cv2.putText(cv2_img, str(score*100)[:5], (int((x+x2)/2)-200,int((y+y2)/2)), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "    cv2.putText(cv2_img, 'str_'+label, (int((x+x2)/2)-100,int((y+y2)/2)-150), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "        \n",
    "cv2.imwrite(image[:-4]+'_txt.jpg', cv2_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fd3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
